#!/usr/bin/env python3
"""
Parse model structure CSV to generate memory-map.json

This script reads the CSV file generated by llama-gguf-dump and creates
a JSON file describing the memory layout of the GGUF model.

Usage:
    python parse_csv.py --csv tinyllama_structure.csv --output data/memory-map.json
"""

import argparse
import csv
import json
import os
from typing import List, Dict, Any


def categorize_tensor(component_type: str, tensor_name: str) -> str:
    """
    Categorize tensor by its purpose.

    Args:
        component_type: Component type from CSV (e.g., "Attention Q", "FFN Gate")
        tensor_name: Full tensor name

    Returns:
        Category string: "embedding", "attention", "ffn", "norm", "output"
    """
    component_lower = component_type.lower()
    name_lower = tensor_name.lower()

    if "embedding" in component_lower or "embd" in name_lower:
        return "embedding"
    elif "attention" in component_lower or "attn" in name_lower:
        return "attention"
    elif "ffn" in component_lower or "feed_forward" in component_lower:
        return "ffn"
    elif "norm" in component_lower:
        return "norm"
    elif "output" in component_lower:
        return "output"
    else:
        return "other"


def get_component_name(component_type: str) -> str:
    """
    Extract component name from component_type.

    Args:
        component_type: Component type from CSV

    Returns:
        Simplified component name: "query", "key", "value", "gate", etc.
    """
    component_lower = component_type.lower()

    if "attention q" in component_lower or "attn_q" in component_lower:
        return "query"
    elif "attention k" in component_lower or "attn_k" in component_lower:
        return "key"
    elif "attention v" in component_lower or "attn_v" in component_lower:
        return "value"
    elif "output projection" in component_lower:
        return "output"
    elif "ffn gate" in component_lower or "ffn_gate" in component_lower:
        return "gate"
    elif "ffn up" in component_lower or "ffn_up" in component_lower:
        return "up"
    elif "ffn down" in component_lower or "ffn_down" in component_lower:
        return "down"
    elif "norm" in component_lower:
        return "norm"
    else:
        return "other"


def parse_csv_to_memory_map(csv_path: str, model_name: str = None) -> Dict[str, Any]:
    """
    Parse CSV file and generate memory map structure.

    Args:
        csv_path: Path to CSV file
        model_name: Optional model name (inferred from CSV filename if not provided)

    Returns:
        Dictionary containing memory map data
    """
    if model_name is None:
        # Infer model name from CSV filename
        model_name = os.path.splitext(os.path.basename(csv_path))[0].replace('_structure', '')

    tensors = []
    total_size = 0
    n_layers = 0
    n_vocab = 0
    n_embd = 0

    with open(csv_path, 'r') as f:
        reader = csv.DictReader(f)

        for row in reader:
            # Parse row
            tensor_name = row['tensor_name']
            file_offset = int(row['file_offset'])
            size_bytes = int(row['size_bytes'])
            layer_id = int(row['layer_id']) if row['layer_id'] != '-1' else None
            component_type = row['component_type']
            n_dims = int(row['n_dims'])

            # Parse shape
            shape = []
            for i in range(n_dims):
                dim_val = int(row[f'dim{i}'])
                if dim_val > 0:
                    shape.append(dim_val)

            # Extract metadata
            category = categorize_tensor(component_type, tensor_name)
            component = get_component_name(component_type)

            # Update global stats
            total_size = max(total_size, file_offset + size_bytes)
            if layer_id is not None:
                n_layers = max(n_layers, layer_id + 1)

            # Infer n_vocab and n_embd from embeddings
            if "token_embd" in tensor_name.lower():
                if len(shape) >= 2:
                    n_embd = shape[0]
                    n_vocab = shape[1]

            # Create tensor entry
            tensor_entry = {
                "name": tensor_name,
                "offset_start": file_offset,
                "offset_end": file_offset + size_bytes,
                "size_bytes": size_bytes,
                "shape": shape,
                "category": category,
                "layer_id": layer_id,
                "component": component,
                "component_type": component_type  # Keep original for reference
            }

            tensors.append(tensor_entry)

    # Sort tensors by offset (should already be sorted, but just in case)
    tensors.sort(key=lambda t: t['offset_start'])

    # Build memory map
    memory_map = {
        "model_name": model_name,
        "total_size_bytes": total_size,
        "metadata": {
            "n_layers": n_layers,
            "n_vocab": n_vocab,
            "n_embd": n_embd,
            "n_tensors": len(tensors)
        },
        "tensors": tensors
    }

    return memory_map


def main():
    parser = argparse.ArgumentParser(
        description='Parse model structure CSV to generate memory-map.json'
    )
    parser.add_argument(
        '--csv',
        required=True,
        help='Path to CSV file (e.g., tinyllama_structure.csv)'
    )
    parser.add_argument(
        '--output',
        required=True,
        help='Path to output JSON file (e.g., data/memory-map.json)'
    )
    parser.add_argument(
        '--model-name',
        help='Model name (optional, inferred from CSV filename if not provided)'
    )
    parser.add_argument(
        '--pretty',
        action='store_true',
        help='Pretty-print JSON output'
    )

    args = parser.parse_args()

    # Parse CSV
    print(f"Reading CSV: {args.csv}")
    memory_map = parse_csv_to_memory_map(args.csv, args.model_name)

    print(f"Parsed {memory_map['metadata']['n_tensors']} tensors")
    print(f"Model: {memory_map['model_name']}")
    print(f"Layers: {memory_map['metadata']['n_layers']}")
    print(f"Total size: {memory_map['total_size_bytes'] / (1024**3):.2f} GB")

    # Create output directory if needed
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created directory: {output_dir}")

    # Write JSON
    indent = 2 if args.pretty else None
    with open(args.output, 'w') as f:
        json.dump(memory_map, f, indent=indent)

    print(f"âœ“ Memory map written to: {args.output}")
    print(f"  File size: {os.path.getsize(args.output) / 1024:.1f} KB")


if __name__ == '__main__':
    main()
