#!/usr/bin/env python3
"""
Parse model structure CSV to generate memory-map.json

This script reads the CSV file generated by llama-gguf-dump and creates
a JSON file describing the memory layout of the GGUF model.

IMPORTANT: Now automatically calculates and applies the GGUF data section offset
to correct the relative offsets from gguf-dump to absolute file offsets.

Usage:
    python parse_csv.py --csv tinyllama_structure.csv --gguf-file model.gguf --output data/memory-map.json
"""

import argparse
import csv
import json
import os
import struct
from typing import List, Dict, Any


def categorize_tensor(component_type: str, tensor_name: str) -> str:
    """
    Categorize tensor by its purpose.

    Args:
        component_type: Component type from CSV (e.g., "Attention Q", "FFN Gate")
        tensor_name: Full tensor name

    Returns:
        Category string: "embedding", "attention", "ffn", "norm", "output"
    """
    component_lower = component_type.lower()
    name_lower = tensor_name.lower()

    if "embedding" in component_lower or "embd" in name_lower:
        return "embedding"
    elif "attention" in component_lower or "attn" in name_lower:
        return "attention"
    elif "ffn" in component_lower or "feed_forward" in component_lower:
        return "ffn"
    elif "norm" in component_lower:
        return "norm"
    elif "output" in component_lower:
        return "output"
    else:
        return "other"


def get_component_name(component_type: str) -> str:
    """
    Extract component name from component_type.

    Args:
        component_type: Component type from CSV

    Returns:
        Simplified component name: "query", "key", "value", "gate", etc.
    """
    component_lower = component_type.lower()

    if "attention q" in component_lower or "attn_q" in component_lower:
        return "query"
    elif "attention k" in component_lower or "attn_k" in component_lower:
        return "key"
    elif "attention v" in component_lower or "attn_v" in component_lower:
        return "value"
    elif "output projection" in component_lower:
        return "output"
    elif "ffn gate" in component_lower or "ffn_gate" in component_lower:
        return "gate"
    elif "ffn up" in component_lower or "ffn_up" in component_lower:
        return "up"
    elif "ffn down" in component_lower or "ffn_down" in component_lower:
        return "down"
    elif "norm" in component_lower:
        return "norm"
    else:
        return "other"


def calculate_gguf_data_offset(gguf_file_path: str) -> int:
    """
    Calculate the absolute offset where tensor data starts in a GGUF file.

    The GGUF file structure is:
    [Header (24 bytes)][Metadata KV pairs][Tensor Info][Alignment padding][DATA SECTION]
                                                                            ^
                                                            We need this offset

    Args:
        gguf_file_path: Path to GGUF file

    Returns:
        Absolute byte offset where data section starts

    Raises:
        ValueError: If file is not a valid GGUF file
    """
    GGUF_MAGIC = 0x46554747  # "GGUF" in little-endian
    GGUF_ALIGNMENT = 32       # Default alignment (GGUF_DEFAULT_ALIGNMENT)

    def read_string(f):
        """Read a length-prefixed string."""
        length = struct.unpack('<Q', f.read(8))[0]
        return f.read(length).decode('utf-8')

    def skip_value(f, value_type):
        """Skip a metadata value based on its type."""
        # Type sizes from gguf.h
        type_sizes = {
            0: 1,   # UINT8
            1: 1,   # INT8
            2: 2,   # UINT16
            3: 2,   # INT16
            4: 4,   # UINT32
            5: 4,   # INT32
            6: 4,   # FLOAT32
            7: 1,   # BOOL
            10: 8,  # UINT64
            11: 8,  # INT64
            12: 8,  # FLOAT64
        }

        if value_type in type_sizes:
            # Simple scalar type
            f.seek(type_sizes[value_type], 1)
        elif value_type == 8:  # STRING
            read_string(f)
        elif value_type == 9:  # ARRAY
            array_type = struct.unpack('<I', f.read(4))[0]
            array_len = struct.unpack('<Q', f.read(8))[0]
            for _ in range(array_len):
                skip_value(f, array_type)
        else:
            raise ValueError(f"Unknown GGUF value type: {value_type}")

    with open(gguf_file_path, 'rb') as f:
        # Read header (24 bytes)
        magic = struct.unpack('<I', f.read(4))[0]
        if magic != GGUF_MAGIC:
            raise ValueError(f"Not a valid GGUF file: magic = 0x{magic:08X}, expected 0x{GGUF_MAGIC:08X}")

        version = struct.unpack('<I', f.read(4))[0]
        n_tensors = struct.unpack('<Q', f.read(8))[0]
        n_kv = struct.unpack('<Q', f.read(8))[0]

        print(f"  GGUF Header: version={version}, tensors={n_tensors}, metadata_kv={n_kv}")

        # Skip metadata KV pairs
        for _ in range(n_kv):
            key = read_string(f)
            value_type = struct.unpack('<I', f.read(4))[0]
            skip_value(f, value_type)

        metadata_end = f.tell()
        print(f"  Metadata ends at: {metadata_end:,} bytes")

        # Skip tensor info
        for _ in range(n_tensors):
            name = read_string(f)
            n_dims = struct.unpack('<I', f.read(4))[0]
            for _ in range(n_dims):
                dim = struct.unpack('<Q', f.read(8))[0]
            tensor_type = struct.unpack('<I', f.read(4))[0]
            offset = struct.unpack('<Q', f.read(8))[0]

        tensor_info_end = f.tell()
        print(f"  Tensor info ends at: {tensor_info_end:,} bytes")

        # Apply alignment (round up to GGUF_ALIGNMENT boundary)
        data_offset = ((tensor_info_end + GGUF_ALIGNMENT - 1) // GGUF_ALIGNMENT) * GGUF_ALIGNMENT

        padding = data_offset - tensor_info_end
        print(f"  Alignment: {GGUF_ALIGNMENT} bytes, padding: {padding} bytes")
        print(f"  ✓ Data section starts at: {data_offset:,} bytes ({data_offset / (1024*1024):.2f} MB)")

        return data_offset


def parse_csv_to_memory_map(csv_path: str, model_name: str = None, data_offset: int = 0) -> Dict[str, Any]:
    """
    Parse CSV file and generate memory map structure.

    Args:
        csv_path: Path to CSV file
        model_name: Optional model name (inferred from CSV filename if not provided)
        data_offset: Data section offset to add to all tensor offsets (default: 0)

    Returns:
        Dictionary containing memory map data
    """
    if model_name is None:
        # Infer model name from CSV filename
        model_name = os.path.splitext(os.path.basename(csv_path))[0].replace('_structure', '')

    tensors = []
    total_size = 0
    n_layers = 0
    n_vocab = 0
    n_embd = 0

    with open(csv_path, 'r') as f:
        # Skip gguf-dump header lines until we find the CSV header
        # The CSV header always starts with "tensor_name,"
        # (Different gguf-dump versions have 3-5 header lines)
        while True:
            pos = f.tell()
            line = f.readline()
            if line.startswith('tensor_name,'):
                # Found CSV header - seek back so DictReader reads it as header
                f.seek(pos)
                break
            if not line:
                raise ValueError("CSV header ('tensor_name,...') not found in file")

        reader = csv.DictReader(f)

        for row_num, row in enumerate(reader, start=1):
            # Skip empty or invalid rows
            if not row or not row.get('tensor_name'):
                continue

            # Parse row with error handling
            try:
                tensor_name = row['tensor_name']
                file_offset = int(row['file_offset'])
                size_bytes = int(row['size_bytes'])
                layer_id = int(row['layer_id']) if row['layer_id'] != '-1' else None
                component_type = row['component_type']
                n_dims = int(row['n_dims'])

                # Parse shape
                shape = []
                for i in range(n_dims):
                    dim_val = int(row[f'dim{i}'])
                    if dim_val > 0:
                        shape.append(dim_val)

                # Extract metadata
                category = categorize_tensor(component_type, tensor_name)
                component = get_component_name(component_type)
            except (ValueError, TypeError, KeyError) as e:
                print(f"Warning: Skipping row {row_num} due to parse error: {e}")
                print(f"  Row data: {dict(list(row.items())[:5])}")  # Show first 5 fields
                continue

            # Update global stats
            total_size = max(total_size, file_offset + size_bytes)
            if layer_id is not None:
                n_layers = max(n_layers, layer_id + 1)

            # Infer n_vocab and n_embd from embeddings
            if "token_embd" in tensor_name.lower():
                if len(shape) >= 2:
                    n_embd = shape[0]
                    n_vocab = shape[1]

            # Create tensor entry
            # NOTE: file_offset from CSV is RELATIVE to data section
            # We add data_offset to get ABSOLUTE file offset
            absolute_offset_start = file_offset + data_offset
            absolute_offset_end = absolute_offset_start + size_bytes

            tensor_entry = {
                "name": tensor_name,
                "offset_start": absolute_offset_start,
                "offset_end": absolute_offset_end,
                "size_bytes": size_bytes,
                "shape": shape,
                "category": category,
                "layer_id": layer_id,
                "component": component,
                "component_type": component_type  # Keep original for reference
            }

            tensors.append(tensor_entry)

    # Sort tensors by offset (should already be sorted, but just in case)
    tensors.sort(key=lambda t: t['offset_start'])

    # Build memory map
    memory_map = {
        "model_name": model_name,
        "total_size_bytes": total_size,
        "metadata": {
            "n_layers": n_layers,
            "n_vocab": n_vocab,
            "n_embd": n_embd,
            "n_tensors": len(tensors)
        },
        "tensors": tensors
    }

    return memory_map


def main():
    parser = argparse.ArgumentParser(
        description='Parse model structure CSV to generate memory-map.json with corrected absolute offsets'
    )
    parser.add_argument(
        '--csv',
        required=True,
        help='Path to CSV file from llama-gguf-dump (e.g., tinyllama_structure.csv)'
    )
    parser.add_argument(
        '--gguf-file',
        required=True,
        help='Path to original GGUF file (for calculating data section offset)'
    )
    parser.add_argument(
        '--output',
        required=True,
        help='Path to output JSON file (e.g., data/memory-map.json)'
    )
    parser.add_argument(
        '--model-name',
        help='Model name (optional, inferred from CSV filename if not provided)'
    )
    parser.add_argument(
        '--pretty',
        action='store_true',
        help='Pretty-print JSON output'
    )

    args = parser.parse_args()

    # Calculate data section offset from GGUF file
    print(f"Analyzing GGUF file: {args.gguf_file}")
    data_offset = calculate_gguf_data_offset(args.gguf_file)
    print()

    # Parse CSV with offset correction
    print(f"Reading CSV: {args.csv}")
    print(f"Applying data section offset: +{data_offset:,} bytes to all tensor offsets")
    memory_map = parse_csv_to_memory_map(args.csv, args.model_name, data_offset)

    print(f"Parsed {memory_map['metadata']['n_tensors']} tensors")
    print(f"Model: {memory_map['model_name']}")
    print(f"Layers: {memory_map['metadata']['n_layers']}")
    print(f"Total size: {memory_map['total_size_bytes'] / (1024**3):.2f} GB")

    # Create output directory if needed
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created directory: {output_dir}")

    # Write JSON
    indent = 2 if args.pretty else None
    with open(args.output, 'w') as f:
        json.dump(memory_map, f, indent=indent)

    print(f"✓ Memory map written to: {args.output}")
    print(f"  File size: {os.path.getsize(args.output) / 1024:.1f} KB")


if __name__ == '__main__':
    main()
