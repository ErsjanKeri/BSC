#!/usr/bin/env python3
"""
Parse model structure CSV to generate memory-map.json

This script reads the CSV file generated by llama-gguf-dump and creates
a JSON file describing the memory layout of the GGUF model.

IMPORTANT: Now automatically calculates and applies the GGUF data section offset
to correct the relative offsets from gguf-dump to absolute file offsets.

Usage:
    python parse_csv.py --csv tinyllama_structure.csv --gguf-file model.gguf --output data/memory-map.json
"""

import argparse
import csv
import json
import os
import struct
from typing import List, Dict, Any


def categorize_tensor(component_type: str, tensor_name: str) -> str:
    """
    Categorize tensor by its purpose.

    Args:
        component_type: Component type from CSV (e.g., "Attention Q", "FFN Gate")
        tensor_name: Full tensor name

    Returns:
        Category string: "embedding", "attention", "ffn", "norm", "output"
    """
    component_lower = component_type.lower()
    name_lower = tensor_name.lower()

    if "embedding" in component_lower or "embd" in name_lower:
        return "embedding"
    elif "attention" in component_lower or "attn" in name_lower:
        return "attention"
    elif "ffn" in component_lower or "feed_forward" in component_lower:
        return "ffn"
    elif "norm" in component_lower:
        return "norm"
    elif "output" in component_lower:
        return "output"
    else:
        return "other"


def get_component_name(component_type: str) -> str:
    """
    Extract component name from component_type.

    Args:
        component_type: Component type from CSV

    Returns:
        Simplified component name: "query", "key", "value", "gate", etc.
    """
    component_lower = component_type.lower()

    if "attention q" in component_lower or "attn_q" in component_lower:
        return "query"
    elif "attention k" in component_lower or "attn_k" in component_lower:
        return "key"
    elif "attention v" in component_lower or "attn_v" in component_lower:
        return "value"
    elif "output projection" in component_lower:
        return "output"
    elif "ffn gate" in component_lower or "ffn_gate" in component_lower:
        return "gate"
    elif "ffn up" in component_lower or "ffn_up" in component_lower:
        return "up"
    elif "ffn down" in component_lower or "ffn_down" in component_lower:
        return "down"
    elif "norm" in component_lower:
        return "norm"
    else:
        return "other"


# NOTE: calculate_gguf_data_offset() function removed.
# The fixed llama-gguf-dump tool now outputs ABSOLUTE offsets (includes data section offset).
# No need to calculate or add offset here - use CSV values directly.


def parse_csv_to_memory_map(csv_path: str, model_name: str = None) -> Dict[str, Any]:
    """
    Parse CSV file and generate memory map structure.

    NOTE: Expects CSV from fixed llama-gguf-dump which outputs ABSOLUTE offsets.
    No offset adjustment needed - CSV values are already absolute file positions.

    Args:
        csv_path: Path to CSV file from llama-gguf-dump
        model_name: Optional model name (inferred from CSV filename if not provided)

    Returns:
        Dictionary containing memory map data
    """
    if model_name is None:
        # Infer model name from CSV filename
        model_name = os.path.splitext(os.path.basename(csv_path))[0].replace('_structure', '')

    tensors = []
    total_size = 0
    n_layers = 0
    n_vocab = 0
    n_embd = 0

    with open(csv_path, 'r') as f:
        # Skip gguf-dump header lines until we find the CSV header
        # The CSV header always starts with "tensor_name,"
        # (Different gguf-dump versions have 3-5 header lines)
        while True:
            pos = f.tell()
            line = f.readline()
            if line.startswith('tensor_name,'):
                # Found CSV header - seek back so DictReader reads it as header
                f.seek(pos)
                break
            if not line:
                raise ValueError("CSV header ('tensor_name,...') not found in file")

        reader = csv.DictReader(f)

        for row_num, row in enumerate(reader, start=1):
            # Skip empty or invalid rows
            if not row or not row.get('tensor_name'):
                continue

            # Parse row with error handling
            try:
                tensor_name = row['tensor_name']
                file_offset = int(row['file_offset'])
                size_bytes = int(row['size_bytes'])
                layer_id = int(row['layer_id']) if row['layer_id'] != '-1' else None
                component_type = row['component_type']
                n_dims = int(row['n_dims'])

                # Parse shape
                shape = []
                for i in range(n_dims):
                    dim_val = int(row[f'dim{i}'])
                    if dim_val > 0:
                        shape.append(dim_val)

                # Extract metadata
                category = categorize_tensor(component_type, tensor_name)
                component = get_component_name(component_type)

                # Extract expert ID if this is an expert tensor (name ends with [N])
                expert_id = None
                if '[' in tensor_name and tensor_name.endswith(']'):
                    try:
                        expert_id = int(tensor_name.split('[')[-1].rstrip(']'))
                    except ValueError:
                        pass  # Not an expert tensor
            except (ValueError, TypeError, KeyError) as e:
                print(f"Warning: Skipping row {row_num} due to parse error: {e}")
                print(f"  Row data: {dict(list(row.items())[:5])}")  # Show first 5 fields
                continue

            # Update global stats
            total_size = max(total_size, file_offset + size_bytes)
            if layer_id is not None:
                n_layers = max(n_layers, layer_id + 1)

            # Infer n_vocab and n_embd from embeddings
            if "token_embd" in tensor_name.lower():
                if len(shape) >= 2:
                    n_embd = shape[0]
                    n_vocab = shape[1]

            # Create tensor entry
            # NOTE: file_offset from fixed gguf-dump is ALREADY ABSOLUTE
            # No offset adjustment needed
            tensor_entry = {
                "name": tensor_name,
                "offset_start": file_offset,
                "offset_end": file_offset + size_bytes,
                "size_bytes": size_bytes,
                "shape": shape,
                "category": category,
                "layer_id": layer_id,
                "component": component,
                "component_type": component_type  # Keep original for reference
            }

            # Add expert_id if present (for MoE expert tensors)
            if expert_id is not None:
                tensor_entry["expert_id"] = expert_id

            tensors.append(tensor_entry)

    # Sort tensors by offset (should already be sorted, but just in case)
    tensors.sort(key=lambda t: t['offset_start'])

    # Build memory map
    memory_map = {
        "model_name": model_name,
        "total_size_bytes": total_size,
        "metadata": {
            "n_layers": n_layers,
            "n_vocab": n_vocab,
            "n_embd": n_embd,
            "n_tensors": len(tensors)
        },
        "tensors": tensors
    }

    return memory_map


def main():
    parser = argparse.ArgumentParser(
        description='Parse model structure CSV to generate memory-map.json with corrected absolute offsets'
    )
    parser.add_argument(
        '--csv',
        required=True,
        help='Path to CSV file from llama-gguf-dump (e.g., tinyllama_structure.csv)'
    )
    parser.add_argument(
        '--gguf-file',
        required=False,
        help='[DEPRECATED] Path to original GGUF file (no longer needed - offsets are absolute)'
    )
    parser.add_argument(
        '--output',
        required=True,
        help='Path to output JSON file (e.g., data/memory-map.json)'
    )
    parser.add_argument(
        '--model-name',
        help='Model name (optional, inferred from CSV filename if not provided)'
    )
    parser.add_argument(
        '--pretty',
        action='store_true',
        help='Pretty-print JSON output'
    )

    args = parser.parse_args()

    # Parse CSV (offsets are already absolute from fixed gguf-dump)
    print(f"Reading CSV: {args.csv}")
    print("Note: Using absolute offsets from CSV (no adjustment needed)")
    memory_map = parse_csv_to_memory_map(args.csv, args.model_name)

    print(f"Parsed {memory_map['metadata']['n_tensors']} tensors")
    print(f"Model: {memory_map['model_name']}")
    print(f"Layers: {memory_map['metadata']['n_layers']}")
    print(f"Total size: {memory_map['total_size_bytes'] / (1024**3):.2f} GB")

    # Create output directory if needed
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created directory: {output_dir}")

    # Write JSON
    indent = 2 if args.pretty else None
    with open(args.output, 'w') as f:
        json.dump(memory_map, f, indent=indent)

    print(f"âœ“ Memory map written to: {args.output}")
    print(f"  File size: {os.path.getsize(args.output) / 1024:.1f} KB")


if __name__ == '__main__':
    main()
