{"model_name": "temp_model", "total_size_bytes": 13779630336, "metadata": {"n_layers": 24, "n_vocab": 201088, "n_embd": 2880, "n_tensors": 459}, "tensors": [{"name": "blk.0.ffn_down_exps.weight", "offset_start": 13008832, "offset_end": 1074692032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 0, "component": "down", "component_type": "FFN Down"}, {"name": "blk.0.ffn_gate_exps.weight", "offset_start": 154013632, "offset_end": 1215696832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 0, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.0.ffn_up_exps.weight", "offset_start": 295018432, "offset_end": 1356701632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 0, "component": "up", "component_type": "FFN Up"}, {"name": "blk.1.ffn_down_exps.weight", "offset_start": 436023232, "offset_end": 1497706432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 1, "component": "down", "component_type": "FFN Down"}, {"name": "blk.1.ffn_gate_exps.weight", "offset_start": 577028032, "offset_end": 1638711232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 1, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.1.ffn_up_exps.weight", "offset_start": 718032832, "offset_end": 1779716032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 1, "component": "up", "component_type": "FFN Up"}, {"name": "blk.10.ffn_down_exps.weight", "offset_start": 859037632, "offset_end": 1920720832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 10, "component": "down", "component_type": "FFN Down"}, {"name": "blk.10.ffn_gate_exps.weight", "offset_start": 1000042432, "offset_end": 2061725632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 10, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.10.ffn_up_exps.weight", "offset_start": 1141047232, "offset_end": 2202730432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 10, "component": "up", "component_type": "FFN Up"}, {"name": "blk.11.ffn_down_exps.weight", "offset_start": 1282052032, "offset_end": 2343735232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 11, "component": "down", "component_type": "FFN Down"}, {"name": "blk.11.ffn_gate_exps.weight", "offset_start": 1423056832, "offset_end": 2484740032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 11, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.11.ffn_up_exps.weight", "offset_start": 1564061632, "offset_end": 2625744832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 11, "component": "up", "component_type": "FFN Up"}, {"name": "blk.12.ffn_down_exps.weight", "offset_start": 1705066432, "offset_end": 2766749632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 12, "component": "down", "component_type": "FFN Down"}, {"name": "blk.12.ffn_gate_exps.weight", "offset_start": 1846071232, "offset_end": 2907754432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 12, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.12.ffn_up_exps.weight", "offset_start": 1987076032, "offset_end": 3048759232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 12, "component": "up", "component_type": "FFN Up"}, {"name": "blk.13.ffn_down_exps.weight", "offset_start": 2128080832, "offset_end": 3189764032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 13, "component": "down", "component_type": "FFN Down"}, {"name": "blk.13.ffn_gate_exps.weight", "offset_start": 2269085632, "offset_end": 3330768832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 13, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.13.ffn_up_exps.weight", "offset_start": 2410090432, "offset_end": 3471773632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 13, "component": "up", "component_type": "FFN Up"}, {"name": "blk.14.ffn_down_exps.weight", "offset_start": 2551095232, "offset_end": 3612778432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 14, "component": "down", "component_type": "FFN Down"}, {"name": "blk.14.ffn_gate_exps.weight", "offset_start": 2692100032, "offset_end": 3753783232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 14, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.14.ffn_up_exps.weight", "offset_start": 2833104832, "offset_end": 3894788032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 14, "component": "up", "component_type": "FFN Up"}, {"name": "blk.15.ffn_down_exps.weight", "offset_start": 2974109632, "offset_end": 4035792832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 15, "component": "down", "component_type": "FFN Down"}, {"name": "blk.15.ffn_gate_exps.weight", "offset_start": 3115114432, "offset_end": 4176797632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 15, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.15.ffn_up_exps.weight", "offset_start": 3256119232, "offset_end": 4317802432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 15, "component": "up", "component_type": "FFN Up"}, {"name": "blk.16.ffn_down_exps.weight", "offset_start": 3397124032, "offset_end": 4458807232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 16, "component": "down", "component_type": "FFN Down"}, {"name": "blk.16.ffn_gate_exps.weight", "offset_start": 3538128832, "offset_end": 4599812032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 16, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.16.ffn_up_exps.weight", "offset_start": 3679133632, "offset_end": 4740816832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 16, "component": "up", "component_type": "FFN Up"}, {"name": "blk.17.ffn_down_exps.weight", "offset_start": 3820138432, "offset_end": 4881821632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 17, "component": "down", "component_type": "FFN Down"}, {"name": "blk.17.ffn_gate_exps.weight", "offset_start": 3961143232, "offset_end": 5022826432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 17, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.17.ffn_up_exps.weight", "offset_start": 4102148032, "offset_end": 5163831232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 17, "component": "up", "component_type": "FFN Up"}, {"name": "blk.18.ffn_down_exps.weight", "offset_start": 4243152832, "offset_end": 5304836032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 18, "component": "down", "component_type": "FFN Down"}, {"name": "blk.18.ffn_gate_exps.weight", "offset_start": 4384157632, "offset_end": 5445840832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 18, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.18.ffn_up_exps.weight", "offset_start": 4525162432, "offset_end": 5586845632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 18, "component": "up", "component_type": "FFN Up"}, {"name": "blk.19.ffn_down_exps.weight", "offset_start": 4666167232, "offset_end": 5727850432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 19, "component": "down", "component_type": "FFN Down"}, {"name": "blk.19.ffn_gate_exps.weight", "offset_start": 4807172032, "offset_end": 5868855232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 19, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.19.ffn_up_exps.weight", "offset_start": 4948176832, "offset_end": 6009860032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 19, "component": "up", "component_type": "FFN Up"}, {"name": "blk.2.ffn_down_exps.weight", "offset_start": 5089181632, "offset_end": 6150864832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 2, "component": "down", "component_type": "FFN Down"}, {"name": "blk.2.ffn_gate_exps.weight", "offset_start": 5230186432, "offset_end": 6291869632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 2, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.2.ffn_up_exps.weight", "offset_start": 5371191232, "offset_end": 6432874432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 2, "component": "up", "component_type": "FFN Up"}, {"name": "blk.20.ffn_down_exps.weight", "offset_start": 5512196032, "offset_end": 6573879232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 20, "component": "down", "component_type": "FFN Down"}, {"name": "blk.20.ffn_gate_exps.weight", "offset_start": 5653200832, "offset_end": 6714884032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 20, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.20.ffn_up_exps.weight", "offset_start": 5794205632, "offset_end": 6855888832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 20, "component": "up", "component_type": "FFN Up"}, {"name": "blk.21.ffn_down_exps.weight", "offset_start": 5935210432, "offset_end": 6996893632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 21, "component": "down", "component_type": "FFN Down"}, {"name": "blk.21.ffn_gate_exps.weight", "offset_start": 6076215232, "offset_end": 7137898432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 21, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.21.ffn_up_exps.weight", "offset_start": 6217220032, "offset_end": 7278903232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 21, "component": "up", "component_type": "FFN Up"}, {"name": "blk.22.ffn_down_exps.weight", "offset_start": 6358224832, "offset_end": 7419908032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 22, "component": "down", "component_type": "FFN Down"}, {"name": "blk.22.ffn_gate_exps.weight", "offset_start": 6499229632, "offset_end": 7560912832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 22, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.22.ffn_up_exps.weight", "offset_start": 6640234432, "offset_end": 7701917632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 22, "component": "up", "component_type": "FFN Up"}, {"name": "blk.23.ffn_down_exps.weight", "offset_start": 6781239232, "offset_end": 7842922432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 23, "component": "down", "component_type": "FFN Down"}, {"name": "blk.23.ffn_gate_exps.weight", "offset_start": 6922244032, "offset_end": 7983927232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 23, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.23.ffn_up_exps.weight", "offset_start": 7063248832, "offset_end": 8124932032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 23, "component": "up", "component_type": "FFN Up"}, {"name": "blk.3.ffn_down_exps.weight", "offset_start": 7204253632, "offset_end": 8265936832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 3, "component": "down", "component_type": "FFN Down"}, {"name": "blk.3.ffn_gate_exps.weight", "offset_start": 7345258432, "offset_end": 8406941632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 3, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.3.ffn_up_exps.weight", "offset_start": 7486263232, "offset_end": 8547946432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 3, "component": "up", "component_type": "FFN Up"}, {"name": "blk.4.ffn_down_exps.weight", "offset_start": 7627268032, "offset_end": 8688951232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 4, "component": "down", "component_type": "FFN Down"}, {"name": "blk.4.ffn_gate_exps.weight", "offset_start": 7768272832, "offset_end": 8829956032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 4, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.4.ffn_up_exps.weight", "offset_start": 7909277632, "offset_end": 8970960832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 4, "component": "up", "component_type": "FFN Up"}, {"name": "blk.5.ffn_down_exps.weight", "offset_start": 8050282432, "offset_end": 9111965632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 5, "component": "down", "component_type": "FFN Down"}, {"name": "blk.5.ffn_gate_exps.weight", "offset_start": 8191287232, "offset_end": 9252970432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 5, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.5.ffn_up_exps.weight", "offset_start": 8332292032, "offset_end": 9393975232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 5, "component": "up", "component_type": "FFN Up"}, {"name": "blk.6.ffn_down_exps.weight", "offset_start": 8473296832, "offset_end": 9534980032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 6, "component": "down", "component_type": "FFN Down"}, {"name": "blk.6.ffn_gate_exps.weight", "offset_start": 8614301632, "offset_end": 9675984832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 6, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.6.ffn_up_exps.weight", "offset_start": 8755306432, "offset_end": 9816989632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 6, "component": "up", "component_type": "FFN Up"}, {"name": "blk.7.ffn_down_exps.weight", "offset_start": 8896311232, "offset_end": 9957994432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 7, "component": "down", "component_type": "FFN Down"}, {"name": "blk.7.ffn_gate_exps.weight", "offset_start": 9037316032, "offset_end": 10098999232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 7, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.7.ffn_up_exps.weight", "offset_start": 9178320832, "offset_end": 10240004032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 7, "component": "up", "component_type": "FFN Up"}, {"name": "blk.8.ffn_down_exps.weight", "offset_start": 9319325632, "offset_end": 10381008832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 8, "component": "down", "component_type": "FFN Down"}, {"name": "blk.8.ffn_gate_exps.weight", "offset_start": 9460330432, "offset_end": 10522013632, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 8, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.8.ffn_up_exps.weight", "offset_start": 9601335232, "offset_end": 10663018432, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 8, "component": "up", "component_type": "FFN Up"}, {"name": "blk.9.ffn_down_exps.weight", "offset_start": 9742340032, "offset_end": 10804023232, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 9, "component": "down", "component_type": "FFN Down"}, {"name": "blk.9.ffn_gate_exps.weight", "offset_start": 9883344832, "offset_end": 10945028032, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 9, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.9.ffn_up_exps.weight", "offset_start": 10024349632, "offset_end": 11086032832, "size_bytes": 1061683200, "shape": [2880, 2880, 32], "category": "ffn", "layer_id": 9, "component": "up", "component_type": "FFN Up"}, {"name": "blk.0.attn_norm.weight", "offset_start": 10165354432, "offset_end": 10165365952, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 0, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.0.ffn_down_exps.bias", "offset_start": 10165365952, "offset_end": 10165734592, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 0, "component": "down", "component_type": "FFN Down"}, {"name": "blk.0.ffn_gate_exps.bias", "offset_start": 10165734592, "offset_end": 10166103232, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 0, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.0.ffn_up_exps.bias", "offset_start": 10166103232, "offset_end": 10166471872, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 0, "component": "up", "component_type": "FFN Up"}, {"name": "blk.0.ffn_gate_inp.bias", "offset_start": 10166471872, "offset_end": 10166472000, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 0, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.0.ffn_gate_inp.weight", "offset_start": 10166472000, "offset_end": 10166840640, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 0, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.0.post_attention_norm.weight", "offset_start": 10166840640, "offset_end": 10166852160, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 0, "component": "other", "component_type": "Other"}, {"name": "blk.0.attn_k.bias", "offset_start": 10166852160, "offset_end": 10166854208, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 0, "component": "key", "component_type": "Attention K"}, {"name": "blk.0.attn_k.weight", "offset_start": 10166854208, "offset_end": 10169803328, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 0, "component": "key", "component_type": "Attention K"}, {"name": "blk.0.attn_output.bias", "offset_start": 10169803328, "offset_end": 10169814848, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 0, "component": "output", "component_type": "Output Projection"}, {"name": "blk.0.attn_output.weight", "offset_start": 10169814848, "offset_end": 10193407808, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 0, "component": "output", "component_type": "Output Projection"}, {"name": "blk.0.attn_q.bias", "offset_start": 10193407808, "offset_end": 10193424192, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 0, "component": "query", "component_type": "Attention Q"}, {"name": "blk.0.attn_q.weight", "offset_start": 10193424192, "offset_end": 10217017152, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 0, "component": "query", "component_type": "Attention Q"}, {"name": "blk.0.attn_sinks.weight", "offset_start": 10217017152, "offset_end": 10217017408, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 0, "component": "other", "component_type": "Other"}, {"name": "blk.0.attn_v.bias", "offset_start": 10217017408, "offset_end": 10217019456, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 0, "component": "value", "component_type": "Attention V"}, {"name": "blk.0.attn_v.weight", "offset_start": 10217019456, "offset_end": 10219968576, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 0, "component": "value", "component_type": "Attention V"}, {"name": "blk.1.attn_norm.weight", "offset_start": 10219968576, "offset_end": 10219980096, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 1, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.1.ffn_down_exps.bias", "offset_start": 10219980096, "offset_end": 10220348736, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 1, "component": "down", "component_type": "FFN Down"}, {"name": "blk.1.ffn_gate_exps.bias", "offset_start": 10220348736, "offset_end": 10220717376, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 1, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.1.ffn_up_exps.bias", "offset_start": 10220717376, "offset_end": 10221086016, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 1, "component": "up", "component_type": "FFN Up"}, {"name": "blk.1.ffn_gate_inp.bias", "offset_start": 10221086016, "offset_end": 10221086144, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 1, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.1.ffn_gate_inp.weight", "offset_start": 10221086144, "offset_end": 10221454784, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 1, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.1.post_attention_norm.weight", "offset_start": 10221454784, "offset_end": 10221466304, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 1, "component": "other", "component_type": "Other"}, {"name": "blk.1.attn_k.bias", "offset_start": 10221466304, "offset_end": 10221468352, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 1, "component": "key", "component_type": "Attention K"}, {"name": "blk.1.attn_k.weight", "offset_start": 10221468352, "offset_end": 10224417472, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 1, "component": "key", "component_type": "Attention K"}, {"name": "blk.1.attn_output.bias", "offset_start": 10224417472, "offset_end": 10224428992, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 1, "component": "output", "component_type": "Output Projection"}, {"name": "blk.1.attn_output.weight", "offset_start": 10224428992, "offset_end": 10248021952, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 1, "component": "output", "component_type": "Output Projection"}, {"name": "blk.1.attn_q.bias", "offset_start": 10248021952, "offset_end": 10248038336, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 1, "component": "query", "component_type": "Attention Q"}, {"name": "blk.1.attn_q.weight", "offset_start": 10248038336, "offset_end": 10271631296, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 1, "component": "query", "component_type": "Attention Q"}, {"name": "blk.1.attn_sinks.weight", "offset_start": 10271631296, "offset_end": 10271631552, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 1, "component": "other", "component_type": "Other"}, {"name": "blk.1.attn_v.bias", "offset_start": 10271631552, "offset_end": 10271633600, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 1, "component": "value", "component_type": "Attention V"}, {"name": "blk.1.attn_v.weight", "offset_start": 10271633600, "offset_end": 10274582720, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 1, "component": "value", "component_type": "Attention V"}, {"name": "blk.10.attn_norm.weight", "offset_start": 10274582720, "offset_end": 10274594240, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 10, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.10.ffn_down_exps.bias", "offset_start": 10274594240, "offset_end": 10274962880, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 10, "component": "down", "component_type": "FFN Down"}, {"name": "blk.10.ffn_gate_exps.bias", "offset_start": 10274962880, "offset_end": 10275331520, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 10, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.10.ffn_up_exps.bias", "offset_start": 10275331520, "offset_end": 10275700160, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 10, "component": "up", "component_type": "FFN Up"}, {"name": "blk.10.ffn_gate_inp.bias", "offset_start": 10275700160, "offset_end": 10275700288, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 10, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.10.ffn_gate_inp.weight", "offset_start": 10275700288, "offset_end": 10276068928, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 10, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.10.post_attention_norm.weight", "offset_start": 10276068928, "offset_end": 10276080448, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 10, "component": "other", "component_type": "Other"}, {"name": "blk.10.attn_k.bias", "offset_start": 10276080448, "offset_end": 10276082496, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 10, "component": "key", "component_type": "Attention K"}, {"name": "blk.10.attn_k.weight", "offset_start": 10276082496, "offset_end": 10279031616, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 10, "component": "key", "component_type": "Attention K"}, {"name": "blk.10.attn_output.bias", "offset_start": 10279031616, "offset_end": 10279043136, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 10, "component": "output", "component_type": "Output Projection"}, {"name": "blk.10.attn_output.weight", "offset_start": 10279043136, "offset_end": 10302636096, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 10, "component": "output", "component_type": "Output Projection"}, {"name": "blk.10.attn_q.bias", "offset_start": 10302636096, "offset_end": 10302652480, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 10, "component": "query", "component_type": "Attention Q"}, {"name": "blk.10.attn_q.weight", "offset_start": 10302652480, "offset_end": 10326245440, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 10, "component": "query", "component_type": "Attention Q"}, {"name": "blk.10.attn_sinks.weight", "offset_start": 10326245440, "offset_end": 10326245696, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 10, "component": "other", "component_type": "Other"}, {"name": "blk.10.attn_v.bias", "offset_start": 10326245696, "offset_end": 10326247744, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 10, "component": "value", "component_type": "Attention V"}, {"name": "blk.10.attn_v.weight", "offset_start": 10326247744, "offset_end": 10329196864, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 10, "component": "value", "component_type": "Attention V"}, {"name": "blk.11.attn_norm.weight", "offset_start": 10329196864, "offset_end": 10329208384, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 11, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.11.ffn_down_exps.bias", "offset_start": 10329208384, "offset_end": 10329577024, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 11, "component": "down", "component_type": "FFN Down"}, {"name": "blk.11.ffn_gate_exps.bias", "offset_start": 10329577024, "offset_end": 10329945664, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 11, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.11.ffn_up_exps.bias", "offset_start": 10329945664, "offset_end": 10330314304, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 11, "component": "up", "component_type": "FFN Up"}, {"name": "blk.11.ffn_gate_inp.bias", "offset_start": 10330314304, "offset_end": 10330314432, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 11, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.11.ffn_gate_inp.weight", "offset_start": 10330314432, "offset_end": 10330683072, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 11, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.11.post_attention_norm.weight", "offset_start": 10330683072, "offset_end": 10330694592, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 11, "component": "other", "component_type": "Other"}, {"name": "blk.11.attn_k.bias", "offset_start": 10330694592, "offset_end": 10330696640, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 11, "component": "key", "component_type": "Attention K"}, {"name": "blk.11.attn_k.weight", "offset_start": 10330696640, "offset_end": 10333645760, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 11, "component": "key", "component_type": "Attention K"}, {"name": "blk.11.attn_output.bias", "offset_start": 10333645760, "offset_end": 10333657280, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 11, "component": "output", "component_type": "Output Projection"}, {"name": "blk.11.attn_output.weight", "offset_start": 10333657280, "offset_end": 10357250240, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 11, "component": "output", "component_type": "Output Projection"}, {"name": "blk.11.attn_q.bias", "offset_start": 10357250240, "offset_end": 10357266624, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 11, "component": "query", "component_type": "Attention Q"}, {"name": "blk.11.attn_q.weight", "offset_start": 10357266624, "offset_end": 10380859584, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 11, "component": "query", "component_type": "Attention Q"}, {"name": "blk.11.attn_sinks.weight", "offset_start": 10380859584, "offset_end": 10380859840, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 11, "component": "other", "component_type": "Other"}, {"name": "blk.11.attn_v.bias", "offset_start": 10380859840, "offset_end": 10380861888, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 11, "component": "value", "component_type": "Attention V"}, {"name": "blk.11.attn_v.weight", "offset_start": 10380861888, "offset_end": 10383811008, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 11, "component": "value", "component_type": "Attention V"}, {"name": "blk.12.attn_norm.weight", "offset_start": 10383811008, "offset_end": 10383822528, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 12, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.12.ffn_down_exps.bias", "offset_start": 10383822528, "offset_end": 10384191168, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 12, "component": "down", "component_type": "FFN Down"}, {"name": "blk.12.ffn_gate_exps.bias", "offset_start": 10384191168, "offset_end": 10384559808, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 12, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.12.ffn_up_exps.bias", "offset_start": 10384559808, "offset_end": 10384928448, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 12, "component": "up", "component_type": "FFN Up"}, {"name": "blk.12.ffn_gate_inp.bias", "offset_start": 10384928448, "offset_end": 10384928576, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 12, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.12.ffn_gate_inp.weight", "offset_start": 10384928576, "offset_end": 10385297216, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 12, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.12.post_attention_norm.weight", "offset_start": 10385297216, "offset_end": 10385308736, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 12, "component": "other", "component_type": "Other"}, {"name": "blk.12.attn_k.bias", "offset_start": 10385308736, "offset_end": 10385310784, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 12, "component": "key", "component_type": "Attention K"}, {"name": "blk.12.attn_k.weight", "offset_start": 10385310784, "offset_end": 10388259904, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 12, "component": "key", "component_type": "Attention K"}, {"name": "blk.12.attn_output.bias", "offset_start": 10388259904, "offset_end": 10388271424, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 12, "component": "output", "component_type": "Output Projection"}, {"name": "blk.12.attn_output.weight", "offset_start": 10388271424, "offset_end": 10411864384, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 12, "component": "output", "component_type": "Output Projection"}, {"name": "blk.12.attn_q.bias", "offset_start": 10411864384, "offset_end": 10411880768, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 12, "component": "query", "component_type": "Attention Q"}, {"name": "blk.12.attn_q.weight", "offset_start": 10411880768, "offset_end": 10435473728, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 12, "component": "query", "component_type": "Attention Q"}, {"name": "blk.12.attn_sinks.weight", "offset_start": 10435473728, "offset_end": 10435473984, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 12, "component": "other", "component_type": "Other"}, {"name": "blk.12.attn_v.bias", "offset_start": 10435473984, "offset_end": 10435476032, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 12, "component": "value", "component_type": "Attention V"}, {"name": "blk.12.attn_v.weight", "offset_start": 10435476032, "offset_end": 10438425152, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 12, "component": "value", "component_type": "Attention V"}, {"name": "blk.13.attn_norm.weight", "offset_start": 10438425152, "offset_end": 10438436672, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 13, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.13.ffn_down_exps.bias", "offset_start": 10438436672, "offset_end": 10438805312, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 13, "component": "down", "component_type": "FFN Down"}, {"name": "blk.13.ffn_gate_exps.bias", "offset_start": 10438805312, "offset_end": 10439173952, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 13, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.13.ffn_up_exps.bias", "offset_start": 10439173952, "offset_end": 10439542592, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 13, "component": "up", "component_type": "FFN Up"}, {"name": "blk.13.ffn_gate_inp.bias", "offset_start": 10439542592, "offset_end": 10439542720, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 13, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.13.ffn_gate_inp.weight", "offset_start": 10439542720, "offset_end": 10439911360, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 13, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.13.post_attention_norm.weight", "offset_start": 10439911360, "offset_end": 10439922880, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 13, "component": "other", "component_type": "Other"}, {"name": "blk.13.attn_k.bias", "offset_start": 10439922880, "offset_end": 10439924928, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 13, "component": "key", "component_type": "Attention K"}, {"name": "blk.13.attn_k.weight", "offset_start": 10439924928, "offset_end": 10442874048, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 13, "component": "key", "component_type": "Attention K"}, {"name": "blk.13.attn_output.bias", "offset_start": 10442874048, "offset_end": 10442885568, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 13, "component": "output", "component_type": "Output Projection"}, {"name": "blk.13.attn_output.weight", "offset_start": 10442885568, "offset_end": 10466478528, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 13, "component": "output", "component_type": "Output Projection"}, {"name": "blk.13.attn_q.bias", "offset_start": 10466478528, "offset_end": 10466494912, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 13, "component": "query", "component_type": "Attention Q"}, {"name": "blk.13.attn_q.weight", "offset_start": 10466494912, "offset_end": 10490087872, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 13, "component": "query", "component_type": "Attention Q"}, {"name": "blk.13.attn_sinks.weight", "offset_start": 10490087872, "offset_end": 10490088128, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 13, "component": "other", "component_type": "Other"}, {"name": "blk.13.attn_v.bias", "offset_start": 10490088128, "offset_end": 10490090176, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 13, "component": "value", "component_type": "Attention V"}, {"name": "blk.13.attn_v.weight", "offset_start": 10490090176, "offset_end": 10493039296, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 13, "component": "value", "component_type": "Attention V"}, {"name": "blk.14.attn_norm.weight", "offset_start": 10493039296, "offset_end": 10493050816, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 14, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.14.ffn_down_exps.bias", "offset_start": 10493050816, "offset_end": 10493419456, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 14, "component": "down", "component_type": "FFN Down"}, {"name": "blk.14.ffn_gate_exps.bias", "offset_start": 10493419456, "offset_end": 10493788096, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 14, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.14.ffn_up_exps.bias", "offset_start": 10493788096, "offset_end": 10494156736, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 14, "component": "up", "component_type": "FFN Up"}, {"name": "blk.14.ffn_gate_inp.bias", "offset_start": 10494156736, "offset_end": 10494156864, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 14, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.14.ffn_gate_inp.weight", "offset_start": 10494156864, "offset_end": 10494525504, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 14, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.14.post_attention_norm.weight", "offset_start": 10494525504, "offset_end": 10494537024, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 14, "component": "other", "component_type": "Other"}, {"name": "blk.14.attn_k.bias", "offset_start": 10494537024, "offset_end": 10494539072, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 14, "component": "key", "component_type": "Attention K"}, {"name": "blk.14.attn_k.weight", "offset_start": 10494539072, "offset_end": 10497488192, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 14, "component": "key", "component_type": "Attention K"}, {"name": "blk.14.attn_output.bias", "offset_start": 10497488192, "offset_end": 10497499712, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 14, "component": "output", "component_type": "Output Projection"}, {"name": "blk.14.attn_output.weight", "offset_start": 10497499712, "offset_end": 10521092672, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 14, "component": "output", "component_type": "Output Projection"}, {"name": "blk.14.attn_q.bias", "offset_start": 10521092672, "offset_end": 10521109056, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 14, "component": "query", "component_type": "Attention Q"}, {"name": "blk.14.attn_q.weight", "offset_start": 10521109056, "offset_end": 10544702016, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 14, "component": "query", "component_type": "Attention Q"}, {"name": "blk.14.attn_sinks.weight", "offset_start": 10544702016, "offset_end": 10544702272, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 14, "component": "other", "component_type": "Other"}, {"name": "blk.14.attn_v.bias", "offset_start": 10544702272, "offset_end": 10544704320, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 14, "component": "value", "component_type": "Attention V"}, {"name": "blk.14.attn_v.weight", "offset_start": 10544704320, "offset_end": 10547653440, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 14, "component": "value", "component_type": "Attention V"}, {"name": "blk.15.attn_norm.weight", "offset_start": 10547653440, "offset_end": 10547664960, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 15, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.15.ffn_down_exps.bias", "offset_start": 10547664960, "offset_end": 10548033600, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 15, "component": "down", "component_type": "FFN Down"}, {"name": "blk.15.ffn_gate_exps.bias", "offset_start": 10548033600, "offset_end": 10548402240, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 15, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.15.ffn_up_exps.bias", "offset_start": 10548402240, "offset_end": 10548770880, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 15, "component": "up", "component_type": "FFN Up"}, {"name": "blk.15.ffn_gate_inp.bias", "offset_start": 10548770880, "offset_end": 10548771008, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 15, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.15.ffn_gate_inp.weight", "offset_start": 10548771008, "offset_end": 10549139648, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 15, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.15.post_attention_norm.weight", "offset_start": 10549139648, "offset_end": 10549151168, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 15, "component": "other", "component_type": "Other"}, {"name": "blk.15.attn_k.bias", "offset_start": 10549151168, "offset_end": 10549153216, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 15, "component": "key", "component_type": "Attention K"}, {"name": "blk.15.attn_k.weight", "offset_start": 10549153216, "offset_end": 10552102336, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 15, "component": "key", "component_type": "Attention K"}, {"name": "blk.15.attn_output.bias", "offset_start": 10552102336, "offset_end": 10552113856, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 15, "component": "output", "component_type": "Output Projection"}, {"name": "blk.15.attn_output.weight", "offset_start": 10552113856, "offset_end": 10575706816, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 15, "component": "output", "component_type": "Output Projection"}, {"name": "blk.15.attn_q.bias", "offset_start": 10575706816, "offset_end": 10575723200, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 15, "component": "query", "component_type": "Attention Q"}, {"name": "blk.15.attn_q.weight", "offset_start": 10575723200, "offset_end": 10599316160, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 15, "component": "query", "component_type": "Attention Q"}, {"name": "blk.15.attn_sinks.weight", "offset_start": 10599316160, "offset_end": 10599316416, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 15, "component": "other", "component_type": "Other"}, {"name": "blk.15.attn_v.bias", "offset_start": 10599316416, "offset_end": 10599318464, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 15, "component": "value", "component_type": "Attention V"}, {"name": "blk.15.attn_v.weight", "offset_start": 10599318464, "offset_end": 10602267584, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 15, "component": "value", "component_type": "Attention V"}, {"name": "blk.16.attn_norm.weight", "offset_start": 10602267584, "offset_end": 10602279104, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 16, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.16.ffn_down_exps.bias", "offset_start": 10602279104, "offset_end": 10602647744, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 16, "component": "down", "component_type": "FFN Down"}, {"name": "blk.16.ffn_gate_exps.bias", "offset_start": 10602647744, "offset_end": 10603016384, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 16, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.16.ffn_up_exps.bias", "offset_start": 10603016384, "offset_end": 10603385024, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 16, "component": "up", "component_type": "FFN Up"}, {"name": "blk.16.ffn_gate_inp.bias", "offset_start": 10603385024, "offset_end": 10603385152, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 16, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.16.ffn_gate_inp.weight", "offset_start": 10603385152, "offset_end": 10603753792, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 16, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.16.post_attention_norm.weight", "offset_start": 10603753792, "offset_end": 10603765312, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 16, "component": "other", "component_type": "Other"}, {"name": "blk.16.attn_k.bias", "offset_start": 10603765312, "offset_end": 10603767360, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 16, "component": "key", "component_type": "Attention K"}, {"name": "blk.16.attn_k.weight", "offset_start": 10603767360, "offset_end": 10606716480, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 16, "component": "key", "component_type": "Attention K"}, {"name": "blk.16.attn_output.bias", "offset_start": 10606716480, "offset_end": 10606728000, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 16, "component": "output", "component_type": "Output Projection"}, {"name": "blk.16.attn_output.weight", "offset_start": 10606728000, "offset_end": 10630320960, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 16, "component": "output", "component_type": "Output Projection"}, {"name": "blk.16.attn_q.bias", "offset_start": 10630320960, "offset_end": 10630337344, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 16, "component": "query", "component_type": "Attention Q"}, {"name": "blk.16.attn_q.weight", "offset_start": 10630337344, "offset_end": 10653930304, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 16, "component": "query", "component_type": "Attention Q"}, {"name": "blk.16.attn_sinks.weight", "offset_start": 10653930304, "offset_end": 10653930560, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 16, "component": "other", "component_type": "Other"}, {"name": "blk.16.attn_v.bias", "offset_start": 10653930560, "offset_end": 10653932608, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 16, "component": "value", "component_type": "Attention V"}, {"name": "blk.16.attn_v.weight", "offset_start": 10653932608, "offset_end": 10656881728, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 16, "component": "value", "component_type": "Attention V"}, {"name": "blk.17.attn_norm.weight", "offset_start": 10656881728, "offset_end": 10656893248, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 17, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.17.ffn_down_exps.bias", "offset_start": 10656893248, "offset_end": 10657261888, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 17, "component": "down", "component_type": "FFN Down"}, {"name": "blk.17.ffn_gate_exps.bias", "offset_start": 10657261888, "offset_end": 10657630528, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 17, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.17.ffn_up_exps.bias", "offset_start": 10657630528, "offset_end": 10657999168, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 17, "component": "up", "component_type": "FFN Up"}, {"name": "blk.17.ffn_gate_inp.bias", "offset_start": 10657999168, "offset_end": 10657999296, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 17, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.17.ffn_gate_inp.weight", "offset_start": 10657999296, "offset_end": 10658367936, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 17, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.17.post_attention_norm.weight", "offset_start": 10658367936, "offset_end": 10658379456, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 17, "component": "other", "component_type": "Other"}, {"name": "blk.17.attn_k.bias", "offset_start": 10658379456, "offset_end": 10658381504, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 17, "component": "key", "component_type": "Attention K"}, {"name": "blk.17.attn_k.weight", "offset_start": 10658381504, "offset_end": 10661330624, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 17, "component": "key", "component_type": "Attention K"}, {"name": "blk.17.attn_output.bias", "offset_start": 10661330624, "offset_end": 10661342144, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 17, "component": "output", "component_type": "Output Projection"}, {"name": "blk.17.attn_output.weight", "offset_start": 10661342144, "offset_end": 10684935104, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 17, "component": "output", "component_type": "Output Projection"}, {"name": "blk.17.attn_q.bias", "offset_start": 10684935104, "offset_end": 10684951488, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 17, "component": "query", "component_type": "Attention Q"}, {"name": "blk.17.attn_q.weight", "offset_start": 10684951488, "offset_end": 10708544448, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 17, "component": "query", "component_type": "Attention Q"}, {"name": "blk.17.attn_sinks.weight", "offset_start": 10708544448, "offset_end": 10708544704, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 17, "component": "other", "component_type": "Other"}, {"name": "blk.17.attn_v.bias", "offset_start": 10708544704, "offset_end": 10708546752, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 17, "component": "value", "component_type": "Attention V"}, {"name": "blk.17.attn_v.weight", "offset_start": 10708546752, "offset_end": 10711495872, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 17, "component": "value", "component_type": "Attention V"}, {"name": "blk.18.attn_norm.weight", "offset_start": 10711495872, "offset_end": 10711507392, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 18, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.18.attn_k.bias", "offset_start": 10711507392, "offset_end": 10711509440, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 18, "component": "key", "component_type": "Attention K"}, {"name": "blk.18.attn_output.bias", "offset_start": 10711509440, "offset_end": 10711520960, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 18, "component": "output", "component_type": "Output Projection"}, {"name": "blk.18.attn_output.weight", "offset_start": 10711520960, "offset_end": 10735113920, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 18, "component": "output", "component_type": "Output Projection"}, {"name": "blk.18.attn_q.bias", "offset_start": 10735113920, "offset_end": 10735130304, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 18, "component": "query", "component_type": "Attention Q"}, {"name": "blk.18.attn_v.bias", "offset_start": 10735130304, "offset_end": 10735132352, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 18, "component": "value", "component_type": "Attention V"}, {"name": "blk.18.ffn_down_exps.bias", "offset_start": 10735132352, "offset_end": 10735500992, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 18, "component": "down", "component_type": "FFN Down"}, {"name": "blk.18.ffn_gate_exps.bias", "offset_start": 10735500992, "offset_end": 10735869632, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 18, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.18.ffn_up_exps.bias", "offset_start": 10735869632, "offset_end": 10736238272, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 18, "component": "up", "component_type": "FFN Up"}, {"name": "blk.18.ffn_gate_inp.bias", "offset_start": 10736238272, "offset_end": 10736238400, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 18, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.18.ffn_gate_inp.weight", "offset_start": 10736238400, "offset_end": 10736607040, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 18, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.18.post_attention_norm.weight", "offset_start": 10736607040, "offset_end": 10736618560, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 18, "component": "other", "component_type": "Other"}, {"name": "blk.18.attn_k.weight", "offset_start": 10736618560, "offset_end": 10739567680, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 18, "component": "key", "component_type": "Attention K"}, {"name": "blk.18.attn_q.weight", "offset_start": 10739567680, "offset_end": 10763160640, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 18, "component": "query", "component_type": "Attention Q"}, {"name": "blk.18.attn_sinks.weight", "offset_start": 10763160640, "offset_end": 10763160896, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 18, "component": "other", "component_type": "Other"}, {"name": "blk.18.attn_v.weight", "offset_start": 10763160896, "offset_end": 10766110016, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 18, "component": "value", "component_type": "Attention V"}, {"name": "blk.19.attn_norm.weight", "offset_start": 10766110016, "offset_end": 10766121536, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 19, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.19.ffn_down_exps.bias", "offset_start": 10766121536, "offset_end": 10766490176, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 19, "component": "down", "component_type": "FFN Down"}, {"name": "blk.19.ffn_gate_exps.bias", "offset_start": 10766490176, "offset_end": 10766858816, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 19, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.19.ffn_up_exps.bias", "offset_start": 10766858816, "offset_end": 10767227456, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 19, "component": "up", "component_type": "FFN Up"}, {"name": "blk.19.ffn_gate_inp.bias", "offset_start": 10767227456, "offset_end": 10767227584, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 19, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.19.ffn_gate_inp.weight", "offset_start": 10767227584, "offset_end": 10767596224, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 19, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.19.post_attention_norm.weight", "offset_start": 10767596224, "offset_end": 10767607744, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 19, "component": "other", "component_type": "Other"}, {"name": "blk.19.attn_k.bias", "offset_start": 10767607744, "offset_end": 10767609792, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 19, "component": "key", "component_type": "Attention K"}, {"name": "blk.19.attn_k.weight", "offset_start": 10767609792, "offset_end": 10770558912, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 19, "component": "key", "component_type": "Attention K"}, {"name": "blk.19.attn_output.bias", "offset_start": 10770558912, "offset_end": 10770570432, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 19, "component": "output", "component_type": "Output Projection"}, {"name": "blk.19.attn_output.weight", "offset_start": 10770570432, "offset_end": 10794163392, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 19, "component": "output", "component_type": "Output Projection"}, {"name": "blk.19.attn_q.bias", "offset_start": 10794163392, "offset_end": 10794179776, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 19, "component": "query", "component_type": "Attention Q"}, {"name": "blk.19.attn_q.weight", "offset_start": 10794179776, "offset_end": 10817772736, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 19, "component": "query", "component_type": "Attention Q"}, {"name": "blk.19.attn_sinks.weight", "offset_start": 10817772736, "offset_end": 10817772992, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 19, "component": "other", "component_type": "Other"}, {"name": "blk.19.attn_v.bias", "offset_start": 10817772992, "offset_end": 10817775040, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 19, "component": "value", "component_type": "Attention V"}, {"name": "blk.19.attn_v.weight", "offset_start": 10817775040, "offset_end": 10820724160, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 19, "component": "value", "component_type": "Attention V"}, {"name": "blk.2.attn_norm.weight", "offset_start": 10820724160, "offset_end": 10820735680, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 2, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.2.ffn_down_exps.bias", "offset_start": 10820735680, "offset_end": 10821104320, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 2, "component": "down", "component_type": "FFN Down"}, {"name": "blk.2.ffn_gate_exps.bias", "offset_start": 10821104320, "offset_end": 10821472960, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 2, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.2.ffn_up_exps.bias", "offset_start": 10821472960, "offset_end": 10821841600, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 2, "component": "up", "component_type": "FFN Up"}, {"name": "blk.2.ffn_gate_inp.bias", "offset_start": 10821841600, "offset_end": 10821841728, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 2, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.2.ffn_gate_inp.weight", "offset_start": 10821841728, "offset_end": 10822210368, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 2, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.2.post_attention_norm.weight", "offset_start": 10822210368, "offset_end": 10822221888, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 2, "component": "other", "component_type": "Other"}, {"name": "blk.2.attn_k.bias", "offset_start": 10822221888, "offset_end": 10822223936, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 2, "component": "key", "component_type": "Attention K"}, {"name": "blk.2.attn_k.weight", "offset_start": 10822223936, "offset_end": 10825173056, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 2, "component": "key", "component_type": "Attention K"}, {"name": "blk.2.attn_output.bias", "offset_start": 10825173056, "offset_end": 10825184576, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 2, "component": "output", "component_type": "Output Projection"}, {"name": "blk.2.attn_output.weight", "offset_start": 10825184576, "offset_end": 10848777536, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 2, "component": "output", "component_type": "Output Projection"}, {"name": "blk.2.attn_q.bias", "offset_start": 10848777536, "offset_end": 10848793920, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 2, "component": "query", "component_type": "Attention Q"}, {"name": "blk.2.attn_q.weight", "offset_start": 10848793920, "offset_end": 10872386880, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 2, "component": "query", "component_type": "Attention Q"}, {"name": "blk.2.attn_sinks.weight", "offset_start": 10872386880, "offset_end": 10872387136, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 2, "component": "other", "component_type": "Other"}, {"name": "blk.2.attn_v.bias", "offset_start": 10872387136, "offset_end": 10872389184, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 2, "component": "value", "component_type": "Attention V"}, {"name": "blk.2.attn_v.weight", "offset_start": 10872389184, "offset_end": 10875338304, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 2, "component": "value", "component_type": "Attention V"}, {"name": "blk.20.attn_norm.weight", "offset_start": 10875338304, "offset_end": 10875349824, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 20, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.20.ffn_down_exps.bias", "offset_start": 10875349824, "offset_end": 10875718464, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 20, "component": "down", "component_type": "FFN Down"}, {"name": "blk.20.ffn_gate_exps.bias", "offset_start": 10875718464, "offset_end": 10876087104, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 20, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.20.ffn_up_exps.bias", "offset_start": 10876087104, "offset_end": 10876455744, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 20, "component": "up", "component_type": "FFN Up"}, {"name": "blk.20.ffn_gate_inp.bias", "offset_start": 10876455744, "offset_end": 10876455872, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 20, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.20.ffn_gate_inp.weight", "offset_start": 10876455872, "offset_end": 10876824512, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 20, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.20.post_attention_norm.weight", "offset_start": 10876824512, "offset_end": 10876836032, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 20, "component": "other", "component_type": "Other"}, {"name": "blk.20.attn_k.bias", "offset_start": 10876836032, "offset_end": 10876838080, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 20, "component": "key", "component_type": "Attention K"}, {"name": "blk.20.attn_k.weight", "offset_start": 10876838080, "offset_end": 10879787200, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 20, "component": "key", "component_type": "Attention K"}, {"name": "blk.20.attn_output.bias", "offset_start": 10879787200, "offset_end": 10879798720, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 20, "component": "output", "component_type": "Output Projection"}, {"name": "blk.20.attn_output.weight", "offset_start": 10879798720, "offset_end": 10903391680, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 20, "component": "output", "component_type": "Output Projection"}, {"name": "blk.20.attn_q.bias", "offset_start": 10903391680, "offset_end": 10903408064, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 20, "component": "query", "component_type": "Attention Q"}, {"name": "blk.20.attn_q.weight", "offset_start": 10903408064, "offset_end": 10927001024, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 20, "component": "query", "component_type": "Attention Q"}, {"name": "blk.20.attn_sinks.weight", "offset_start": 10927001024, "offset_end": 10927001280, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 20, "component": "other", "component_type": "Other"}, {"name": "blk.20.attn_v.bias", "offset_start": 10927001280, "offset_end": 10927003328, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 20, "component": "value", "component_type": "Attention V"}, {"name": "blk.20.attn_v.weight", "offset_start": 10927003328, "offset_end": 10929952448, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 20, "component": "value", "component_type": "Attention V"}, {"name": "blk.21.attn_norm.weight", "offset_start": 10929952448, "offset_end": 10929963968, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 21, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.21.ffn_down_exps.bias", "offset_start": 10929963968, "offset_end": 10930332608, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 21, "component": "down", "component_type": "FFN Down"}, {"name": "blk.21.ffn_gate_exps.bias", "offset_start": 10930332608, "offset_end": 10930701248, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 21, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.21.ffn_up_exps.bias", "offset_start": 10930701248, "offset_end": 10931069888, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 21, "component": "up", "component_type": "FFN Up"}, {"name": "blk.21.ffn_gate_inp.bias", "offset_start": 10931069888, "offset_end": 10931070016, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 21, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.21.ffn_gate_inp.weight", "offset_start": 10931070016, "offset_end": 10931438656, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 21, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.21.post_attention_norm.weight", "offset_start": 10931438656, "offset_end": 10931450176, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 21, "component": "other", "component_type": "Other"}, {"name": "blk.21.attn_k.bias", "offset_start": 10931450176, "offset_end": 10931452224, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 21, "component": "key", "component_type": "Attention K"}, {"name": "blk.21.attn_k.weight", "offset_start": 10931452224, "offset_end": 10934401344, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 21, "component": "key", "component_type": "Attention K"}, {"name": "blk.21.attn_output.bias", "offset_start": 10934401344, "offset_end": 10934412864, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 21, "component": "output", "component_type": "Output Projection"}, {"name": "blk.21.attn_output.weight", "offset_start": 10934412864, "offset_end": 10958005824, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 21, "component": "output", "component_type": "Output Projection"}, {"name": "blk.21.attn_q.bias", "offset_start": 10958005824, "offset_end": 10958022208, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 21, "component": "query", "component_type": "Attention Q"}, {"name": "blk.21.attn_q.weight", "offset_start": 10958022208, "offset_end": 10981615168, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 21, "component": "query", "component_type": "Attention Q"}, {"name": "blk.21.attn_sinks.weight", "offset_start": 10981615168, "offset_end": 10981615424, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 21, "component": "other", "component_type": "Other"}, {"name": "blk.21.attn_v.bias", "offset_start": 10981615424, "offset_end": 10981617472, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 21, "component": "value", "component_type": "Attention V"}, {"name": "blk.21.attn_v.weight", "offset_start": 10981617472, "offset_end": 10984566592, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 21, "component": "value", "component_type": "Attention V"}, {"name": "blk.22.attn_norm.weight", "offset_start": 10984566592, "offset_end": 10984578112, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 22, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.22.ffn_down_exps.bias", "offset_start": 10984578112, "offset_end": 10984946752, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 22, "component": "down", "component_type": "FFN Down"}, {"name": "blk.22.ffn_gate_exps.bias", "offset_start": 10984946752, "offset_end": 10985315392, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 22, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.22.ffn_up_exps.bias", "offset_start": 10985315392, "offset_end": 10985684032, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 22, "component": "up", "component_type": "FFN Up"}, {"name": "blk.22.ffn_gate_inp.bias", "offset_start": 10985684032, "offset_end": 10985684160, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 22, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.22.ffn_gate_inp.weight", "offset_start": 10985684160, "offset_end": 10986052800, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 22, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.22.post_attention_norm.weight", "offset_start": 10986052800, "offset_end": 10986064320, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 22, "component": "other", "component_type": "Other"}, {"name": "blk.22.attn_k.bias", "offset_start": 10986064320, "offset_end": 10986066368, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 22, "component": "key", "component_type": "Attention K"}, {"name": "blk.22.attn_k.weight", "offset_start": 10986066368, "offset_end": 10989015488, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 22, "component": "key", "component_type": "Attention K"}, {"name": "blk.22.attn_output.bias", "offset_start": 10989015488, "offset_end": 10989027008, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 22, "component": "output", "component_type": "Output Projection"}, {"name": "blk.22.attn_output.weight", "offset_start": 10989027008, "offset_end": 11012619968, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 22, "component": "output", "component_type": "Output Projection"}, {"name": "blk.22.attn_q.bias", "offset_start": 11012619968, "offset_end": 11012636352, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 22, "component": "query", "component_type": "Attention Q"}, {"name": "blk.22.attn_q.weight", "offset_start": 11012636352, "offset_end": 11036229312, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 22, "component": "query", "component_type": "Attention Q"}, {"name": "blk.22.attn_sinks.weight", "offset_start": 11036229312, "offset_end": 11036229568, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 22, "component": "other", "component_type": "Other"}, {"name": "blk.22.attn_v.bias", "offset_start": 11036229568, "offset_end": 11036231616, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 22, "component": "value", "component_type": "Attention V"}, {"name": "blk.22.attn_v.weight", "offset_start": 11036231616, "offset_end": 11039180736, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 22, "component": "value", "component_type": "Attention V"}, {"name": "blk.23.attn_norm.weight", "offset_start": 11039180736, "offset_end": 11039192256, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 23, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.23.ffn_down_exps.bias", "offset_start": 11039192256, "offset_end": 11039560896, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 23, "component": "down", "component_type": "FFN Down"}, {"name": "blk.23.ffn_gate_exps.bias", "offset_start": 11039560896, "offset_end": 11039929536, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 23, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.23.ffn_up_exps.bias", "offset_start": 11039929536, "offset_end": 11040298176, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 23, "component": "up", "component_type": "FFN Up"}, {"name": "blk.23.ffn_gate_inp.bias", "offset_start": 11040298176, "offset_end": 11040298304, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 23, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.23.ffn_gate_inp.weight", "offset_start": 11040298304, "offset_end": 11040666944, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 23, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.23.post_attention_norm.weight", "offset_start": 11040666944, "offset_end": 11040678464, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 23, "component": "other", "component_type": "Other"}, {"name": "blk.23.attn_k.bias", "offset_start": 11040678464, "offset_end": 11040680512, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 23, "component": "key", "component_type": "Attention K"}, {"name": "blk.23.attn_k.weight", "offset_start": 11040680512, "offset_end": 11043629632, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 23, "component": "key", "component_type": "Attention K"}, {"name": "blk.23.attn_output.bias", "offset_start": 11043629632, "offset_end": 11043641152, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 23, "component": "output", "component_type": "Output Projection"}, {"name": "blk.23.attn_output.weight", "offset_start": 11043641152, "offset_end": 11067234112, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 23, "component": "output", "component_type": "Output Projection"}, {"name": "blk.23.attn_q.bias", "offset_start": 11067234112, "offset_end": 11067250496, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 23, "component": "query", "component_type": "Attention Q"}, {"name": "blk.23.attn_q.weight", "offset_start": 11067250496, "offset_end": 11090843456, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 23, "component": "query", "component_type": "Attention Q"}, {"name": "blk.23.attn_sinks.weight", "offset_start": 11090843456, "offset_end": 11090843712, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 23, "component": "other", "component_type": "Other"}, {"name": "blk.23.attn_v.bias", "offset_start": 11090843712, "offset_end": 11090845760, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 23, "component": "value", "component_type": "Attention V"}, {"name": "blk.23.attn_v.weight", "offset_start": 11090845760, "offset_end": 11093794880, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 23, "component": "value", "component_type": "Attention V"}, {"name": "blk.3.attn_norm.weight", "offset_start": 11093794880, "offset_end": 11093806400, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 3, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.3.ffn_down_exps.bias", "offset_start": 11093806400, "offset_end": 11094175040, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 3, "component": "down", "component_type": "FFN Down"}, {"name": "blk.3.ffn_gate_exps.bias", "offset_start": 11094175040, "offset_end": 11094543680, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 3, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.3.ffn_up_exps.bias", "offset_start": 11094543680, "offset_end": 11094912320, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 3, "component": "up", "component_type": "FFN Up"}, {"name": "blk.3.ffn_gate_inp.bias", "offset_start": 11094912320, "offset_end": 11094912448, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 3, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.3.ffn_gate_inp.weight", "offset_start": 11094912448, "offset_end": 11095281088, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 3, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.3.post_attention_norm.weight", "offset_start": 11095281088, "offset_end": 11095292608, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 3, "component": "other", "component_type": "Other"}, {"name": "blk.3.attn_k.bias", "offset_start": 11095292608, "offset_end": 11095294656, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 3, "component": "key", "component_type": "Attention K"}, {"name": "blk.3.attn_k.weight", "offset_start": 11095294656, "offset_end": 11098243776, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 3, "component": "key", "component_type": "Attention K"}, {"name": "blk.3.attn_output.bias", "offset_start": 11098243776, "offset_end": 11098255296, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 3, "component": "output", "component_type": "Output Projection"}, {"name": "blk.3.attn_output.weight", "offset_start": 11098255296, "offset_end": 11121848256, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 3, "component": "output", "component_type": "Output Projection"}, {"name": "blk.3.attn_q.bias", "offset_start": 11121848256, "offset_end": 11121864640, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 3, "component": "query", "component_type": "Attention Q"}, {"name": "blk.3.attn_q.weight", "offset_start": 11121864640, "offset_end": 11145457600, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 3, "component": "query", "component_type": "Attention Q"}, {"name": "blk.3.attn_sinks.weight", "offset_start": 11145457600, "offset_end": 11145457856, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 3, "component": "other", "component_type": "Other"}, {"name": "blk.3.attn_v.bias", "offset_start": 11145457856, "offset_end": 11145459904, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 3, "component": "value", "component_type": "Attention V"}, {"name": "blk.3.attn_v.weight", "offset_start": 11145459904, "offset_end": 11148409024, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 3, "component": "value", "component_type": "Attention V"}, {"name": "blk.4.attn_norm.weight", "offset_start": 11148409024, "offset_end": 11148420544, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 4, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.4.ffn_down_exps.bias", "offset_start": 11148420544, "offset_end": 11148789184, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 4, "component": "down", "component_type": "FFN Down"}, {"name": "blk.4.ffn_gate_exps.bias", "offset_start": 11148789184, "offset_end": 11149157824, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 4, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.4.ffn_up_exps.bias", "offset_start": 11149157824, "offset_end": 11149526464, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 4, "component": "up", "component_type": "FFN Up"}, {"name": "blk.4.ffn_gate_inp.bias", "offset_start": 11149526464, "offset_end": 11149526592, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 4, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.4.ffn_gate_inp.weight", "offset_start": 11149526592, "offset_end": 11149895232, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 4, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.4.post_attention_norm.weight", "offset_start": 11149895232, "offset_end": 11149906752, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 4, "component": "other", "component_type": "Other"}, {"name": "blk.4.attn_k.bias", "offset_start": 11149906752, "offset_end": 11149908800, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 4, "component": "key", "component_type": "Attention K"}, {"name": "blk.4.attn_k.weight", "offset_start": 11149908800, "offset_end": 11152857920, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 4, "component": "key", "component_type": "Attention K"}, {"name": "blk.4.attn_output.bias", "offset_start": 11152857920, "offset_end": 11152869440, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 4, "component": "output", "component_type": "Output Projection"}, {"name": "blk.4.attn_output.weight", "offset_start": 11152869440, "offset_end": 11176462400, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 4, "component": "output", "component_type": "Output Projection"}, {"name": "blk.4.attn_q.bias", "offset_start": 11176462400, "offset_end": 11176478784, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 4, "component": "query", "component_type": "Attention Q"}, {"name": "blk.4.attn_q.weight", "offset_start": 11176478784, "offset_end": 11200071744, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 4, "component": "query", "component_type": "Attention Q"}, {"name": "blk.4.attn_sinks.weight", "offset_start": 11200071744, "offset_end": 11200072000, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 4, "component": "other", "component_type": "Other"}, {"name": "blk.4.attn_v.bias", "offset_start": 11200072000, "offset_end": 11200074048, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 4, "component": "value", "component_type": "Attention V"}, {"name": "blk.4.attn_v.weight", "offset_start": 11200074048, "offset_end": 11203023168, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 4, "component": "value", "component_type": "Attention V"}, {"name": "blk.5.attn_norm.weight", "offset_start": 11203023168, "offset_end": 11203034688, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 5, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.5.ffn_down_exps.bias", "offset_start": 11203034688, "offset_end": 11203403328, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 5, "component": "down", "component_type": "FFN Down"}, {"name": "blk.5.ffn_gate_exps.bias", "offset_start": 11203403328, "offset_end": 11203771968, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 5, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.5.ffn_up_exps.bias", "offset_start": 11203771968, "offset_end": 11204140608, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 5, "component": "up", "component_type": "FFN Up"}, {"name": "blk.5.ffn_gate_inp.bias", "offset_start": 11204140608, "offset_end": 11204140736, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 5, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.5.ffn_gate_inp.weight", "offset_start": 11204140736, "offset_end": 11204509376, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 5, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.5.post_attention_norm.weight", "offset_start": 11204509376, "offset_end": 11204520896, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 5, "component": "other", "component_type": "Other"}, {"name": "blk.5.attn_k.bias", "offset_start": 11204520896, "offset_end": 11204522944, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 5, "component": "key", "component_type": "Attention K"}, {"name": "blk.5.attn_k.weight", "offset_start": 11204522944, "offset_end": 11207472064, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 5, "component": "key", "component_type": "Attention K"}, {"name": "blk.5.attn_output.bias", "offset_start": 11207472064, "offset_end": 11207483584, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 5, "component": "output", "component_type": "Output Projection"}, {"name": "blk.5.attn_output.weight", "offset_start": 11207483584, "offset_end": 11231076544, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 5, "component": "output", "component_type": "Output Projection"}, {"name": "blk.5.attn_q.bias", "offset_start": 11231076544, "offset_end": 11231092928, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 5, "component": "query", "component_type": "Attention Q"}, {"name": "blk.5.attn_q.weight", "offset_start": 11231092928, "offset_end": 11254685888, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 5, "component": "query", "component_type": "Attention Q"}, {"name": "blk.5.attn_sinks.weight", "offset_start": 11254685888, "offset_end": 11254686144, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 5, "component": "other", "component_type": "Other"}, {"name": "blk.5.attn_v.bias", "offset_start": 11254686144, "offset_end": 11254688192, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 5, "component": "value", "component_type": "Attention V"}, {"name": "blk.5.attn_v.weight", "offset_start": 11254688192, "offset_end": 11257637312, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 5, "component": "value", "component_type": "Attention V"}, {"name": "blk.6.attn_norm.weight", "offset_start": 11257637312, "offset_end": 11257648832, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 6, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.6.ffn_gate_exps.bias", "offset_start": 11257648832, "offset_end": 11258017472, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 6, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.6.ffn_up_exps.bias", "offset_start": 11258017472, "offset_end": 11258386112, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 6, "component": "up", "component_type": "FFN Up"}, {"name": "blk.6.ffn_gate_inp.bias", "offset_start": 11258386112, "offset_end": 11258386240, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 6, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.6.ffn_gate_inp.weight", "offset_start": 11258386240, "offset_end": 11258754880, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 6, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.6.attn_k.bias", "offset_start": 11258754880, "offset_end": 11258756928, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 6, "component": "key", "component_type": "Attention K"}, {"name": "blk.6.attn_k.weight", "offset_start": 11258756928, "offset_end": 11261706048, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 6, "component": "key", "component_type": "Attention K"}, {"name": "blk.6.attn_output.bias", "offset_start": 11261706048, "offset_end": 11261717568, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 6, "component": "output", "component_type": "Output Projection"}, {"name": "blk.6.attn_output.weight", "offset_start": 11261717568, "offset_end": 11285310528, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 6, "component": "output", "component_type": "Output Projection"}, {"name": "blk.6.attn_q.bias", "offset_start": 11285310528, "offset_end": 11285326912, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 6, "component": "query", "component_type": "Attention Q"}, {"name": "blk.6.attn_q.weight", "offset_start": 11285326912, "offset_end": 11308919872, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 6, "component": "query", "component_type": "Attention Q"}, {"name": "blk.6.attn_sinks.weight", "offset_start": 11308919872, "offset_end": 11308920128, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 6, "component": "other", "component_type": "Other"}, {"name": "blk.6.attn_v.bias", "offset_start": 11308920128, "offset_end": 11308922176, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 6, "component": "value", "component_type": "Attention V"}, {"name": "blk.6.attn_v.weight", "offset_start": 11308922176, "offset_end": 11311871296, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 6, "component": "value", "component_type": "Attention V"}, {"name": "output.weight", "offset_start": 11311871296, "offset_end": 12470138176, "size_bytes": 1158266880, "shape": [2880, 201088], "category": "output", "layer_id": null, "component": "output", "component_type": "Output Projection"}, {"name": "token_embd.weight", "offset_start": 12470138176, "offset_end": 13628405056, "size_bytes": 1158266880, "shape": [2880, 201088], "category": "embedding", "layer_id": null, "component": "other", "component_type": "Token Embeddings"}, {"name": "blk.6.ffn_down_exps.bias", "offset_start": 13628405056, "offset_end": 13628773696, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 6, "component": "down", "component_type": "FFN Down"}, {"name": "blk.6.post_attention_norm.weight", "offset_start": 13628773696, "offset_end": 13628785216, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 6, "component": "other", "component_type": "Other"}, {"name": "blk.7.attn_norm.weight", "offset_start": 13628785216, "offset_end": 13628796736, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 7, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.7.ffn_down_exps.bias", "offset_start": 13628796736, "offset_end": 13629165376, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 7, "component": "down", "component_type": "FFN Down"}, {"name": "blk.7.ffn_gate_exps.bias", "offset_start": 13629165376, "offset_end": 13629534016, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 7, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.7.ffn_up_exps.bias", "offset_start": 13629534016, "offset_end": 13629902656, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 7, "component": "up", "component_type": "FFN Up"}, {"name": "blk.7.ffn_gate_inp.bias", "offset_start": 13629902656, "offset_end": 13629902784, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 7, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.7.ffn_gate_inp.weight", "offset_start": 13629902784, "offset_end": 13630271424, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 7, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.7.post_attention_norm.weight", "offset_start": 13630271424, "offset_end": 13630282944, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 7, "component": "other", "component_type": "Other"}, {"name": "blk.7.attn_k.bias", "offset_start": 13630282944, "offset_end": 13630284992, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 7, "component": "key", "component_type": "Attention K"}, {"name": "blk.7.attn_k.weight", "offset_start": 13630284992, "offset_end": 13633234112, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 7, "component": "key", "component_type": "Attention K"}, {"name": "blk.7.attn_output.bias", "offset_start": 13633234112, "offset_end": 13633245632, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 7, "component": "output", "component_type": "Output Projection"}, {"name": "blk.7.attn_output.weight", "offset_start": 13633245632, "offset_end": 13656838592, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 7, "component": "output", "component_type": "Output Projection"}, {"name": "blk.7.attn_q.bias", "offset_start": 13656838592, "offset_end": 13656854976, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 7, "component": "query", "component_type": "Attention Q"}, {"name": "blk.7.attn_q.weight", "offset_start": 13656854976, "offset_end": 13680447936, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 7, "component": "query", "component_type": "Attention Q"}, {"name": "blk.7.attn_sinks.weight", "offset_start": 13680447936, "offset_end": 13680448192, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 7, "component": "other", "component_type": "Other"}, {"name": "blk.7.attn_v.bias", "offset_start": 13680448192, "offset_end": 13680450240, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 7, "component": "value", "component_type": "Attention V"}, {"name": "blk.7.attn_v.weight", "offset_start": 13680450240, "offset_end": 13683399360, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 7, "component": "value", "component_type": "Attention V"}, {"name": "blk.8.attn_norm.weight", "offset_start": 13683399360, "offset_end": 13683410880, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 8, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.8.ffn_down_exps.bias", "offset_start": 13683410880, "offset_end": 13683779520, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 8, "component": "down", "component_type": "FFN Down"}, {"name": "blk.8.ffn_gate_exps.bias", "offset_start": 13683779520, "offset_end": 13684148160, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 8, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.8.ffn_up_exps.bias", "offset_start": 13684148160, "offset_end": 13684516800, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 8, "component": "up", "component_type": "FFN Up"}, {"name": "blk.8.ffn_gate_inp.bias", "offset_start": 13684516800, "offset_end": 13684516928, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 8, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.8.ffn_gate_inp.weight", "offset_start": 13684516928, "offset_end": 13684885568, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 8, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.8.post_attention_norm.weight", "offset_start": 13684885568, "offset_end": 13684897088, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 8, "component": "other", "component_type": "Other"}, {"name": "blk.8.attn_k.bias", "offset_start": 13684897088, "offset_end": 13684899136, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 8, "component": "key", "component_type": "Attention K"}, {"name": "blk.8.attn_k.weight", "offset_start": 13684899136, "offset_end": 13687848256, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 8, "component": "key", "component_type": "Attention K"}, {"name": "blk.8.attn_output.bias", "offset_start": 13687848256, "offset_end": 13687859776, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 8, "component": "output", "component_type": "Output Projection"}, {"name": "blk.8.attn_output.weight", "offset_start": 13687859776, "offset_end": 13711452736, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 8, "component": "output", "component_type": "Output Projection"}, {"name": "blk.8.attn_q.bias", "offset_start": 13711452736, "offset_end": 13711469120, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 8, "component": "query", "component_type": "Attention Q"}, {"name": "blk.8.attn_q.weight", "offset_start": 13711469120, "offset_end": 13735062080, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 8, "component": "query", "component_type": "Attention Q"}, {"name": "blk.8.attn_sinks.weight", "offset_start": 13735062080, "offset_end": 13735062336, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 8, "component": "other", "component_type": "Other"}, {"name": "blk.8.attn_v.bias", "offset_start": 13735062336, "offset_end": 13735064384, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 8, "component": "value", "component_type": "Attention V"}, {"name": "blk.8.attn_v.weight", "offset_start": 13735064384, "offset_end": 13738013504, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 8, "component": "value", "component_type": "Attention V"}, {"name": "blk.9.attn_norm.weight", "offset_start": 13738013504, "offset_end": 13738025024, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 9, "component": "norm", "component_type": "Attention Norm"}, {"name": "blk.9.ffn_down_exps.bias", "offset_start": 13738025024, "offset_end": 13738393664, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 9, "component": "down", "component_type": "FFN Down"}, {"name": "blk.9.ffn_gate_exps.bias", "offset_start": 13738393664, "offset_end": 13738762304, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 9, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.9.ffn_up_exps.bias", "offset_start": 13738762304, "offset_end": 13739130944, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 9, "component": "up", "component_type": "FFN Up"}, {"name": "blk.9.ffn_gate_inp.bias", "offset_start": 13739130944, "offset_end": 13739131072, "size_bytes": 128, "shape": [32], "category": "ffn", "layer_id": 9, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.9.ffn_gate_inp.weight", "offset_start": 13739131072, "offset_end": 13739499712, "size_bytes": 368640, "shape": [2880, 32], "category": "ffn", "layer_id": 9, "component": "gate", "component_type": "FFN Gate"}, {"name": "blk.9.post_attention_norm.weight", "offset_start": 13739499712, "offset_end": 13739511232, "size_bytes": 11520, "shape": [2880], "category": "other", "layer_id": 9, "component": "other", "component_type": "Other"}, {"name": "blk.9.attn_k.bias", "offset_start": 13739511232, "offset_end": 13739513280, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 9, "component": "key", "component_type": "Attention K"}, {"name": "blk.9.attn_k.weight", "offset_start": 13739513280, "offset_end": 13742462400, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 9, "component": "key", "component_type": "Attention K"}, {"name": "blk.9.attn_output.bias", "offset_start": 13742462400, "offset_end": 13742473920, "size_bytes": 11520, "shape": [2880], "category": "attention", "layer_id": 9, "component": "output", "component_type": "Output Projection"}, {"name": "blk.9.attn_output.weight", "offset_start": 13742473920, "offset_end": 13766066880, "size_bytes": 23592960, "shape": [4096, 2880], "category": "attention", "layer_id": 9, "component": "output", "component_type": "Output Projection"}, {"name": "blk.9.attn_q.bias", "offset_start": 13766066880, "offset_end": 13766083264, "size_bytes": 16384, "shape": [4096], "category": "attention", "layer_id": 9, "component": "query", "component_type": "Attention Q"}, {"name": "blk.9.attn_q.weight", "offset_start": 13766083264, "offset_end": 13789676224, "size_bytes": 23592960, "shape": [2880, 4096], "category": "attention", "layer_id": 9, "component": "query", "component_type": "Attention Q"}, {"name": "blk.9.attn_sinks.weight", "offset_start": 13789676224, "offset_end": 13789676480, "size_bytes": 256, "shape": [64], "category": "attention", "layer_id": 9, "component": "other", "component_type": "Other"}, {"name": "blk.9.attn_v.bias", "offset_start": 13789676480, "offset_end": 13789678528, "size_bytes": 2048, "shape": [512], "category": "attention", "layer_id": 9, "component": "value", "component_type": "Attention V"}, {"name": "blk.9.attn_v.weight", "offset_start": 13789678528, "offset_end": 13792627648, "size_bytes": 2949120, "shape": [2880, 512], "category": "attention", "layer_id": 9, "component": "value", "component_type": "Attention V"}, {"name": "output_norm.weight", "offset_start": 13792627648, "offset_end": 13792639168, "size_bytes": 11520, "shape": [2880], "category": "output", "layer_id": null, "component": "output", "component_type": "Output Projection"}]}