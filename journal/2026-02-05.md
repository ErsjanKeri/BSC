# 5 February 2026 - Desktop UI Refinements and Expert Activation Experiment Setup

## Summary

Today focused on refining the desktop UI visualization and setting up a comprehensive experimental framework for analyzing expert activation patterns across multiple domains. Major accomplishments include fixing critical UI bugs (tooltip persistence, window layout), implementing dual-plot visualization with proper sizing, and creating an automated data collection pipeline for multi-domain expert analysis.

**Key Achievements**:
1. Fixed desktop UI layout and tooltip bugs
2. Implemented per-token + accumulated dual visualization
3. Set up automated 5-domain experiment (500 tokens total)
4. Collected initial expert activation data
5. Updated BSC documentation with system architecture

---

## Part 1: Desktop UI Bug Fixes and Improvements

### Issue 1: Tooltip Persistence Bug

**Problem**: Tooltips remained visible even after moving cursor away from heatmap.

**Root Cause**: `hovered_tensor_` was not being reset at the beginning of each frame.

**Solution**:
```cpp
void HeatmapView::renderHeatmapCanvas() {
    // Reset hover state at start of frame
    hovered_tensor_ = nullptr;  // ← Added this

    // Render plots...
}
```

**Result**: Tooltips now correctly disappear when cursor moves away.

### Issue 2: Window Layout Chaos

**Problem**: Windows were nesting incorrectly - `heatmapView.render()` created "Memory Access Heatmap" window inside the outer "Heatmap" window from main.cpp.

**Symptom**: 50/50 split layout didn't work properly, windows overlapped.

**Root Cause**: Both main.cpp and view classes calling `ImGui::Begin()`, creating nested windows.

**Solution**: Removed `ImGui::Begin/End` from view render methods:

```cpp
// Before (broken):
void HeatmapView::render() {
    ImGui::Begin("Memory Access Heatmap", ...);  // ← Nested window!
    // ... render content ...
    ImGui::End();
}

// After (fixed):
void HeatmapView::render() {
    // Render directly into current window (caller provides context)
    // ... render content ...
}
```

Applied same fix to `TraceTableView::render()`.

**Result**: Clean 50/50 vertical split - Trace Table (left) | Heatmap (right).

---

## Part 2: Dual Visualization Implementation

### Design Goal

Show both per-token (temporal) and accumulated (aggregate) access patterns:

```
┌─────────────────────────────────────┐
│ Per-Token Heatmap (100px)          │ ← Timeline scrubbing, viridis
│ ▓▓▒▒░░▓▓████ (current token)       │ ← Changes as you scrub
├─────────────────────────────────────┤
│ Accumulated Graph (450px)          │ ← All 100 tokens, blue fill
│ Shows total pattern                │ ← Static (doesn't change)
└─────────────────────────────────────┘
```

**Rationale**:
- **Top plot**: See how pattern evolves token-by-token (temporal analysis)
- **Bottom plot**: See overall hot regions across full inference (aggregate analysis)

### Implementation

**Per-Token Heatmap** (HeatmapView::renderColoredStrip):
- Viridis colormap (gray → purple → blue → green → yellow)
- Timeline slider controls which token's pattern is shown
- Only DISK accesses counted (not runtime buffers)
- Expert-level granularity (appends [expertId] to tensor names)
- Hover tooltip shows tensor details

**Accumulated Graph** (main.cpp::renderAccumulatedGraph):
- Blue filled area chart
- Aggregates accesses across ALL 100 tokens
- Y-axis shows total access counts (0 to max)
- Doesn't change with timeline (shows complete picture)
- Hover tooltip shows tensor + total accumulated count

### Plot Sizing

Adjusted heights for optimal visualization:

**Before**:
- Dual subplot: 25% colored strip, 75% step function graph
- Both showing per-token data (redundant)

**After**:
- Colored strip: 100px (compact, focused on pattern)
- Accumulated graph: 450px (prominent, shows quantitative data)
- Different data sources (per-token vs accumulated)

### Color Matching Fix

**Problem**: Fill had transparency (alpha=0.6), line was solid (alpha=1.0) - visual mismatch.

**Solution**:
```cpp
// Same color for both fill and line
ImVec4 blue_color = ImVec4(0.2f, 0.5f, 0.8f, 1.0f);  // Solid blue

// Fill
ImPlot::PushStyleColor(ImPlotCol_Fill, blue_color);
ImPlot::PlotShaded("##accumulated_fill", ...);
ImPlot::PopStyleColor();

// Line (identical color)
ImPlot::PushStyleColor(ImPlotCol_Line, blue_color);
ImPlot::PlotLine("##accumulated_line", ...);
ImPlot::PopStyleColor();
```

**Result**: Filled area and boundary line now use identical blue (no transparency mismatch).

---

## Part 3: Expert Activation Experiment Setup

### Research Questions

**Primary**: Do expert activation patterns show temporal correlation and domain-specific clustering?

**Specific hypotheses**:
1. **H1 (Temporal Autocorrelation)**: If expert X activated at token t, is it likely at token t+1?
2. **H2 (Domain Clustering)**: Do different domains (code, math, creative) activate different expert subsets?
3. **H3 (Stickiness)**: Are certain experts globally hot, or domain-specific?

### Experimental Design

**Data Collection Strategy**: 5 prompts × 100 tokens = 500 tokens total

**Domain Coverage**:

1. **Code Generation**: Binary search tree implementation (Python)
2. **Mathematical Reasoning**: Train distance problem with step-by-step solution
3. **Creative Writing**: AI discovering friendship (narrative)
4. **Factual Knowledge**: CRISPR gene editing explanation
5. **Mixed Domain**: Algorithm comparison + code (tests cross-domain behavior)

**Why these domains?**
- Cover diverse cognitive tasks
- Test if experts specialize by domain
- Include mixed domain to see if patterns combine

### Automation Framework

Created `prompts.json` with structured experiment configuration:

```json
{
  "experiment_name": "expert-analysis-2026-01-26",
  "prompts": [
    {
      "id": "domain-1-code",
      "domain": "code",
      "prompt": "Write a Python function...",
      "n_predict": 100
    },
    // ... 4 more domains
  ]
}
```

Created `run_expert_experiments.sh` - Master automation script:

**Workflow**:
```bash
1. Backup settings.json
2. For each prompt (1-5):
   a. Update settings.json with new prompt
   b. Run run_experiment.py (existing pipeline)
   c. Move output to experiments/expert-analysis-2026-01-26/domain-X/
3. Restore original settings.json
4. Create summary.json (metadata)
5. Create tarball for transfer
```

**Key design decision**: Work with existing `settings.json` workflow instead of modifying `run_experiment.py` - less invasive, leverages tested code.

### Data Organization

```
experiments/expert-analysis-2026-01-26/
├── domain-1-code/
│   ├── traces/
│   │   ├── token-00000.json
│   │   └── ... (100 files)
│   ├── graphs/
│   └── buffer-timeline.json
├── domain-2-math/
├── domain-3-creative/
├── domain-4-factual/
├── domain-5-mixed/
└── summary.json
```

### Data Collection Results

**Ran on TUM server**: `./run_expert_experiments.sh`

**Output**:
- 5 domains successfully processed
- 100 token traces per domain
- Total: 500 tokens, ~850 MB data
- Tarball: `expert-analysis-2026-01-26.tar.gz` (~400 MB compressed)

**Transferred locally** for analysis with Desktop UI.

### Initial Observations (Before Analysis)

Loaded domain-1-code into Desktop UI:

**Accumulated access graph shows**:
- Clear hotspots around 9-12 GB file region (later layers)
- Blue filled area makes pattern very visible
- Expert-level granularity visible (individual expert bars)

**Next steps** (planned):
1. Build expert frequency matrix (Domain × Expert ID)
2. Compute temporal autocorrelation
3. Visualize expert transitions
4. Identify domain-specific vs global-hot experts

---

## Part 4: Documentation Updates

### BSC README.md Overhaul

**Added sections**:

**1. System Architecture**
- 3-layer pipeline diagram (Data Collection → Processing → Visualization)
- Design principles: Modularity, Performance, Testability
- Clear data flow illustration

**2. Performance Metrics**
- Quantified instrumentation overhead (<1%)
- Data processing speed (1-2 seconds)
- Visualization performance (110 FPS desktop, 60 FPS web)
- Bug discovery table with dates

**3. Related Work Comparison**
- Fisher's thesis (Virginia Tech 2025) - NUMA page placement
- Key distinction: NUMA (RAM↔RAM, 2x latency) vs SSD-backed (RAM↔Storage, 2000x latency)
- Why Fisher's findings (page cache mitigates) don't apply to SSD scenario
- Alphabetical tensor ordering discovery

**4. Updated Project Structure**
- Included desktopui/ directory
- Updated journal entry list (through Feb 5)

### Thesis Title Refinement

**Evolved from**:
- "From DRAM to Disk..." (too pedagogical)
- "Tensor-Level Access Pattern Analysis..." (generic)

**Current leading candidate**:
> "Storage-Backed Large Language Model Inference: Access Pattern Characterization and Prefetching Optimization for Sparse Expert Architectures"

**Rationale**:
- Specific: SSD-backed, not just "memory hierarchy"
- Two-part contribution: Characterization (measurement) + Optimization (solution)
- Mentions sparse experts (MoE architecture - key differentiator)
- Professional tone (appeals to industry/trading firms)

Alternative shorter version:
> "Tensor-Level Access Pattern Analysis and Optimization for Storage-Backed LLM Inference"

**Key framing insight**: Emphasize "characterization" (systematic measurement) over "analysis" - sounds more rigorous, appeals to performance engineering mindset.

---

## Part 5: Comparison with Fisher's Thesis

### Fisher (Virginia Tech, April 2025)

**Thesis**: "Analysis of Memory Access Patterns for Large Language Model Inference"

**Focus**: NUMA page placement (local vs remote DRAM)

**Method**:
- Tool: `perf mem` (hardware performance counters)
- Granularity: 4-16 MB memory granules
- Model: Google Gemma v2 27B (dense model)
- Environment: Dell R640 with 348GB RAM (model fits!)

**Key Finding**:
> "Once in page cache, layout doesn't matter"
> Warm start: 0.5% penalty (minimal)
> Cold start: 56% penalty (significant)

**Conclusion**: After first load, model stays in page cache → remote access penalty eliminated by kernel page migration.

### Critical Distinctions from Our Work

**1. Memory Tier**:
- Fisher: Fast DRAM ↔ Slow DRAM (both RAM)
- Latency gap: 50ns ↔ 150ns (3x)
- Us: DRAM ↔ SSD (storage!)
- Latency gap: 50ns ↔ 100μs (2000x!)

**2. System Assumptions**:
- Fisher: Model fits in system RAM (27GB model, 348GB RAM)
- Page cache assumption holds → layout irrelevant after warmup
- Us: Model exceeds RAM (20GB model, limited memory)
- Pages evicted → repeated SSD reads → layout COULD matter

**3. Model Architecture**:
- Fisher: Gemma 27B (dense model, no sparsity)
- Us: GPT-OSS-20B (MoE with 32 experts, 12.5% sparsity)
- Expert selection patterns not explored by Fisher

**4. Instrumentation Depth**:
- Fisher: 4-16 MB granules (coarse)
- Sampling-based (`perf mem` has errors)
- Us: Tensor-level (individual operations)
- Exhaustive (logs every tensor access)
- Expert-level granularity (which 4-of-32 selected)

### Why Fisher's Findings Don't Invalidate Our Work

**Fisher is correct for**:
- Standard inference (model in RAM)
- High-memory systems
- After warmup (page cache populated)

**Fisher's assumptions break for**:
- SSD-backed inference (our scenario)
- Limited RAM (pages get evicted)
- Continuous paging (repeated disk I/O)

**Our thesis addresses a different problem space** - where Fisher's page cache assumption doesn't hold.

### Validation of Our Approach

**Fisher also found**:
- Clear temporal hotspot migration (Figure 4.7, 4.11, 4.13)
- Spatial stride patterns (~2GB stride from BLAS operations)
- Used autocorrelation analysis (Figure 4.6, 4.9)

**This validates our observations** - temporal patterns in llama.cpp memory access are real and reproducible across different research groups.

---

## Part 6: GGUF Alphabetical Ordering (Continued Investigation)

### Verification from Fisher's Work

Fisher's thesis (65 pages) **does not mention** GGUF file layout or alphabetical ordering. This confirms:
- NUMA research doesn't investigate physical file layout (virtual addressing via mmap)
- Our discovery is novel - not addressed in recent related work

### Why Fisher Didn't Need to Investigate

**For NUMA systems**:
- mmap creates virtual address mapping
- Physical file layout on disk ≠ virtual address layout in memory
- Kernel handles page faults, layout doesn't matter for RAM-based tiers

**For SSD-backed systems**:
- Physical file layout = actual SSD seek patterns
- Alphabetical ordering (L0, L1, L10, L11, ..., L2, ...) causes random seeks
- **This could matter** - needs empirical measurement (blktrace)

### Proposed Comparative Experiment (Future Work)

**Version A**: Current GGUF (alphabetical ordering)
**Version B**: Re-ordered GGUF (layer-sequential: L0, L1, L2, ...)

**Measure with blktrace**:
- Seek distances
- Sequential vs random I/O ratio
- Bandwidth utilization
- Inference time

**If Version B is faster**: Layout optimization opportunity (thesis contribution!)
**If versions equal**: Opus/Fisher correct (focus elsewhere)

---

## Part 7: Desktop UI Technical Implementation

### Plot Height Optimization

**User feedback**: "Colored strip too tall, accumulated graph too small"

**Changes**:
```cpp
// Colored strip (per-token pattern)
ImPlot::BeginPlot("##colored_strip", ImVec2(-1, 100));  // Was: 200px

// Accumulated graph (all tokens)
ImPlot::BeginPlot("##accumulated_graph", ImVec2(-1, 450));  // Was: 250px
```

**Rationale**:
- Per-token pattern: Quick visual scan (doesn't need much height)
- Accumulated pattern: Quantitative analysis (needs space to see values)

### Tooltip Implementation for Accumulated Graph

Added hover detection and tooltip rendering to `renderAccumulatedGraph()`:

```cpp
static const MemoryTensor* hovered_tensor = nullptr;
hovered_tensor = nullptr;  // Reset each frame

if (ImPlot::BeginPlot(...)) {
    // ... draw graph ...

    // Hover detection
    if (ImPlot::IsPlotHovered()) {
        ImPlotPoint mouse_pos = ImPlot::GetPlotMousePos();
        double mouse_gb = mouse_pos.x;

        // Find which tensor
        for (const auto& tensor : memoryMap.tensors) {
            double start_gb = tensor.offset_start / (1024.0 * 1024.0 * 1024.0);
            double end_gb = tensor.offset_end / (1024.0 * 1024.0 * 1024.0);
            if (mouse_gb >= start_gb && mouse_gb <= end_gb) {
                hovered_tensor = &tensor;
                break;
            }
        }
    }
}

// Show tooltip
if (hovered_tensor) {
    ImGui::BeginTooltip();
    ImGui::Text("Tensor: %s", hovered_tensor->name.c_str());
    // ... show layer, expert, size, offset ...
    ImGui::Text("Total Accesses: %u", accumulatedCounts[name]);
    ImGui::EndTooltip();
}
```

**Result**: Both plots now have full tooltip support showing tensor metadata + access counts.

### Color Consistency

**Issue**: Fill (alpha=0.6) and line (alpha=1.0) had different transparency.

**Fix**: Use identical color for both:
```cpp
ImVec4 blue_color = ImVec4(0.2f, 0.5f, 0.8f, 1.0f);  // Same for both
```

**Result**: Cleaner visual appearance, fill and outline match perfectly.

---

## Part 8: Experimental Methodology

### Prompt Design Considerations

**Question**: How to ensure domains are sufficiently different to test clustering hypothesis?

**Approach**: Choose prompts that engage different cognitive capabilities:

**Code (Structured Logic)**:
- Binary search tree (algorithm, data structures)
- Expected: Logic/structure-specialized experts

**Math (Numerical Reasoning)**:
- Train distance calculation (arithmetic, step-by-step)
- Expected: Math/reasoning experts

**Creative (Language Generation)**:
- Narrative story with sensory descriptions
- Expected: Language/creativity experts

**Factual (Knowledge Recall)**:
- CRISPR gene editing explanation
- Expected: Knowledge retrieval experts

**Mixed (Multi-Modal)**:
- Algorithm comparison + implementation
- Expected: Blend of code + reasoning experts
- Control condition to test if patterns combine

### Statistical Analysis Plan (After Data Collection)

**Phase 1: Domain × Expert Frequency Matrix**

```python
import numpy as np

# Build 5 × 32 matrix
freq_matrix = np.zeros((5, 32))  # 5 domains, 32 experts

for domain_idx, domain in enumerate(domains):
    for token_data in domain_tokens:
        for entry in token_data.entries:
            if entry['operation_type'] == 'MUL_MAT_ID':
                for expert_id in entry['expert_ids'][:4]:  # Top-4
                    freq_matrix[domain_idx, expert_id] += 1

# Normalize by domain
freq_matrix_pct = freq_matrix / freq_matrix.sum(axis=1, keepdims=True) * 100
```

**Expected result**:
- If experts specialize by domain: Each row (domain) has distinct hot experts
- If experts are general-purpose: All rows look similar

**Phase 2: Temporal Autocorrelation**

```python
from scipy.stats import pearsonr

for layer in range(24):
    # Build expert sequence for this layer across 100 tokens
    expert_seq = [get_experts_at_token(t, layer) for t in range(100)]

    # Compute autocorrelation with lag=1
    autocorr = correlation(expert_seq[:-1], expert_seq[1:])

    if autocorr > 0.5:
        print(f"Layer {layer}: Strong autocorrelation ({autocorr:.2f})")
        # Experts are "sticky" - repeatedly selected
```

**Phase 3: Transition Matrix (Markov Chain)**

```python
# P(expert_t+1 | expert_t)
transition_matrix = np.zeros((32, 32))

for t in range(99):  # 100 tokens, 99 transitions
    experts_t = get_experts_at_token(t)
    experts_t1 = get_experts_at_token(t+1)

    for e_now in experts_t:
        for e_next in experts_t1:
            transition_matrix[e_now, e_next] += 1

# Normalize rows (probability distribution)
transition_matrix /= transition_matrix.sum(axis=1, keepdims=True)
```

**If diagonal is high**: Experts are sticky (same expert reused)
**If uniform**: Random selection

**Phase 4: Clustering (K-means)**

```python
from sklearn.cluster import KMeans

# For each token, create feature vector: [freq_expert_0, ..., freq_expert_31]
token_vectors = []
for token in all_tokens:
    expert_freq = count_experts(token)  # 32-dimensional vector
    token_vectors.append(expert_freq)

# Cluster into k=5 groups (number of domains)
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(token_vectors)

# Check: Do clusters match original domains?
for domain_idx, domain_tokens in enumerate(tokens_by_domain):
    cluster_labels = [clusters[token_global_idx] for token_global_idx in domain_tokens]
    purity = max(Counter(cluster_labels).values()) / len(cluster_labels)
    print(f"{domain}: {purity*100:.1f}% purity")
```

**If purity > 80%**: Strong domain clustering (experts specialize)
**If purity ≈ 20%**: Random (no specialization)

---

## Part 9: Desktop UI Current Capabilities

### What Works

**Data Loading**:
- Loads 100 token traces (170,000+ entries)
- Loads 2,691 tensors (12.85 GB memory map)
- Calculates accumulated counts across all tokens
- Expert-level expansion (459 → 2,691 tensors)

**Visualization**:
- 50/50 split: Trace Table | Heatmap
- Token selector bar (switch between tokens)
- Per-token heatmap with timeline scrubbing
- Accumulated access graph (all tokens)
- Virtual scrolling table (handles millions of rows)
- Filtering: Layer, operation type, memory source

**Interaction**:
- Hover tooltips on both plots
- Tooltip shows: Name, layer, expert ID, category, size, offset, access count
- Timeline scrubbing updates per-token heatmap
- 110 FPS rendering (smooth)

### Performance Characteristics

**With 100 tokens loaded**:
- Memory usage: ~500 MB (data structures)
- Load time: ~15 seconds (100 JSON files)
- Rendering: 110 FPS (ImPlot GPU-accelerated)
- Tooltip response: Immediate (<1ms)

**Scalability**:
- Tested with 5 domains (500 tokens total)
- Desktop UI can handle the scale
- WebUI would crash (browser memory limits)

---

## Part 10: Graph Dumping Behavior (Investigation)

### User Question: Why Only 3 Graph Files?

**Observation**: 100 token traces generated, but only 3 graph files (token-00000.dot, token-00001.dot, token-00002.dot).

**Investigation**:

llama.cpp dumps computation graphs for first few tokens only:
- Token 0: Prompt processing phase (initial graph structure)
- Token 1: Generation phase start (KV cache now active, graph changes)
- Token 2: Steady state (confirms graph is stable)

**After token 2**: Graph structure doesn't change → no need to save 100 identical graphs.

**Why this is correct**:
- Graphs show model architecture (static)
- Traces show execution data (dynamic) ← What we need for expert analysis
- Saving 100 graphs = wasted disk space

**Verification**: Confirmed this is llama.cpp's default behavior (not configurable in our instrumentation).

**Conclusion**: Expected and correct - graph dumping limit is intentional optimization.

---

## Part 11: Technical Achievements Summary

### Bugs Fixed Today

| Issue | Impact | Solution |
|-------|--------|----------|
| Tooltip persistence | Tooltips stuck on screen | Reset hover state each frame |
| Nested windows | 50/50 split broken | Remove ImGui::Begin from views |
| Color mismatch | Fill/line different transparency | Use identical color for both |

### Code Quality Improvements

**Modularity**:
- Views render into provided window (caller controls layout)
- Separation of concerns (per-token vs accumulated logic)
- Reusable components (tooltip rendering, hover detection)

**User Experience**:
- Proper window layout (no manual dragging needed)
- Tooltips work on all plots
- Visual consistency (matching colors)
- Intuitive sizing (emphasis on accumulated pattern)

---

## Part 12: Current Status and Next Steps

### Completed Today

✅ Desktop UI fully functional with dual visualization
✅ All layout and tooltip bugs fixed
✅ 500-token expert activation dataset collected
✅ Automated experiment framework operational
✅ Documentation updated with system architecture

### Ready for Analysis

**Data available**:
- 5 domains × 100 tokens = 500 tokens
- Expert activation patterns captured
- All data loaded in Desktop UI

**Next steps**:
1. **Extract expert IDs** from all traces (Python script)
2. **Build frequency matrix** (Domain × Expert)
3. **Compute autocorrelation** (temporal patterns)
4. **Visualize transition matrix** (expert switching)
5. **Run clustering** (K-means on token vectors)
6. **Generate figures** for thesis

### Open Research Questions

**1. Do experts specialize by domain?**
- **Method**: Domain × Expert frequency heatmap
- **Expected**: If yes, each domain has distinct hot experts

**2. Are expert selections temporally correlated?**
- **Method**: Autocorrelation with lag=1
- **Expected**: If yes, expert at token t predicts expert at t+1

**3. Which experts are globally hot?**
- **Method**: Sum across all domains and tokens
- **Expected**: Identify top-10 most-used experts (candidates for caching)

**4. Does alphabetical GGUF ordering hurt SSD performance?**
- **Method**: Compare with layer-sequential version (future work)
- **Expected**: TBD - needs blktrace measurement

---

## Part 13: Comparison with Related Work (Updated)

### Our Contribution vs Fisher

**Fisher's contribution**:
- Heatmap tool for NUMA page placement
- Quantified NUMA policy impact (56% cold start penalty)
- Proposed offline prefetching based on patterns

**Our contribution**:
- SSD-backed scenario (different problem space)
- MoE expert sparsity analysis (Fisher used dense model)
- Tensor-level instrumentation (Fisher used 4-16 MB granules)
- Alphabetical ordering discovery (Fisher didn't investigate file layout)
- Multi-domain expert analysis (Fisher used single workload)

**Complementary, not redundant**: Different memory tiers, different architectures, different granularity.

---

## Part 14: Lessons Learned

### What Worked Well

**1. Modular Design Pays Off**
- Updating view rendering (remove ImGui::Begin) didn't break data loading
- Swapping per-token graph for accumulated graph = change one function
- Tooltip code reusable across plots

**2. Working with Existing Infrastructure**
- `run_expert_experiments.sh` leverages `settings.json` workflow
- Didn't modify `run_experiment.py` - less risk
- Automation script = 180 lines, minimal complexity

**3. User-Driven Iteration**
- "Tooltip doesn't disappear" → found exact bug
- "Make graph bigger" → simple height change
- "Colors don't match" → one-line fix
- Tight feedback loop (minutes from request to fix)

### Challenges Overcome

**1. Subplot Complexity**
- Initial dual-subplot approach was over-engineered
- Simpler: Separate plots, independent rendering
- Lesson: Don't force framework features if simpler solution exists

**2. Variable Interpolation in Shell**
- Bash + Python heredoc variable passing initially failed
- Solution: Use separate Python one-liners per variable
- More verbose but more reliable

**3. Understanding Existing Code**
- main.cpp already had accumulated graph logic
- Initially tried to reimplement in HeatmapView (duplication!)
- Better: Leverage existing code, just render it differently

---

## Conclusion

Today's work significantly improved the desktop UI's usability and set up a comprehensive framework for expert activation analysis. The dual visualization (per-token + accumulated) provides both temporal and aggregate views of memory access patterns, while the automated experiment framework enables systematic data collection across multiple domains.

**The desktop UI is now production-ready** for analyzing expert activation patterns and answering the key research questions about domain specialization and temporal correlation.

**Next session**: Analyze the collected 500-token dataset to extract expert activation patterns, build frequency matrices, and test hypotheses about domain clustering and temporal autocorrelation.

---

**End of Entry**
