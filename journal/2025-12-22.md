# December 22, 2025 - CORRECTED RESULTS: blktrace Action Filtering Fix

**CRITICAL**: This journal supersedes and INVALIDATES all previous findings (21DECP2.md, 21DECP3.md, 21DECP4.md).

---

## Executive Summary

**Major Bug Discovered**: Analysis code was counting each I/O operation **5 TIMES** due to missing blktrace action filtering.

**Fix Applied**: Filter by `action = 'D'` (Dispatch) to count only what actually reaches the disk.

**Impact**: Previous findings about "3x I/O amplification" and "67% backward seeks" were **ENTIRELY ARTIFACTS** of this bug.

**Key Corrected Findings**:
1. ‚úÖ LLM parameter access is **100% SEQUENTIAL** when model fits in RAM
2. ‚úÖ NO re-reading occurs when model fits in RAM (1.0x amplification, not 3x)
3. ‚úÖ "Backward seeks" only appear under memory pressure (thrashing artifacts)
4. ‚úÖ Thrashing threshold: Free RAM must exceed (model_size + ~0.5-1 GB overhead)
5. ‚ùì 100% model coverage for only 100 tokens needs investigation

---

## PART 1: The Bug and The Fix

### 1.1 What Was Wrong

**Original filter** (BROKEN):
```python
WHERE rwbs LIKE '%R%'
AND pid = {llama_pid}
AND sector >= {gguf_start_sector}
AND sector <= {gguf_end_sector}
```

**Problem**: blktrace records MULTIPLE actions for each I/O operation:

| Action | Meaning | When Recorded |
|--------|---------|---------------|
| `Q` | Queued | App requests I/O |
| `G` | Get request | Kernel allocates request structure |
| `P` | Plug | I/O scheduler batches requests |
| `D` | Dispatch | Request sent to NVMe driver |
| `C` | Complete | Data arrives in memory |

**All 5 actions had the SAME sector and size!**

**Result**: Every I/O operation counted 5 times ‚Üí 5x inflation of all metrics!

### 1.2 Evidence of the Bug

**Experiment 20251222_110125** before fix:
```bash
$ awk '{if (NF >= 6) print $6}' blkparse_raw.txt | grep -E '^[A-Z]$' | sort | uniq -c
  52670 Q
  52670 P
  52670 G
  52670 D
  52670 C
```

**EXACTLY 52,670 of each action** ‚Üí Each I/O tracked through full lifecycle!

Our analysis was capturing ALL 52,670 √ó 5 = 263,350 events instead of 52,670 unique I/Os.

### 1.3 The Fix

**Corrected filter** (FIXED):
```python
WHERE action = 'D' AND rwbs LIKE '%R%'
AND pid = {llama_pid}
AND sector >= {gguf_start_sector}
AND sector <= {gguf_end_sector}
```

**Why 'D' (Dispatch)?**
- Shows what ACTUALLY reached the NVMe device
- After I/O scheduler optimization and merging
- One-to-one mapping with physical disk I/O
- Most relevant for understanding SSD access patterns

### 1.4 Code Change

**File**: `Benchmark/utils/analysis_tools.py:251`

**Commit**: 2025-12-22 11:00 UTC

```python
# Line 251: Added action = 'D' filter
WHERE action = 'D' AND rwbs LIKE '%R%'
```

---

## PART 2: Corrected Experimental Results

### 2.1 Experiment Matrix

**Model**: gpt-oss-20b-F16.gguf (12.85 GB)
**Tokens**: 100
**Date**: 2025-12-22
**Hardware**: nvme0n1 (1 extent, contiguous file)
**System RAM**: 30 GB total

Five experiments with increasing memory pressure:

| Exp ID | mlock | Free RAM | Duration | tok/s |
|--------|-------|----------|----------|-------|
| 113019 | 0 GB  | 30 GB    | 20.68s   | 4.84  |
| 113241 | 15 GB | 15 GB    | 21.33s   | 4.69  |
| 113330 | 16 GB | 14 GB    | 28.85s   | 3.47  |
| 113421 | 17 GB | 13 GB    | 30.00s   | 3.33  |
| 113512 | 20 GB | 10 GB    | 33.15s   | 3.02  |

### 2.2 Complete Results Table

| Exp ID | mlock | Free RAM | Time (s) | tok/s | Total Read (GB) | Unique (GB) | Amplification | Cache Œî (GB) | Sequential | Backward | Small Gap | Med Gap | Large Gap |
|--------|-------|----------|----------|-------|-----------------|-------------|---------------|--------------|------------|----------|-----------|---------|-----------|
| 113019 | **0 GB** | **30 GB** | 20.68 | **4.84** | 12.85 | 12.85 | **1.00x** | 12.93 | **100.0%** | **0.0%** | 0.0% | 0.0% | 0.0% |
| 113241 | 15 GB | 15 GB | 21.33 | 4.69 | 12.91 | 12.85 | 1.00x | 12.68 | **98.5%** | 0.0% | 0.9% | 0.3% | 0.3% |
| 113330 | 16 GB | 14 GB | 28.85 | 3.47 | 13.88 | 12.85 | 1.08x | 11.86 | 80.1% | 0.5% | 10.6% | 4.7% | 4.1% |
| 113421 | 17 GB | 13 GB | 30.00 | 3.33 | 14.06 | 12.85 | 1.09x | 10.83 | 76.9% | 0.4% | 12.5% | 5.8% | 4.4% |
| 113512 | 20 GB | 10 GB | 33.15 | **3.02** | 14.52 | 12.85 | 1.13x | 7.89 | 70.7% | 0.8% | 15.1% | 7.3% | 6.2% |

**Column Definitions**:
- **Total Read**: Sum of all I/O operations (may include re-reads)
- **Unique**: Distinct sectors accessed (actual model data touched)
- **Amplification**: Total / Unique (re-read factor)
- **Cache Œî**: Increase in system page cache (mem_after - mem_before)
- **Sequential**: Perfect consecutive sector access (gap = 0)
- **Backward**: Sector decreases from previous access (gap < 0)
- **Small/Med/Large Gap**: Forward seeks with gaps (<128KB, 128KB-1MB, >1MB)

---

## PART 3: Critical Findings

### 3.1 Finding #1: LLM Access is 100% Sequential (NOT Random!) ‚úÖ

**Old (INVALID) Claim**:
> "~67% backward seeks are intrinsic to transformer inference, consistent across models"

**Corrected Reality**:
```
0 GB lock (no memory pressure):
- Sequential: 100.0%
- Backward:     0.0% (only 1 out of 52,668 operations!)
- Gaps:         0.0%
```

**Interpretation**:
When the model fits comfortably in RAM, LLM inference accesses parameters in **PERFECTLY SEQUENTIAL ORDER**.

**Why This Makes Sense**:
- Transformer layers are processed in order (layer 0 ‚Üí 1 ‚Üí 2 ‚Üí ... ‚Üí N)
- Parameters are stored sequentially in the .gguf file
- llama.cpp reads through the file sequentially during inference
- NO random jumping between attention heads or embedding tables

**Critical Implication for Thesis**:
- ‚úÖ LLM parameter offloading to SSD is HIGHLY FAVORABLE
- ‚úÖ Sequential access ‚Üí NVMe sequential read performance (~3-3.5 GB/s)
- ‚úÖ No need for complex prefetching or prediction algorithms
- ‚úÖ Simple sequential readahead should work optimally

---

### 3.2 Finding #2: NO Re-Reading When Model Fits (NOT 3x!) ‚úÖ

**Old (INVALID) Claim**:
> "3x I/O amplification factor is consistent across models"
> "Total reads: 38.80 GB / Unique: 12.85 GB = 3.02x"

**Corrected Reality**:
```
0 GB lock:  12.85 GB total / 12.85 GB unique = 1.00x amplification
```

**Interpretation**:
When model fits in RAM, there is **ZERO re-reading**. Every sector is read EXACTLY ONCE.

The "3x amplification" was entirely from counting each I/O operation 5 times (Q, G, P, D, C actions).

**Critical Implication for Thesis**:
- ‚úÖ No redundant I/O when model fits
- ‚úÖ Efficient memory-mapped access
- ‚úÖ Validates llama.cpp's streaming architecture

---

### 3.3 Finding #3: "Backward Seeks" Are Thrashing Artifacts ‚úÖ

**Old (INVALID) Claim**:
> "Backward seeks remain constant (~75%) regardless of memory pressure"
> "Proves fragmentation is dominant factor, not inference access pattern"

**Corrected Reality - Backward Seeks vs Memory Pressure**:

| mlock | Free RAM | Sequential | Backward | Interpretation |
|-------|----------|------------|----------|----------------|
| 0 GB  | 30 GB    | **100.0%** | **0.0%** | Model fits perfectly |
| 15 GB | 15 GB    | **98.5%**  | **0.0%** | Still fits well |
| 16 GB | 14 GB    | 80.1%      | **0.5%** | **Thrashing begins!** |
| 17 GB | 13 GB    | 76.9%      | 0.4%     | Moderate thrashing |
| 20 GB | 10 GB    | 70.7%      | **0.8%** | Heavy thrashing |

**Interpretation**:
"Backward seeks" are NOT from transformer architecture! They appear ONLY under memory pressure:

**What causes them**:
1. Page evictions under memory pressure
2. Kernel trying to reload recently-evicted pages
3. I/O scheduler dealing with out-of-order page faults
4. Multiple threads/processes competing for limited page cache

**Critical Implication for Thesis**:
- ‚úÖ True transformer access = sequential
- ‚úÖ "Random" access is kernel thrashing artifact
- ‚úÖ Design offloading systems for sequential I/O, not random

---

### 3.4 Finding #4: Sharp Thrashing Cliff at 16 GB Lock ‚ö†Ô∏è

**Performance Degradation**:

| mlock | Free RAM | tok/s | Œî from baseline |
|-------|----------|-------|-----------------|
| 0 GB  | 30 GB    | 4.84  | baseline        |
| 15 GB | 15 GB    | 4.69  | **-3%** ‚úì       |
| 16 GB | 14 GB    | 3.47  | **-28%** üö®     |
| 17 GB | 13 GB    | 3.33  | -31%            |
| 20 GB | 10 GB    | 3.02  | -38%            |

**Sharp cliff between 15 GB and 16 GB!**

**Analysis**:
```
Model file:        12.85 GB
Overhead (pmap):   ~0.5-1.0 GB (KV cache, buffers, libs)
Total needed:      ~13.4-13.8 GB

16 GB lock ‚Üí 14 GB free < 13.8 GB ‚Üí DOESN'T QUITE FIT!
```

**Thrashing Threshold Formula**:
```
Free RAM ‚â• (model_size + overhead)

Where overhead ‚âà 0.5-1.0 GB for 100 tokens
```

**Critical Implication for Thesis**:
- ‚ö†Ô∏è Must ensure available RAM > model size + overhead
- ‚ö†Ô∏è Even small deficit (0.5 GB) causes 28% slowdown
- ‚ö†Ô∏è Thrashing threshold is SHARP, not gradual

---

### 3.5 Finding #5: Re-Reading Only Happens Under Memory Pressure

**I/O Amplification vs Memory Pressure**:

| mlock | Free RAM | Total (GB) | Unique (GB) | Amplification | Re-reads (GB) |
|-------|----------|------------|-------------|---------------|---------------|
| 0 GB  | 30 GB    | 12.85      | 12.85       | 1.00x         | 0.00          |
| 15 GB | 15 GB    | 12.91      | 12.85       | 1.00x         | 0.06          |
| 16 GB | 14 GB    | 13.88      | 12.85       | 1.08x         | **1.03**      |
| 17 GB | 13 GB    | 14.06      | 12.85       | 1.09x         | 1.21          |
| 20 GB | 10 GB    | 14.52      | 12.85       | 1.13x         | **1.67**      |

**Interpretation**:
- When model fits: ZERO re-reads
- Under pressure: 8-13% extra I/O from page evictions

**What gets re-read?**
Likely hypothesis:
1. Hot parameters (embeddings, frequently used layer weights)
2. Pages evicted by LRU during inference
3. Pages touched early, evicted, needed again at end

**Critical Implication for Thesis**:
- ‚úÖ Re-reading is MINIMAL even under pressure (1.13x max)
- ‚úÖ Much better than expected (not 3x!)
- ‚úÖ Suggests llama.cpp has good locality of reference

---

### 3.6 Finding #6: cache_delta Validates Unique Sectors Metric ‚úÖ

**Comparison at Low Memory Pressure**:

| mlock | Unique Sectors (GB) | Cache Œî (GB) | Match % | Status |
|-------|---------------------|--------------|---------|--------|
| 0 GB  | 12.85               | 12.93        | 100.6%  | ‚úÖ Excellent |
| 15 GB | 12.85               | 12.68        | 98.7%   | ‚úÖ Excellent |

**Interpretation**:
At low memory pressure, cache_delta (system-wide page cache change) matches unique sectors (blktrace analysis) within 1-2%!

**This proves**:
1. ‚úÖ Unique sectors calculation is correct
2. ‚úÖ drop_caches() works properly
3. ‚úÖ Minimal contamination from other processes
4. ‚úÖ Both metrics measure the same thing: actual model data cached

**Under Memory Pressure**:

| mlock | Unique Sectors (GB) | Cache Œî (GB) | Match % | Status |
|-------|---------------------|--------------|---------|--------|
| 16 GB | 12.85               | 11.86        | 92.3%   | Some eviction |
| 17 GB | 12.85               | 10.83        | 84.3%   | More eviction |
| 20 GB | 12.85               | 7.89         | 61.4%   | Heavy eviction |

**Interpretation**:
Under memory pressure, cache_delta < unique_sectors because kernel evicts pages DURING inference.

- Unique sectors: Total data ACCESSED (from blktrace I/O log)
- Cache delta: Data STILL RESIDENT at end (from /proc/meminfo)

**Difference = Pages accessed then evicted during inference**

---

## PART 4: Questionable Findings Requiring Further Investigation

### 4.1 Question #1: Is 100% Coverage Reasonable? ‚ùì

**Observation**:
```
Model size:      12.85 GB
Unique accessed: 12.85 GB (100.0%)
Tokens generated: 100
```

**For only 100 tokens, we access the ENTIRE 12.85 GB model!**

**Possible Explanations**:

**Hypothesis A: Initialization File Scan**
- llama.cpp scans entire .gguf file during model loading
- Validates metadata, checksums, format
- Happens before first token generation
- ALL subsequent tokens use cached data

**Hypothesis B: True Inference Behavior**
- 100 tokens requires processing through ALL transformer layers
- Each layer touches its parameters
- Even unused vocabulary embeddings are mapped into memory

**Test to Determine**:
```bash
Run experiments with varying token counts:
- 10 tokens   ‚Üí If still 100%, proves initialization scan
- 100 tokens  ‚Üí Baseline (current)
- 1000 tokens ‚Üí If still 100%, validates hypothesis
```

**Expected Results**:
- If initialization: ALL experiments show 100% coverage
- If inference: Coverage increases with token count

**Critical for Thesis**:
- Need to understand if 100% is fundamental or initialization artifact
- Affects design of partial model loading strategies

---

### 4.2 Question #2: Is 1.0x Amplification (No Re-reading) Reasonable? ‚ùì

**Observation**:
```
0 GB lock: 12.85 GB total / 12.85 GB unique = 1.00x
```

**Every sector read EXACTLY ONCE. No re-reading!**

**Why This Might Be Correct**:

‚úÖ **Memory-mapped I/O**
- llama.cpp uses mmap() to map .gguf file
- Kernel handles page faults transparently
- Each page loaded exactly once on first access
- Stays cached for duration of inference

‚úÖ **Sequential Access Pattern**
- Transformers process layers sequentially
- No need to re-visit previous layers
- Parameters accessed once per forward pass

‚úÖ **Sufficient RAM**
- 30 GB free >> 12.85 GB model
- Everything stays resident in page cache
- No eviction pressure

**Why This Might Be Suspicious**:

‚ùì **What about KV cache?**
- Attention mechanism needs key-value pairs from all previous tokens
- Does this require re-reading embeddings?
- Or is KV cache separate from model file?

‚ùì **What about multi-head attention?**
- Different heads access Q, K, V matrices
- Are these sequential or require jumping?

‚ùì **What about kernel readahead?**
- Kernel might speculatively read ahead
- Could this cause "over-reading"?

**Test to Validate**:
```bash
strace -e trace=read,pread64,mmap llama-cli ...
# Analyze:
# 1. Is model file mmap'd or read()?
# 2. How many times is file descriptor accessed?
# 3. Are there sequential read() calls or single mmap?
```

**Critical for Thesis**:
- Need to understand llama.cpp's I/O architecture
- Validates assumptions about offloading efficiency

---

### 4.3 Question #3: Is 100% Sequential Access Reasonable? ‚ùì

**Observation**:
```
0 GB lock:
- Perfect sequential (gap=0): 100.0%
- Backward seeks:              0.0%
- Forward gaps:                0.0%
```

**52,668 consecutive I/O operations with ZERO gaps!**

**Why This Might Be Correct**:

‚úÖ **Layer-by-layer processing**
- Transformer processes layer 0 ‚Üí 1 ‚Üí 2 ‚Üí ... ‚Üí N
- Parameters stored in same order in file
- Results in sequential file access

‚úÖ **Memory-mapped streaming**
- mmap() with sequential access hint (MADV_SEQUENTIAL)
- Kernel optimizes for sequential pattern
- Readahead prefetches next pages

‚úÖ **Single-threaded inference**
- llama-cli processes one token at a time
- No parallel threads causing interleaved access

**Why This Might Be Suspicious**:

‚ùì **What about attention?**
- Multi-head attention accesses Q, K, V matrices
- Are these laid out sequentially in file?
- Or do different heads jump around?

‚ùì **What about embeddings?**
- Vocabulary embeddings: ~500 MB
- Only small fraction needed per token
- Why read entire embedding table sequentially?

‚ùì **What about quantization?**
- F16 quantization groups parameters
- Does this affect access pattern?

**Critical for Thesis**:
- If truly sequential: ‚úÖ SSD offloading is highly efficient
- If random: ‚ö†Ô∏è Need sophisticated prefetching
- Need to profile attention mechanism specifically

---

### 4.4 Question #4: Is 1.08-1.13x Amplification Under Pressure the Interesting Finding? üî¨

**Observation**:

| mlock | Amplification | Re-reads (GB) |
|-------|---------------|---------------|
| 16 GB | 1.08x         | 1.03          |
| 17 GB | 1.09x         | 1.21          |
| 20 GB | 1.13x         | 1.67          |

**Even under severe memory pressure (20 GB lock, 10 GB free), only 13% extra I/O!**

**Why This Is Interesting**:

‚úÖ **Much better than expected**
- Naive assumption: thrashing causes 10x or 100x re-reads
- Reality: only 1.13x even at heavy pressure
- Suggests good locality of reference in llama.cpp

‚úÖ **Predictable degradation**
- Re-reads increase gradually with memory pressure
- Not chaotic or unpredictable
- Could model this relationship

‚úÖ **Still functional under pressure**
- 3.02 tok/s at 20 GB lock vs 4.84 tok/s baseline
- 38% slower, but not unusably slow
- Suggests offloading is viable even with limited RAM

**Research Questions**:

1. **What pages get evicted and re-read?**
   - Hot parameters? (embeddings, layer norms)
   - Recently used layers?
   - Random eviction by LRU?

2. **Can we predict which pages to pin in RAM?**
   - Keep embeddings resident
   - Allow layer weights to swap
   - Optimize for minimum re-reads

3. **How does this scale with larger models?**
   - gpt-oss-20b: 13% extra I/O at severe pressure
   - Would 100 GB model show 13% or 130%?

**Critical for Thesis**:
- ‚úÖ This is the REAL finding about memory-constrained inference
- ‚úÖ Quantify exact tradeoff: RAM vs performance vs I/O overhead
- ‚úÖ Design intelligent page replacement policies

---

## PART 5: Implications for Thesis

### 5.1 What We Now Know with High Confidence

1. **‚úÖ LLM parameter access is sequential**
   - 100% sequential when model fits in RAM
   - No random seeking or jumping
   - Ideal for SSD streaming architectures

2. **‚úÖ No redundant I/O when model fits**
   - 1.0x amplification (every sector read once)
   - Efficient memory-mapped access
   - Validates llama.cpp's design

3. **‚úÖ Thrashing threshold is sharp and predictable**
   - Free RAM must exceed (model_size + ~1 GB)
   - Sharp performance cliff when threshold crossed
   - Gradual degradation beyond threshold

4. **‚úÖ Re-reading is minimal even under pressure**
   - Only 13% extra I/O at severe memory pressure
   - Much better than naive expectations
   - Suggests parameter offloading is viable

5. **‚úÖ Measurement methodology is now reliable**
   - cache_delta matches unique sectors at low pressure
   - Bandwidth numbers are realistic
   - Gap distribution is meaningful

### 5.2 What We Still Need to Investigate

1. **‚ùì 100% coverage for 100 tokens**
   - Test with varying token counts (10, 100, 1000)
   - Determine if initialization or inference
   - Understand parameter access patterns

2. **‚ùì 1.0x amplification correctness**
   - Profile with strace to see actual syscalls
   - Understand mmap vs read() usage
   - Validate with llama.cpp source code

3. **‚ùì 100% sequential access pattern**
   - Profile attention mechanism specifically
   - Understand embedding table access
   - Validate with different model architectures

4. **‚ùì What gets re-read under pressure?**
   - Analyze temporal patterns in blktrace
   - Identify hot parameters
   - Design optimal page pinning strategies

### 5.3 Thesis Contributions (Revised)

**Primary Contribution**:
"LLM parameter offloading to SSD is highly efficient due to sequential access patterns and minimal re-reading even under memory pressure."

**Supporting Evidence**:
1. 100% sequential access when model fits (not random)
2. 1.0x I/O amplification with sufficient RAM (no redundancy)
3. Only 1.13x amplification under severe pressure (13% overhead)
4. Sharp thrashing threshold allows predictable capacity planning

**Design Implications**:
1. Sequential readahead is sufficient (no complex prefetching)
2. Simple LRU eviction is adequate (minimal re-reading)
3. Target: Free RAM ‚â• model_size + 1 GB for optimal performance
4. Expect ~30-40% slowdown when RAM insufficient but still functional

### 5.4 What Previous Findings Are Now Invalid

‚ùå **"67% backward seeks intrinsic to transformers"** ‚Üí FALSE
- Access is 100% sequential
- Backward seeks are thrashing artifacts

‚ùå **"3x I/O amplification factor"** ‚Üí FALSE
- Was 5x overcounting artifact
- True amplification is 1.0x when model fits

‚ùå **"Fragmentation causes 8% backward seeks"** ‚Üí UNTESTED
- Previous experiments had action overcounting
- Need to re-run with corrected methodology

‚ùå **"Bandwidth 6-7 GB/s"** ‚Üí FALSE
- Was measuring queue submission, not actual I/O
- True bandwidth ~1 GB/s under pressure

---

## PART 6: Next Steps (Prioritized)

### Priority 1: Validate 100% Coverage Finding üî¨

**Test Plan**:
```bash
# Vary token count
tokens=10    ‚Üí Check unique sectors
tokens=100   ‚Üí Baseline (current)
tokens=1000  ‚Üí Check unique sectors
tokens=10000 ‚Üí Check unique sectors
```

**Expected Outcomes**:
- If same coverage: Proves initialization scan
- If increasing: Proves inference dependency

**Timeline**: 4 experiments √ó 30 min = 2 hours

---

### Priority 2: Profile with strace üî¨

**Test Plan**:
```bash
strace -e trace=read,pread64,mmap,madvise -o strace.log \
    llama-cli -m model.gguf -p "test" -n 100

# Analyze:
grep "model.gguf" strace.log
# Count read() calls
# Check for mmap() calls
# Look for MADV_SEQUENTIAL
```

**Goal**: Understand llama.cpp's actual I/O syscalls

**Timeline**: 1 hour

---

### Priority 3: Re-run Experiments for llama-2-7b üî¨

**Test Plan**:
```bash
# Use CORRECTED methodology (action='D')
model=llama-2-7b-chat.Q4_K_M.gguf (3.80 GB)
tokens=100

# Test matrix:
mlock=0 GB   ‚Üí Baseline
mlock=10 GB  ‚Üí Comfortable fit
mlock=15 GB  ‚Üí Near threshold
mlock=17 GB  ‚Üí Beyond threshold
```

**Goal**: Validate findings scale to smaller models

**Timeline**: 4 experiments √ó 15 min = 1 hour

---

### Priority 4: Analyze Temporal Patterns in blktrace üî¨

**Test Plan**:
```bash
# Split blktrace by time phase
# Phase 1: First 2 seconds (initialization)
# Phase 2: Remaining time (inference)

# Analyze each separately:
# - Unique sectors per phase
# - Sequential vs random per phase
# - Which sectors accessed multiple times
```

**Goal**: Separate initialization from inference I/O

**Timeline**: Custom analysis script + 1 experiment = 2 hours

---

### Priority 5: Investigate What Gets Re-Read Under Pressure üî¨

**Test Plan**:
```bash
# For 20 GB lock experiment:
# Analyze blktrace to find:
# 1. Which sectors are read multiple times?
# 2. What is their offset in model file?
# 3. Are they embeddings? Layer weights? Norms?

# Map sector ‚Üí file offset ‚Üí parameter type
```

**Goal**: Identify hot parameters for pinning

**Timeline**: 4 hours for detailed analysis

---

## PART 7: Summary and Conclusions

### Key Takeaways

1. **üîß Critical Bug Fixed**: blktrace action filtering corrected
2. **‚úÖ Sequential Access**: LLM inference is 100% sequential (not random!)
3. **‚úÖ Minimal Re-reading**: Only 13% extra I/O under severe pressure
4. **‚úÖ Sharp Threshold**: Thrashing starts when free RAM < model + 1 GB
5. **‚ùì Open Questions**: 100% coverage, 1.0x amplification need validation

### What Changed in Methodology

**Before** (BROKEN):
```python
WHERE rwbs LIKE '%R%'
```
‚Üí Counted each I/O 5 times (Q, G, P, D, C)

**After** (FIXED):
```python
WHERE action = 'D' AND rwbs LIKE '%R%'
```
‚Üí Counts only Dispatch (actual disk I/O)

### Impact on Previous Work

**INVALIDATED**:
- 21DECP2.md (tmpfs experiments)
- 21DECP3.md (fragmentation experiments)
- 21DECP4.md (gpt-oss MoE experiments)

All moved to `journal/INVALIDATED/` folder.

### Confidence Levels

**High Confidence** ‚úÖ:
- Sequential access pattern
- Minimal re-reading
- Thrashing threshold
- Measurement methodology

**Medium Confidence** ‚ö†Ô∏è:
- 100% coverage (needs token count validation)
- 1.0x amplification (needs strace validation)

**Low Confidence** ‚ùì:
- Exact thrashing mechanisms
- What parameters get re-read
- How findings scale to larger models

---

**Document Status**: ‚úÖ VALID - Uses corrected methodology
**Last Updated**: 2025-12-22
**Next Review**: After validation experiments (token count, strace, llama-2-7b)



# OLD NOTES: 
# December 22, 2025 - CORRECTED RESULTS: blktrace Action Filtering Fix

**CRITICAL**: This journal supersedes and INVALIDATES all previous findings (21DECP2.md, 21DECP3.md, 21DECP4.md).

---

## Executive Summary

**Major Bug Discovered**: Analysis code was counting each I/O operation **5 TIMES** due to missing blktrace action filtering.

**Fix Applied**: Filter by `action = 'D'` (Dispatch) to count only what actually reaches the disk.
Context: **blktrace** is used for the block layer and records the lifecycle of block I/O requests. The Kernel emits events into per CPU relay buffers


**Impact**: Previous findings about "3x I/O amplification" and "67% backward seeks" were **ENTIRELY ARTIFACTS** of this bug.

**Key Corrected Findings**:
1. ‚úÖ LLM parameter access is **100% SEQUENTIAL** when model fits in RAM
2. ‚úÖ NO re-reading occurs when model fits in RAM (1.0x amplification, not 3x)
3. ‚úÖ "Backward seeks" only appear under memory pressure (thrashing artifacts)
4. ‚úÖ Thrashing threshold: Free RAM must exceed (model_size + ~0.5-1 GB overhead)
5. ‚ùì 100% model coverage for only 100 tokens needs investigation

---

## PART 1: The Bug and The Fix

### 1.1 What Was Wrong

**Original filter** (BROKEN):
```python
WHERE rwbs LIKE '%R%'
AND pid = {llama_pid}
AND sector >= {gguf_start_sector}
AND sector <= {gguf_end_sector}
```

**Problem**: blktrace records MULTIPLE actions for each I/O operation:

| Action | Meaning | When Recorded |
|--------|---------|---------------|
| `Q` | Queued | App requests I/O |
| `G` | Get request | Kernel allocates request structure |
| `P` | Plug | I/O scheduler batches requests |
| `D` | Dispatch | Request sent to NVMe driver |
| `C` | Complete | Data arrives in memory |

**All 5 actions had the SAME sector and size!**

**Result**: Every I/O operation counted 5 times ‚Üí 5x inflation of all metrics!

### 1.2 Evidence of the Bug

**Experiment 20251222_110125** before fix:
```bash
$ awk '{if (NF >= 6) print $6}' blkparse_raw.txt | grep -E '^[A-Z]$' | sort | uniq -c
  52670 Q
  52670 P
  52670 G
  52670 D
  52670 C
```

**EXACTLY 52,670 of each action** ‚Üí Each I/O tracked through full lifecycle!

Our analysis was capturing ALL 52,670 √ó 5 = 263,350 events instead of 52,670 unique I/Os.

### 1.3 The Fix

**Corrected filter** (FIXED):
```python
WHERE action = 'D' AND rwbs LIKE '%R%'
AND pid = {llama_pid}
AND sector >= {gguf_start_sector}
AND sector <= {gguf_end_sector}
```

**Why 'D' (Dispatch)?**
- Shows what ACTUALLY reached the NVMe device
- After I/O scheduler optimization and merging
- One-to-one mapping with physical disk I/O
- Most relevant for understanding SSD access patterns

### 1.4 Code Change

**File**: `Benchmark/utils/analysis_tools.py:251`

**Commit**: 2025-12-22 11:00 UTC

```python
# Line 251: Added action = 'D' filter
WHERE action = 'D' AND rwbs LIKE '%R%'
```

---

## PART 2: Corrected Experimental Results

### 2.1 Experiment Matrix

**Model**: gpt-oss-20b-F16.gguf (12.85 GB)
**Tokens**: 100
**Date**: 2025-12-22
**Hardware**: nvme0n1 (1 extent, contiguous file)
**System RAM**: 30 GB total

Five experiments with increasing memory pressure:

| Exp ID | mlock | Free RAM | Duration | tok/s |
|--------|-------|----------|----------|-------|
| 113019 | 0 GB  | 30 GB    | 20.68s   | 4.84  |
| 113241 | 15 GB | 15 GB    | 21.33s   | 4.69  |
| 113330 | 16 GB | 14 GB    | 28.85s   | 3.47  |
| 113421 | 17 GB | 13 GB    | 30.00s   | 3.33  |
| 113512 | 20 GB | 10 GB    | 33.15s   | 3.02  |

### 2.2 Complete Results Table

| Exp ID | mlock | Free RAM | Time (s) | tok/s | Total Read (GB) | Unique (GB) | Amplification | Cache Œî (GB) | Sequential | Backward | Small Gap | Med Gap | Large Gap |
|--------|-------|----------|----------|-------|-----------------|-------------|---------------|--------------|------------|----------|-----------|---------|-----------|
| 113019 | **0 GB** | **30 GB** | 20.68 | **4.84** | 12.85 | 12.85 | **1.00x** | 12.93 | **100.0%** | **0.0%** | 0.0% | 0.0% | 0.0% |
| 113241 | 15 GB | 15 GB | 21.33 | 4.69 | 12.91 | 12.85 | 1.00x | 12.68 | **98.5%** | 0.0% | 0.9% | 0.3% | 0.3% |
| 113330 | 16 GB | 14 GB | 28.85 | 3.47 | 13.88 | 12.85 | 1.08x | 11.86 | 80.1% | 0.5% | 10.6% | 4.7% | 4.1% |
| 113421 | 17 GB | 13 GB | 30.00 | 3.33 | 14.06 | 12.85 | 1.09x | 10.83 | 76.9% | 0.4% | 12.5% | 5.8% | 4.4% |
| 113512 | 20 GB | 10 GB | 33.15 | **3.02** | 14.52 | 12.85 | 1.13x | 7.89 | 70.7% | 0.8% | 15.1% | 7.3% | 6.2% |

**Column Definitions**:
- **Total Read**: Sum of all I/O operations (may include re-reads)
- **Unique**: Distinct sectors accessed (actual model data touched)
- **Amplification**: Total / Unique (re-read factor)
- **Cache Œî**: Increase in system page cache (mem_after - mem_before)
- **Sequential**: Perfect consecutive sector access (gap = 0)
- **Backward**: Sector decreases from previous access (gap < 0)
- **Small/Med/Large Gap**: Forward seeks with gaps (<128KB, 128KB-1MB, >1MB)

---

## PART 3: Critical Findings

### 3.1 Finding #1: LLM Access is 100% Sequential (NOT Random!) ‚úÖ

**Old (INVALID) Claim**:
> "~67% backward seeks are intrinsic to transformer inference, consistent across models"

**Corrected Reality**:
```
0 GB lock (no memory pressure):
- Sequential: 100.0%
- Backward:     0.0% (only 1 out of 52,668 operations!)
- Gaps:         0.0%
```

**Interpretation**:
When the model fits comfortably in RAM, LLM inference accesses parameters in **PERFECTLY SEQUENTIAL ORDER**.

**Why This Makes Sense**:
- Transformer layers are processed in order (layer 0 ‚Üí 1 ‚Üí 2 ‚Üí ... ‚Üí N)
- Parameters are stored sequentially in the .gguf file
- llama.cpp reads through the file sequentially during inference
- NO random jumping between attention heads or embedding tables

**Critical Implication for Thesis**:
- ‚úÖ LLM parameter offloading to SSD is HIGHLY FAVORABLE
- ‚úÖ Sequential access ‚Üí NVMe sequential read performance (~3-3.5 GB/s)
- ‚úÖ No need for complex prefetching or prediction algorithms
- ‚úÖ Simple sequential readahead should work optimally

---

### 3.2 Finding #2: NO Re-Reading When Model Fits (NOT 3x!) ‚úÖ

**Old (INVALID) Claim**:
> "3x I/O amplification factor is consistent across models"
> "Total reads: 38.80 GB / Unique: 12.85 GB = 3.02x"

**Corrected Reality**:
```
0 GB lock:  12.85 GB total / 12.85 GB unique = 1.00x amplification
```

**Interpretation**:
When model fits in RAM, there is **ZERO re-reading**. Every sector is read EXACTLY ONCE.

The "3x amplification" was entirely from counting each I/O operation 5 times (Q, G, P, D, C actions).

**Critical Implication for Thesis**:
- ‚úÖ No redundant I/O when model fits
- ‚úÖ Efficient memory-mapped access
- ‚úÖ Validates llama.cpp's streaming architecture

---

### 3.3 Finding #3: "Backward Seeks" Are Thrashing Artifacts ‚úÖ

**Old (INVALID) Claim**:
> "Backward seeks remain constant (~75%) regardless of memory pressure"
> "Proves fragmentation is dominant factor, not inference access pattern"

**Corrected Reality - Backward Seeks vs Memory Pressure**:

| mlock | Free RAM | Sequential | Backward | Interpretation |
|-------|----------|------------|----------|----------------|
| 0 GB  | 30 GB    | **100.0%** | **0.0%** | Model fits perfectly |
| 15 GB | 15 GB    | **98.5%**  | **0.0%** | Still fits well |
| 16 GB | 14 GB    | 80.1%      | **0.5%** | **Thrashing begins!** |
| 17 GB | 13 GB    | 76.9%      | 0.4%     | Moderate thrashing |
| 20 GB | 10 GB    | 70.7%      | **0.8%** | Heavy thrashing |

**Interpretation**:
"Backward seeks" are NOT from transformer architecture! They appear ONLY under memory pressure:

**What causes them**:
1. Page evictions under memory pressure
2. Kernel trying to reload recently-evicted pages
3. I/O scheduler dealing with out-of-order page faults
4. Multiple threads/processes competing for limited page cache

**Critical Implication for Thesis**:
- ‚úÖ True transformer access = sequential
- ‚úÖ "Random" access is kernel thrashing artifact
- ‚úÖ Design offloading systems for sequential I/O, not random

---

### 3.4 Finding #4: Sharp Thrashing Cliff at 16 GB Lock ‚ö†Ô∏è

**Performance Degradation**:

| mlock | Free RAM | tok/s | Œî from baseline |
|-------|----------|-------|-----------------|
| 0 GB  | 30 GB    | 4.84  | baseline        |
| 15 GB | 15 GB    | 4.69  | **-3%** ‚úì       |
| 16 GB | 14 GB    | 3.47  | **-28%** üö®     |
| 17 GB | 13 GB    | 3.33  | -31%            |
| 20 GB | 10 GB    | 3.02  | -38%            |

**Sharp cliff between 15 GB and 16 GB!**

**Analysis**:
```
Model file:        12.85 GB
Overhead (pmap):   ~0.5-1.0 GB (KV cache, buffers, libs)
Total needed:      ~13.4-13.8 GB

16 GB lock ‚Üí 14 GB free < 13.8 GB ‚Üí DOESN'T QUITE FIT!
```

**Thrashing Threshold Formula**:
```
Free RAM ‚â• (model_size + overhead)

Where overhead ‚âà 0.5-1.0 GB for 100 tokens
```

**Critical Implication for Thesis**:
- ‚ö†Ô∏è Must ensure available RAM > model size + overhead
- ‚ö†Ô∏è Even small deficit (0.5 GB) causes 28% slowdown
- ‚ö†Ô∏è Thrashing threshold is SHARP, not gradual

---

### 3.5 Finding #5: Re-Reading Only Happens Under Memory Pressure

**I/O Amplification vs Memory Pressure**:

| mlock | Free RAM | Total (GB) | Unique (GB) | Amplification | Re-reads (GB) |
|-------|----------|------------|-------------|---------------|---------------|
| 0 GB  | 30 GB    | 12.85      | 12.85       | 1.00x         | 0.00          |
| 15 GB | 15 GB    | 12.91      | 12.85       | 1.00x         | 0.06          |
| 16 GB | 14 GB    | 13.88      | 12.85       | 1.08x         | **1.03**      |
| 17 GB | 13 GB    | 14.06      | 12.85       | 1.09x         | 1.21          |
| 20 GB | 10 GB    | 14.52      | 12.85       | 1.13x         | **1.67**      |

**Interpretation**:
- When model fits: ZERO re-reads
- Under pressure: 8-13% extra I/O from page evictions

**What gets re-read?**
Likely hypothesis:
1. Hot parameters (embeddings, frequently used layer weights)
2. Pages evicted by LRU during inference
3. Pages touched early, evicted, needed again at end

**Critical Implication for Thesis**:
- ‚úÖ Re-reading is MINIMAL even under pressure (1.13x max)
- ‚úÖ Much better than expected (not 3x!)
- ‚úÖ Suggests llama.cpp has good locality of reference

---

### 3.6 Finding #6: cache_delta Validates Unique Sectors Metric ‚úÖ

**Comparison at Low Memory Pressure**:

| mlock | Unique Sectors (GB) | Cache Œî (GB) | Match % | Status |
|-------|---------------------|--------------|---------|--------|
| 0 GB  | 12.85               | 12.93        | 100.6%  | ‚úÖ Excellent |
| 15 GB | 12.85               | 12.68        | 98.7%   | ‚úÖ Excellent |

**Interpretation**:
At low memory pressure, cache_delta (system-wide page cache change) matches unique sectors (blktrace analysis) within 1-2%!

**This proves**:
1. ‚úÖ Unique sectors calculation is correct
2. ‚úÖ drop_caches() works properly
3. ‚úÖ Minimal contamination from other processes
4. ‚úÖ Both metrics measure the same thing: actual model data cached

**Under Memory Pressure**:

| mlock | Unique Sectors (GB) | Cache Œî (GB) | Match % | Status |
|-------|---------------------|--------------|---------|--------|
| 16 GB | 12.85               | 11.86        | 92.3%   | Some eviction |
| 17 GB | 12.85               | 10.83        | 84.3%   | More eviction |
| 20 GB | 12.85               | 7.89         | 61.4%   | Heavy eviction |

**Interpretation**:
Under memory pressure, cache_delta < unique_sectors because kernel evicts pages DURING inference.

- Unique sectors: Total data ACCESSED (from blktrace I/O log)
- Cache delta: Data STILL RESIDENT at end (from /proc/meminfo)

**Difference = Pages accessed then evicted during inference**

---

## PART 4: Questionable Findings Requiring Further Investigation

### 4.1 Question #1: Is 100% Coverage Reasonable? ‚ùì

**Observation**:
```
Model size:      12.85 GB
Unique accessed: 12.85 GB (100.0%)
Tokens generated: 100
```

**For only 100 tokens, we access the ENTIRE 12.85 GB model!**

**Possible Explanations**:

**Hypothesis A: Initialization File Scan**
- llama.cpp scans entire .gguf file during model loading
- Validates metadata, checksums, format
- Happens before first token generation
- ALL subsequent tokens use cached data

**Hypothesis B: True Inference Behavior**
- 100 tokens requires processing through ALL transformer layers
- Each layer touches its parameters
- Even unused vocabulary embeddings are mapped into memory

**Test to Determine**:
```bash
Run experiments with varying token counts:
- 10 tokens   ‚Üí If still 100%, proves initialization scan
- 100 tokens  ‚Üí Baseline (current)
- 1000 tokens ‚Üí If still 100%, validates hypothesis
```

**Expected Results**:
- If initialization: ALL experiments show 100% coverage
- If inference: Coverage increases with token count

**Critical for Thesis**:
- Need to understand if 100% is fundamental or initialization artifact
- Affects design of partial model loading strategies

---

### 4.2 Question #2: Is 1.0x Amplification (No Re-reading) Reasonable? ‚ùì

**Observation**:
```
0 GB lock: 12.85 GB total / 12.85 GB unique = 1.00x
```

**Every sector read EXACTLY ONCE. No re-reading!**

**Why This Might Be Correct**:

‚úÖ **Memory-mapped I/O**
- llama.cpp uses mmap() to map .gguf file
- Kernel handles page faults transparently
- Each page loaded exactly once on first access
- Stays cached for duration of inference

‚úÖ **Sequential Access Pattern**
- Transformers process layers sequentially
- No need to re-visit previous layers
- Parameters accessed once per forward pass

‚úÖ **Sufficient RAM**
- 30 GB free >> 12.85 GB model
- Everything stays resident in page cache
- No eviction pressure

**Why This Might Be Suspicious**:

‚ùì **What about KV cache?**
- Attention mechanism needs key-value pairs from all previous tokens
- Does this require re-reading embeddings?
- Or is KV cache separate from model file?

‚ùì **What about multi-head attention?**
- Different heads access Q, K, V matrices
- Are these sequential or require jumping?

‚ùì **What about kernel readahead?**
- Kernel might speculatively read ahead
- Could this cause "over-reading"?

**Test to Validate**:
```bash
strace -e trace=read,pread64,mmap llama-cli ...
# Analyze:
# 1. Is model file mmap'd or read()?
# 2. How many times is file descriptor accessed?
# 3. Are there sequential read() calls or single mmap?
```

**Critical for Thesis**:
- Need to understand llama.cpp's I/O architecture
- Validates assumptions about offloading efficiency

---

### 4.3 Question #3: Is 100% Sequential Access Reasonable? ‚ùì

**Observation**:
```
0 GB lock:
- Perfect sequential (gap=0): 100.0%
- Backward seeks:              0.0%
- Forward gaps:                0.0%
```

**52,668 consecutive I/O operations with ZERO gaps!**

**Why This Might Be Correct**:

‚úÖ **Layer-by-layer processing**
- Transformer processes layer 0 ‚Üí 1 ‚Üí 2 ‚Üí ... ‚Üí N
- Parameters stored in same order in file
- Results in sequential file access

‚úÖ **Memory-mapped streaming**
- mmap() with sequential access hint (MADV_SEQUENTIAL)
- Kernel optimizes for sequential pattern
- Readahead prefetches next pages

‚úÖ **Single-threaded inference**
- llama-cli processes one token at a time
- No parallel threads causing interleaved access

**Why This Might Be Suspicious**:

‚ùì **What about attention?**
- Multi-head attention accesses Q, K, V matrices
- Are these laid out sequentially in file?
- Or do different heads jump around?

‚ùì **What about embeddings?**
- Vocabulary embeddings: ~500 MB
- Only small fraction needed per token
- Why read entire embedding table sequentially?

‚ùì **What about quantization?**
- F16 quantization groups parameters
- Does this affect access pattern?

**Critical for Thesis**:
- If truly sequential: ‚úÖ SSD offloading is highly efficient
- If random: ‚ö†Ô∏è Need sophisticated prefetching
- Need to profile attention mechanism specifically

---

### 4.4 Question #4: Is 1.08-1.13x Amplification Under Pressure the Interesting Finding? üî¨

**Observation**:

| mlock | Amplification | Re-reads (GB) |
|-------|---------------|---------------|
| 16 GB | 1.08x         | 1.03          |
| 17 GB | 1.09x         | 1.21          |
| 20 GB | 1.13x         | 1.67          |

**Even under severe memory pressure (20 GB lock, 10 GB free), only 13% extra I/O!**

**Why This Is Interesting**:

‚úÖ **Much better than expected**
- Naive assumption: thrashing causes 10x or 100x re-reads
- Reality: only 1.13x even at heavy pressure
- Suggests good locality of reference in llama.cpp

‚úÖ **Predictable degradation**
- Re-reads increase gradually with memory pressure
- Not chaotic or unpredictable
- Could model this relationship

‚úÖ **Still functional under pressure**
- 3.02 tok/s at 20 GB lock vs 4.84 tok/s baseline
- 38% slower, but not unusably slow
- Suggests offloading is viable even with limited RAM

**Research Questions**:

1. **What pages get evicted and re-read?**
   - Hot parameters? (embeddings, layer norms)
   - Recently used layers?
   - Random eviction by LRU?

2. **Can we predict which pages to pin in RAM?**
   - Keep embeddings resident
   - Allow layer weights to swap
   - Optimize for minimum re-reads

3. **How does this scale with larger models?**
   - gpt-oss-20b: 13% extra I/O at severe pressure
   - Would 100 GB model show 13% or 130%?

**Critical for Thesis**:
- ‚úÖ This is the REAL finding about memory-constrained inference
- ‚úÖ Quantify exact tradeoff: RAM vs performance vs I/O overhead
- ‚úÖ Design intelligent page replacement policies

---

## PART 5: Implications for Thesis

### 5.1 What We Now Know with High Confidence

1. **‚úÖ LLM parameter access is sequential**
   - 100% sequential when model fits in RAM
   - No random seeking or jumping
   - Ideal for SSD streaming architectures

2. **‚úÖ No redundant I/O when model fits**
   - 1.0x amplification (every sector read once)
   - Efficient memory-mapped access
   - Validates llama.cpp's design

3. **‚úÖ Thrashing threshold is sharp and predictable**
   - Free RAM must exceed (model_size + ~1 GB)
   - Sharp performance cliff when threshold crossed
   - Gradual degradation beyond threshold

4. **‚úÖ Re-reading is minimal even under pressure**
   - Only 13% extra I/O at severe memory pressure
   - Much better than naive expectations
   - Suggests parameter offloading is viable

5. **‚úÖ Measurement methodology is now reliable**
   - cache_delta matches unique sectors at low pressure
   - Bandwidth numbers are realistic
   - Gap distribution is meaningful

### 5.2 What We Still Need to Investigate

1. **‚ùì 100% coverage for 100 tokens**
   - Test with varying token counts (10, 100, 1000)
   - Determine if initialization or inference
   - Understand parameter access patterns

2. **‚ùì 1.0x amplification correctness**
   - Profile with strace to see actual syscalls
   - Understand mmap vs read() usage
   - Validate with llama.cpp source code

3. **‚ùì 100% sequential access pattern**
   - Profile attention mechanism specifically
   - Understand embedding table access
   - Validate with different model architectures

4. **‚ùì What gets re-read under pressure?**
   - Analyze temporal patterns in blktrace
   - Identify hot parameters
   - Design optimal page pinning strategies

### 5.3 Thesis Contributions (Revised)

**Primary Contribution**:
"LLM parameter offloading to SSD is highly efficient due to sequential access patterns and minimal re-reading even under memory pressure."

**Supporting Evidence**:
1. 100% sequential access when model fits (not random)
2. 1.0x I/O amplification with sufficient RAM (no redundancy)
3. Only 1.13x amplification under severe pressure (13% overhead)
4. Sharp thrashing threshold allows predictable capacity planning

**Design Implications**:
1. Sequential readahead is sufficient (no complex prefetching)
2. Simple LRU eviction is adequate (minimal re-reading)
3. Target: Free RAM ‚â• model_size + 1 GB for optimal performance
4. Expect ~30-40% slowdown when RAM insufficient but still functional

### 5.4 What Previous Findings Are Now Invalid

‚ùå **"67% backward seeks intrinsic to transformers"** ‚Üí FALSE
- Access is 100% sequential
- Backward seeks are thrashing artifacts

‚ùå **"3x I/O amplification factor"** ‚Üí FALSE
- Was 5x overcounting artifact
- True amplification is 1.0x when model fits

‚ùå **"Fragmentation causes 8% backward seeks"** ‚Üí UNTESTED
- Previous experiments had action overcounting
- Need to re-run with corrected methodology

‚ùå **"Bandwidth 6-7 GB/s"** ‚Üí FALSE
- Was measuring queue submission, not actual I/O
- True bandwidth ~1 GB/s under pressure

---

## PART 6: Next Steps (Prioritized)

### Priority 1: Validate 100% Coverage Finding üî¨

**Test Plan**:
```bash
# Vary token count
tokens=10    ‚Üí Check unique sectors
tokens=100   ‚Üí Baseline (current)
tokens=1000  ‚Üí Check unique sectors
tokens=10000 ‚Üí Check unique sectors
```

**Expected Outcomes**:
- If same coverage: Proves initialization scan
- If increasing: Proves inference dependency

**Timeline**: 4 experiments √ó 30 min = 2 hours

---

### Priority 2: Profile with strace üî¨

**Test Plan**:
```bash
strace -e trace=read,pread64,mmap,madvise -o strace.log \
    llama-cli -m model.gguf -p "test" -n 100

# Analyze:
grep "model.gguf" strace.log
# Count read() calls
# Check for mmap() calls
# Look for MADV_SEQUENTIAL
```

**Goal**: Understand llama.cpp's actual I/O syscalls

**Timeline**: 1 hour

---

### Priority 3: Re-run Experiments for llama-2-7b üî¨

**Test Plan**:
```bash
# Use CORRECTED methodology (action='D')
model=llama-2-7b-chat.Q4_K_M.gguf (3.80 GB)
tokens=100

# Test matrix:
mlock=0 GB   ‚Üí Baseline
mlock=10 GB  ‚Üí Comfortable fit
mlock=15 GB  ‚Üí Near threshold
mlock=17 GB  ‚Üí Beyond threshold
```

**Goal**: Validate findings scale to smaller models

**Timeline**: 4 experiments √ó 15 min = 1 hour

---

### Priority 4: Analyze Temporal Patterns in blktrace üî¨

**Test Plan**:
```bash
# Split blktrace by time phase
# Phase 1: First 2 seconds (initialization)
# Phase 2: Remaining time (inference)

# Analyze each separately:
# - Unique sectors per phase
# - Sequential vs random per phase
# - Which sectors accessed multiple times
```

**Goal**: Separate initialization from inference I/O

**Timeline**: Custom analysis script + 1 experiment = 2 hours

---

### Priority 5: Investigate What Gets Re-Read Under Pressure üî¨

**Test Plan**:
```bash
# For 20 GB lock experiment:
# Analyze blktrace to find:
# 1. Which sectors are read multiple times?
# 2. What is their offset in model file?
# 3. Are they embeddings? Layer weights? Norms?

# Map sector ‚Üí file offset ‚Üí parameter type
```

**Goal**: Identify hot parameters for pinning

**Timeline**: 4 hours for detailed analysis

---

## PART 7: Summary and Conclusions

### Key Takeaways

1. **üîß Critical Bug Fixed**: blktrace action filtering corrected
2. **‚úÖ Sequential Access**: LLM inference is 100% sequential (not random!)
3. **‚úÖ Minimal Re-reading**: Only 13% extra I/O under severe pressure
4. **‚úÖ Sharp Threshold**: Thrashing starts when free RAM < model + 1 GB
5. **‚ùì Open Questions**: 100% coverage, 1.0x amplification need validation

### What Changed in Methodology

**Before** (BROKEN):
```python
WHERE rwbs LIKE '%R%'
```
‚Üí Counted each I/O 5 times (Q, G, P, D, C)

**After** (FIXED):
```python
WHERE action = 'D' AND rwbs LIKE '%R%'
```
‚Üí Counts only Dispatch (actual disk I/O)

### Impact on Previous Work

**INVALIDATED**:
- 21DECP2.md (tmpfs experiments)
- 21DECP3.md (fragmentation experiments)
- 21DECP4.md (gpt-oss MoE experiments)

All moved to `journal/INVALIDATED/` folder.

### Confidence Levels

**High Confidence** ‚úÖ:
- Sequential access pattern
- Minimal re-reading
- Thrashing threshold
- Measurement methodology

**Medium Confidence** ‚ö†Ô∏è:
- 100% coverage (needs token count validation)
- 1.0x amplification (needs strace validation)

**Low Confidence** ‚ùì:
- Exact thrashing mechanisms
- What parameters get re-read
- How findings scale to larger models

---

**Document Status**: ‚úÖ VALID - Uses corrected methodology
**Last Updated**: 2025-12-22
**Next Review**: After validation experiments (token count, strace, llama-2-7b)
