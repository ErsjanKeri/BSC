# 2026-01-08 - Tensor Tracing Journal

## Summary

Today's work focused on creating an automated experiment pipeline for tensor tracing. Built a complete end-to-end system that executes llama.cpp with tracing, parses all generated data, and prepares it for WebUI visualization.

## Part 1: Enhanced Trace Parser with JSON Export

**Objective**: Add ability to split binary trace file into per-token JSON files.

### Changes to `tools/parse_trace.py`

Added new function `export_to_json_per_token()` that:
- Groups trace entries by `token_id`
- Computes per-token metadata (duration, entry count)
- Formats source tensors properly:
  - Tensor pointers as hex strings (`0x...`)
  - Separates `disk_offset` vs `buffer_id` based on `memory_source`
  - Adds relative timestamps from token start
- Outputs one JSON file per token: `token-00000.json`, `token-00001.json`, etc.

**New CLI flag**: `--export-json OUTPUT_DIR`

Example usage:
```bash
python3 tools/parse_trace.py /tmp/tensor_trace.bin --export-json webui/public/data/traces/
```

**Output format** matches existing trace JSON structure:
```json
{
  "token_id": 0,
  "metadata": {
    "total_entries": 582,
    "duration_ms": 367.141,
    "timestamp_start_ns": 1659468596651000,
    "format_version": "256-byte"
  },
  "entries": [
    {
      "entry_id": 0,
      "timestamp_ns": 1659468596651000,
      "timestamp_relative_ms": 0.0,
      "token_id": 0,
      "layer_id": null,
      "thread_id": 24832,
      "phase": "PROMPT",
      "operation_type": "SOFT_MAX_BACK",
      "dst_name": "inp_embd",
      "num_sources": 2,
      "sources": [
        {
          "name": "token_embd.weight",
          "tensor_ptr": "0x2834e6580",
          "size_bytes": 36864000,
          "layer_id": null,
          "memory_source": "DISK",
          "disk_offset": 55469440
        },
        ...
      ]
    },
    ...
  ]
}
```

### Import additions
```python
import json
from collections import defaultdict
```

## Part 2: Automated Experiment Runner

**Objective**: Create single script that runs entire tensor tracing pipeline automatically.

### Created `run_experiment.py`

Main entry point for running experiments. Modeled after `disk-benchmarking/run_experiment.py`.

**Pipeline stages**:

1. **Load Settings** (`settings.json`)
   - Experiment parameters (model, prompt, tokens to generate)
   - Path configuration (llama.cpp location, binaries, output directories)
   - Temp file locations

2. **Verify Prerequisites**
   - Check llama-completion binary exists
   - Check llama-gguf-dump binary exists
   - Check model file exists
   - Check all parser scripts exist

3. **Clean Temp Files**
   - Remove `/tmp/tensor_trace.bin`
   - Remove `/tmp/buffer_stats.jsonl`
   - Remove `/tmp/graphs/` directory

4. **Run Inference**
   - Execute `llama-completion` with `-no-cnv` flag
   - Monitor execution time
   - Verify trace files generated

5. **Parse GGUF → Memory Map**
   - Run `llama-gguf-dump` to generate CSV
   - Parse CSV to `memory-map.json` using `tools/parse_csv.py`

6. **Parse Trace → JSON per Token**
   - Parse binary trace using `tools/parse_trace.py --export-json`
   - Generate `traces/token-00000.json`, `token-00001.json`, etc.

7. **Parse Graphs → JSON per Token**
   - Parse each DOT file using `tools/parse_dot.py`
   - Generate `graphs/token-00000.json`, `token-00001.json`, etc.

8. **Parse Buffer Stats → Timeline**
   - Parse JSONL using `tools/parse_buffer_stats.py`
   - Generate `buffer-timeline.json`

9. **Display Summary**
   - Show file counts, inference time, output location

### Key Features

- **Stop on first error**: No silent failures (per user requirement)
- **Always uses `-no-cnv`**: Disables conversation mode for clean batch generation
- **Automatic path resolution**: Handles relative paths from settings.json
- **Comprehensive error handling**: Clear error messages with context
- **Timestamped logging**: `[HH:MM:SS]` prefix on all log messages

### Error Handling Philosophy

Every subprocess call checks return code and exits immediately with stderr output on failure. Example:

```python
result = subprocess.run(cmd, capture_output=True, text=True)
if result.returncode != 0 and check:
    error_exit(f"{description} failed:\n{result.stderr}")
```

### Settings File Format

Created `settings.json`:

```json
{
  "experiment": {
    "model_path": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    "tokens_to_generate": 3,
    "prompt": "Hello world"
  },
  "paths": {
    "llama_cpp_dir": "../../llama.cpp",
    "llama_binary": "build/bin/llama-completion",
    "webui_data_dir": "webui/public/data"
  },
  "temp_files": {
    "trace_bin": "/tmp/tensor_trace.bin",
    "buffer_stats": "/tmp/buffer_stats.jsonl",
    "graphs_dir": "/tmp/graphs"
  }
}
```

**Path Resolution**:
- All paths relative to script directory
- `llama_cpp_dir` resolves to `../../llama.cpp` → `/Users/ersibesi/Desktop/LLAMA/llama.cpp`
- Model path relative to `llama_cpp_dir`
- Final path: `/Users/ersibesi/Desktop/LLAMA/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf`

## Part 3: Testing and Validation

### First Test Run

**Command**:
```bash
python3 run_experiment.py
```

**Results**:
- ✅ All prerequisites verified
- ✅ Temp files cleaned
- ✅ Inference completed (1.86s)
- ✅ Memory map generated (62 KB)
- ✅ Buffer timeline generated (737 bytes)
- ✅ Graph files: 3 tokens (token-00000, token-00001, token-00002)
- ⚠️  Trace files: 1 token (only token-00000)

### Token Count Issue Discovered

**Expected**: 3 token trace files (for 3 generated tokens)
**Actual**: 1 token trace file (all entries have token_id=0)

**Root Cause**: The C tracing code in `tensor_trace.c` does not properly increment `token_id` for generated tokens. All trace entries are marked with `token_id = 0`.

**Evidence**:
```bash
$ python3 tools/parse_trace.py /tmp/tensor_trace.bin --stats
Tokens: [0]  # ← All entries have token_id = 0
```

**Graph files are correct**: Each generation token gets its own DOT file:
- `/tmp/graphs/token_00000.dot`
- `/tmp/graphs/token_00001.dot`
- `/tmp/graphs/token_00002.dot`

**Conclusion**: This is a limitation in the current tensor tracing implementation. The graph dumping (controlled by ggml.c) correctly tracks token IDs, but the tensor trace logging does not.

### Generated File Structure

After running experiment:

```
webui/public/data/
├── memory-map.json              (62 KB)
├── buffer-timeline.json         (737 B)
├── graphs/
│   ├── token-00000.json        (338 KB)
│   ├── token-00001.json        (338 KB)
│   └── token-00002.json        (338 KB)
└── traces/
    └── token-00000.json         (916 KB)
```

**Note**: Only one trace file because all entries have `token_id = 0`.

## Part 4: Code Quality and Documentation

### Files Modified

1. **`tools/parse_trace.py`** (enhanced)
   - Added `export_to_json_per_token()` function
   - Added `--export-json` CLI argument
   - Added imports: `json`, `defaultdict`
   - Line count: ~496 lines (+68 lines)

2. **`run_experiment.py`** (created)
   - Complete pipeline automation
   - 525 lines of Python
   - Comprehensive error handling
   - Modeled after disk-benchmarking workflow

3. **`settings.json`** (created)
   - Experiment configuration
   - Path configuration
   - 17 lines JSON

### Code Style

- **Docstrings**: All functions documented with Args and Returns
- **Error messages**: Clear, actionable error messages with context
- **Logging**: Timestamped logs for all major steps
- **Type hints**: Not used (to match existing codebase style)
- **Comments**: Inline comments for complex logic

### Comparison with disk-benchmarking

**Similarities**:
- `load_settings()` function pattern
- `resolve_paths()` for path management
- `log()` function with timestamps
- `error_exit()` for failure handling
- Pipeline structure: clean → run → parse → display

**Differences**:
- No `sudo` requirement (no blktrace)
- No background processes (no mem_locker)
- Multiple parsing steps (GGUF, trace, graphs, buffers)
- Per-token file generation
- Simpler execution model

## Part 5: Known Issues and Limitations

### Issue 1: Token ID Not Tracked in Trace

**Problem**: All trace entries have `token_id = 0`, regardless of which generation token they belong to.

**Impact**:
- Cannot split trace by token accurately
- All trace data lumped into token-00000.json
- Cannot correlate trace with graph on per-token basis

**Workaround**: Graph files are correctly split by token, so graphs can still be visualized per-token.

**Proper Fix** (future work):
1. Modify `tensor_trace.c` to track current token ID
2. Pass token ID through `ggml_backend_sched_eval_callback`
3. Update trace entries with correct token ID during generation phase

### Issue 2: Name Truncation (from 2026-01-07)

**Problem**: Source tensor names limited to 19 chars in trace struct.

**Example**: `"blk.0.attn_norm.weight"` → `"blk.0.attn_norm.wei"`

**Status**: Documented limitation, use address-based correlation.

### Issue 3: Operation Type Enum (fixed 2026-01-07)

**Problem**: Was fixed yesterday - operation enum had missing entries causing incorrect operation names.

**Status**: ✅ Resolved - all 95 operations correctly mapped.

## Part 6: File Sizes and Performance

### Model
- **File**: `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf`
- **Size**: 669 MB (701,267,968 bytes)
- **Layers**: 22
- **Vocabulary**: 32,000 tokens

### Experiment Parameters
- **Prompt**: "Hello world" (~2 tokens)
- **Tokens to generate**: 3
- **Total inference time**: 1.86 seconds

### Generated Data Sizes

| File | Size | Count |
|------|------|-------|
| memory-map.json | 62 KB | 1 |
| buffer-timeline.json | 737 B | 1 |
| graphs/token-*.json | 338 KB each | 3 files |
| traces/token-*.json | 916 KB | 1 file (should be 3) |
| **Total** | **~1.9 MB** | 6 files |

### Trace File Details
- **Binary trace**: `/tmp/tensor_trace.bin`
- **Size**: ~300 KB (exact size varies)
- **Entries**: 1,162 entries
- **Format**: 256 bytes per entry
- **Operations logged**: 1,162 tensor operations
- **Unique source tensors**: 451

### Buffer Stats
- **Peak occupancy**: Varies per run
- **Events**: Allocation and deallocation events
- **Format**: JSONL (one event per line)

### Graph Files
- **Format**: Graphviz DOT → JSON
- **Nodes per graph**: ~600-800 nodes
- **Edges per graph**: ~600-800 edges
- **Layers captured**: 22 (all TinyLlama layers)

## Part 7: Usage Instructions

### Running an Experiment

1. **Edit settings** (optional):
   ```bash
   vim settings.json
   ```

2. **Run experiment**:
   ```bash
   cd /Users/ersibesi/Desktop/LLAMA/BSC/tensor-tracing
   python3 run_experiment.py
   ```

3. **View results**:
   - Data ready in `webui/public/data/`
   - No need to restart dev server
   - Frontend automatically loads new data

### Changing Experiment Parameters

Edit `settings.json`:

```json
{
  "experiment": {
    "model_path": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    "tokens_to_generate": 5,           // ← Change this
    "prompt": "Once upon a time"       // ← Change this
  },
  ...
}
```

### Using Different Models

1. Place model in `llama.cpp/` directory
2. Update `settings.json`:
   ```json
   "model_path": "your-model-name.gguf"
   ```

### Debugging

**View trace statistics**:
```bash
python3 tools/parse_trace.py /tmp/tensor_trace.bin --stats
```

**View trace entries**:
```bash
python3 tools/parse_trace.py /tmp/tensor_trace.bin --limit 20
```

**Verify trace format**:
```bash
python3 tools/parse_trace.py /tmp/tensor_trace.bin --verify
```

**Check buffer stats**:
```bash
python3 tools/parse_buffer_stats.py /tmp/buffer_stats.jsonl --summary
```

## Part 8: Next Steps and Future Work

### High Priority

1. **Fix token_id tracking in tensor_trace.c**
   - Modify C code to track current token during generation
   - Pass token_id through callback chain
   - Update trace entries with correct token_id
   - Would enable proper per-token trace splitting

2. **Add experiment metadata**
   - Save experiment parameters to JSON
   - Include model name, prompt, token count
   - Timestamp of experiment
   - Git commit hash of llama.cpp

3. **Add validation checks**
   - Verify token counts match (trace vs graphs)
   - Check for missing tokens
   - Validate layer counts
   - Sanity check operation distributions

### Medium Priority

4. **Progress indicators**
   - Show progress during parsing (especially for large traces)
   - Estimate time remaining for long operations

5. **Incremental parsing**
   - Don't re-parse unchanged files
   - Check timestamps/hashes
   - Skip if output is newer than input

6. **Experiment comparison**
   - Save experiments with timestamps
   - Allow comparing multiple runs
   - Diff two experiments

### Low Priority

7. **Parallel parsing**
   - Parse graphs in parallel (one per token)
   - Speed up for experiments with many tokens

8. **Compression**
   - Compress large JSON files
   - Trade-off: file size vs load time

9. **Name truncation fix**
   - Increase name buffer size in trace struct
   - Or use variable-length names with offsets

## Part 9: Architecture Notes

### Pipeline Design Philosophy

**Goal**: Zero-friction experiment execution
- Single command to run everything
- No manual copying/moving files
- No need to remember command sequences
- Clear error messages on failure

**Design Choices**:
1. **Stop on first error**: Fail fast, don't continue with corrupted data
2. **No silent failures**: Every error logged with context
3. **Automatic cleanup**: Remove old files before starting
4. **Atomic operations**: Either full success or full failure (no partial state)

### File Organization

```
tensor-tracing/
├── run_experiment.py          # Main entry point
├── settings.json              # Configuration
├── tools/                     # Parser scripts
│   ├── parse_trace.py        # Binary trace → JSON
│   ├── parse_dot.py          # DOT graph → JSON
│   ├── parse_buffer_stats.py # JSONL → JSON
│   └── parse_csv.py          # CSV → JSON (memory map)
└── webui/
    └── public/
        └── data/              # Output directory (ready for frontend)
            ├── memory-map.json
            ├── buffer-timeline.json
            ├── traces/
            │   └── token-*.json
            └── graphs/
                └── token-*.json
```

### Data Flow

```
Model (GGUF)
  ↓ llama-gguf-dump
CSV structure
  ↓ parse_csv.py
memory-map.json ──────────────────┐
                                   │
llama-completion (with tracing)    │
  ↓                                 ↓
├─→ /tmp/tensor_trace.bin ───→ traces/*.json
├─→ /tmp/buffer_stats.jsonl ─→ buffer-timeline.json
└─→ /tmp/graphs/*.dot ───────→ graphs/*.json
                                   │
                                   ↓
                           webui/public/data/
                           (ready for visualization)
```

### Error Propagation

All subprocess calls use this pattern:

```python
result = subprocess.run(cmd, capture_output=True, text=True)
if result.returncode != 0:
    error_exit(f"Step failed:\n{result.stderr}")
```

This ensures:
- Immediate failure on error
- Full error message visible to user
- No cascading failures from bad data

## Part 10: Summary Statistics

### Lines of Code Written Today

| File | Language | Lines | Type |
|------|----------|-------|------|
| run_experiment.py | Python | 525 | New file |
| tools/parse_trace.py | Python | +68 | Modified |
| settings.json | JSON | 17 | New file |
| **Total** | | **610** | |

### Functions Added

**In `tools/parse_trace.py`**:
- `export_to_json_per_token()` - Split trace by token and export to JSON

**In `run_experiment.py`**:
- `log()` - Timestamped logging
- `error_exit()` - Exit with error message
- `run_cmd()` - Run subprocess with error handling
- `load_settings()` - Load JSON configuration
- `resolve_paths()` - Convert relative to absolute paths
- `verify_prerequisites()` - Check all required files exist
- `clean_temp_files()` - Remove old trace files
- `run_llama_inference()` - Execute llama-completion
- `parse_gguf_to_memory_map()` - Generate memory map JSON
- `parse_trace_to_json()` - Split trace into per-token files
- `parse_graphs_to_json()` - Convert DOT to JSON per token
- `parse_buffer_stats()` - Generate timeline JSON
- `display_summary()` - Show experiment results
- `main()` - Pipeline orchestration

**Total**: 14 functions

### Time Spent

Approximate breakdown:
- Planning and requirements (30 min)
- Implementing `export_to_json_per_token()` (45 min)
- Implementing `run_experiment.py` (90 min)
- Testing and debugging (30 min)
- Documentation (this journal entry) (45 min)
- **Total**: ~4 hours

### Commits Recommended

1. `feat(parse_trace): add JSON export per token`
   - Add export_to_json_per_token() function
   - Add --export-json CLI argument
   - Format source tensors with hex addresses

2. `feat(experiment): add automated pipeline runner`
   - Create run_experiment.py
   - Create settings.json template
   - Implement full pipeline: clean → run → parse → display
   - Stop on first error, always use -no-cnv flag

## End of 2026-01-08 Journal
