# December 21, 2025 - LLM Parameter Offloading Experiments

## PART 4: gpt-oss-20b MoE Model Testing & Cross-Model Analysis

### 4.1 Experimental Setup

**Model:** gpt-oss-20b-F16.gguf
- Architecture: Mixture of Experts (32 experts, 4 active per token)
- Quantization: F16 (full precision, not quantized)
- Size: 12.85 GB
- Storage: `/mnt/experiment_ssd` (nvme0n1)
- Extents: **1** (perfectly contiguous)

**Test Matrix:**
- Run 1: 0 GB memory lock (30 GB free)
- Run 2: 10 GB memory lock (20 GB free)
- Run 3: 12 GB memory lock (18 GB free)

**Comparison Reference:**
- llama-2-7b-chat.Q4_K_M.gguf: 3.80 GB, 1 extent, 20 GB memory lock

---

### 4.2 Results Summary

| Configuration | Model | Size (GB) | mlock (GB) | Free RAM (GB) | Time (s) | tok/s | Total Read (GB) | Unique (GB) | Coverage | Backward Seeks |
|---------------|-------|-----------|------------|---------------|----------|-------|-----------------|-------------|----------|----------------|
| **Run 1** | gpt-oss-20b | 12.85 | 0 | 30 | 20.78 | 4.81 | 38.81 | 12.85 | 100.0% | **66.9%** |
| **Run 2** | gpt-oss-20b | 12.85 | 10 | 20 | 20.73 | 4.82 | 38.80 | 12.85 | 100.0% | **66.8%** |
| **Run 3** | gpt-oss-20b | 12.85 | 12 | 18 | 20.68 | 4.84 | 38.79 | 12.85 | 100.0% | **66.8%** |
| **Reference** | llama-2-7b | 3.80 | 20 | 10 | 14.34 | 6.97 | 11.82 | 3.80 | 100.0% | **67.0%** |

---

### 4.3 Critical Finding #1: Performance Insensitivity to Memory Pressure

**Observation:**
- 0 GB lock ‚Üí 12 GB lock: Only 0.1s difference (20.78s ‚Üí 20.68s)
- Performance virtually identical across all three runs
- **No thrashing behavior observed**

**Analysis:**
All three configurations had sufficient free RAM:
- 0 GB lock: 30 GB free / 12.85 GB model = **2.33x ratio**
- 10 GB lock: 20 GB free / 12.85 GB model = **1.56x ratio**
- 12 GB lock: 18 GB free / 12.85 GB model = **1.40x ratio**

Even the worst case (1.40x) is well above the thrashing threshold observed with llama-2-7b.

**Comparison with llama-2-7b thrashing:**
- llama-2-7b at 22 GB lock: 8 GB free / 3.8 GB model = **2.1x ratio** ‚Üí severe thrashing (30s, 3.31 tok/s)
- gpt-oss-20b at 12 GB lock: 18 GB free / 12.85 GB model = **1.4x ratio** ‚Üí no thrashing (20.68s, 4.84 tok/s)

**This is inconsistent!** Why does gpt-oss not thrash at 1.4x ratio when llama-2-7b thrashed at 2.1x?

**Possible Explanations:**
1. **Thrashing threshold may not scale linearly** - could depend on absolute RAM amount, not just ratio
2. **Model loading differences** - llama.cpp may handle different model sizes differently
3. **Need to test gpt-oss at higher memory locks** (20 GB, 22 GB, 25 GB) to find actual thrashing point

---

### 4.4 Critical Finding #2: Gap Distribution Scales Across Models

**Key Data:**

| Model | Size | Architecture | Sequential | Backward Seeks |
|-------|------|--------------|------------|----------------|
| llama-2-7b | 3.80 GB | Standard Transformer | 30.2% | **67.0%** |
| gpt-oss-20b | 12.85 GB | MoE (32 experts) | 33.1% | **66.9%** |

**Findings:**
1. **Nearly identical gap distribution** despite:
   - 3.4x size difference (3.8 GB ‚Üí 12.85 GB)
   - Different architectures (standard vs MoE)
   - Different quantization (Q4_K_M vs F16)

2. **Backward seeks consistent at ~67%** across both models
   - gpt-oss: 66.8-66.9% (variance: 0.1%)
   - llama-2-7b: 67.0-67.6% (variance: 0.6%)

3. **Sequential access slightly higher for gpt-oss**
   - gpt-oss: 33.1% vs llama-2-7b: 30.2%
   - Only 2.9% difference

**Interpretation:**
This is **strong evidence that ~67% backward seeks are intrinsic to transformer inference**, not:
- Fragmentation artifacts (both models have 1 extent)
- Model size effects (scales from 3.8 GB to 12.85 GB)
- Architecture variations (standard vs MoE shows same pattern)

**Implication:**
The MoE architecture (32 experts, 4 active per token) does **not** significantly change the access pattern. Despite experts being stored in different file regions, the overall pattern remains ~67% backward seeks.

---

### 4.5 Critical Finding #3: I/O Amplification Factor

**Calculation:**

| Model | Total Read (GB) | Unique Data (GB) | Amplification Factor |
|-------|-----------------|------------------|----------------------|
| llama-2-7b | 11.82 | 3.80 | **3.11x** |
| gpt-oss-20b (avg) | 38.80 | 12.85 | **3.02x** |

**Observation:**
- Both models re-read data approximately **3x**
- I/O amplification is **consistent across model sizes**

**What This Means:**
For every unique byte of model data accessed:
- The system reads it ~3 times on average
- This suggests similar re-reading patterns during inference
- Likely due to:
  - Page evictions under memory pressure
  - Repeated access to shared parameters (embeddings, layer norms)
  - KV cache references requiring parameter re-reads

**Scaling Implication:**
If this 3x amplification holds generally, then for a model requiring N GB of unique parameter access:
- Expect ~3N GB of total I/O
- gpt-oss validates: 12.85 GB √ó 3 = 38.55 GB (measured: 38.80 GB)
- llama-2-7b validates: 3.80 GB √ó 3 = 11.40 GB (measured: 11.82 GB)

---

### 4.6 Critical Finding #4: Cache Delta Discrepancy

**Observed Cache Behavior:**

| Model | Size (GB) | Memory Lock | Free RAM (GB) | Cache Delta (GB) | Cache % |
|-------|-----------|-------------|---------------|------------------|---------|
| llama-2-7b | 3.80 | 20 GB | 10 GB | 0.92 | **24%** |
| llama-2-7b | 3.80 | 21 GB | 9 GB | 0.99 | **26%** |
| gpt-oss-20b | 12.85 | 0 GB | 30 GB | 12.94 | **100%** |
| gpt-oss-20b | 12.85 | 10 GB | 20 GB | 12.93 | **100%** |
| gpt-oss-20b | 12.85 | 12 GB | 18 GB | 12.92 | **100%** |

**The Mystery:**
- llama-2-7b with 10 GB free RAM: Only caches **24%** of 3.8 GB model
- gpt-oss-20b with 18 GB free RAM: Caches **100%** of 12.85 GB model

**This doesn't make sense!**
- llama-2-7b had 10 GB free, could easily cache entire 3.8 GB model
- Yet only ~0.9 GB was cached
- gpt-oss-20b had 18 GB free, and cached entire 12.85 GB model

**Possible Explanations:**
1. **OS page eviction policy differences**
   - With more free RAM (gpt-oss tests), OS more aggressive at caching
   - With less free RAM (llama-2-7b tests), OS evicts pages more aggressively

2. **Measurement timing issue**
   - Cache delta measured immediately after inference
   - llama-2-7b: OS may have started evicting pages before snapshot
   - gpt-oss-20b: OS kept everything due to abundant free RAM

3. **Different memory pressure scenarios**
   - llama-2-7b at 20 GB lock is closer to thrashing threshold
   - OS may be more aggressive about page eviction near threshold
   - gpt-oss-20b at 12 GB lock still has comfortable margin

**Action Item:**
Need to compare **apples-to-apples**:
- Run llama-2-7b with 0 GB lock (30 GB free) to see if cache delta reaches 100%
- Run gpt-oss-20b with 20+ GB lock to see if cache delta drops

---

### 4.7 Critical Finding #5: Performance Degradation on nvme0n1

**Performance Comparison:**

| Model | Device | Size (GB) | Time (s) | tok/s | Notes |
|-------|--------|-----------|----------|-------|-------|
| llama-2-7b | nvme1n1 | 3.80 | 11.67 | **8.57** | Fragmented (19 extents), faster SSD |
| llama-2-7b | nvme0n1 | 3.80 | 14.34 | **6.97** | Contiguous (1 extent), slower SSD |
| gpt-oss-20b | nvme0n1 | 12.85 | 20.73 | **4.82** | Contiguous (1 extent), slower SSD |

**Observations:**
1. **llama-2-7b performance drop on nvme0n1:**
   - nvme1n1 ‚Üí nvme0n1: 8.57 ‚Üí 6.97 tok/s (**19% degradation**)
   - Expected due to slower hardware

2. **gpt-oss-20b significantly slower:**
   - 4.82 tok/s vs llama-2-7b's 6.97 tok/s (**31% slower**)
   - But model is 3.4x larger (12.85 GB vs 3.8 GB)
   - And F16 (full precision) vs Q4_K_M (quantized)

3. **Tokens per second does NOT scale with model size:**
   - Larger model ‚Üí more computation per token
   - F16 quantization ‚Üí more memory bandwidth per parameter
   - MoE architecture ‚Üí routing overhead

**Bandwidth Utilization:**

| Model | Avg Bandwidth (MB/s) | Peak Bandwidth (MB/s) | Device |
|-------|----------------------|-----------------------|--------|
| llama-2-7b | N/A | N/A | nvme0n1 |
| gpt-oss-20b | 2838 | 3107 | nvme0n1 |

gpt-oss sustains **2.8 GB/s average bandwidth** on nvme0n1. This is surprisingly high for a "slower SSD."

---

### 4.8 Cross-Model Gap Distribution Breakdown

**Detailed Gap Analysis:**

| Gap Type | llama-2-7b (1 extent) | gpt-oss-20b (avg, 1 extent) | Difference |
|----------|----------------------|----------------------------|------------|
| Perfect Sequential (gap=0) | 30.2% | 33.1% | +2.9% |
| Small gaps (<128KB) | 1.3% | 0.0% | -1.3% |
| Medium gaps (128KB-1MB) | 0.9% | 0.0% | -0.9% |
| Large gaps (>1MB) | 0.6% | 0.0% | -0.6% |
| **Backward seeks** | **67.0%** | **66.9%** | **-0.1%** |

**Key Insights:**
1. **Backward seeks nearly identical** (67.0% vs 66.9%, only 0.1% difference)
2. **gpt-oss has slightly more perfect sequential access** (33.1% vs 30.2%)
3. **gpt-oss has virtually zero forward gaps** (small/medium/large all ‚âà0%)
4. **llama-2-7b has small forward gaps** (total 2.8% across small/medium/large)

**Interpretation:**
- Both models exhibit same fundamental pattern: **1/3 sequential, 2/3 backward**
- gpt-oss is slightly "cleaner" (fewer forward gaps)
- Possibly due to:
  - Different llama.cpp optimizations for larger models
  - F16 memory layout differences
  - MoE expert access pattern being more predictable

---

### 4.9 100% Coverage Analysis

**Consistency Across All Runs:**

| Experiment | Model | Unique Sectors | Coverage |
|------------|-------|----------------|----------|
| llama-2-7b Run 1 | 3.80 GB | 7,970,712 | **100.0%** |
| llama-2-7b Run 2 | 3.80 GB | 7,970,712 | **100.0%** |
| llama-2-7b Run 3 | 3.80 GB | 7,970,712 | **100.0%** |
| gpt-oss-20b Run 1 | 12.85 GB | 26,938,752 | **100.0%** |
| gpt-oss-20b Run 2 | 12.85 GB | 26,938,752 | **100.0%** |
| gpt-oss-20b Run 3 | 12.85 GB | 26,938,752 | **100.0%** |

**Perfect Consistency:**
- Unique sectors **identical** across all runs of same model
- Both models achieve **exactly 100% coverage**
- Validates measurement reliability

**What 100% Coverage Means:**
For 100-token inference, the system accesses **every single sector** of the model file.

**Why 100% Coverage for Only 100 Tokens?**

Possible reasons (still need validation):
1. **Model initialization scan:**
   - llama.cpp may scan entire file during startup
   - GGUF format validation requires full file read

2. **All layers activated:**
   - Even for 100 tokens, all transformer layers process data
   - Each layer's parameters touched at least once

3. **Embedding table:**
   - Entire vocabulary table might be loaded/validated
   - Even if only subset of tokens used

4. **MoE expert loading:**
   - For gpt-oss: all 32 experts might be initialized
   - Even though only 4 active per token

**Critical Question:**
Would coverage drop with fewer tokens (e.g., 10 tokens)? Or stay 100% due to initialization overhead?

---

### 4.10 Implications for Thesis

**What We Now Know with High Confidence:**

1. ‚úÖ **~67% backward seeks are intrinsic to transformer inference**
   - Consistent across two different models
   - Consistent across two different architectures (standard vs MoE)
   - Consistent across two different sizes (3.8 GB vs 12.85 GB)
   - Fragmentation eliminated (both models 1 extent)

2. ‚úÖ **I/O amplification factor is ~3x**
   - Scales across model sizes
   - Predictable for capacity planning

3. ‚úÖ **100% model coverage during inference**
   - Every sector accessed during 100-token generation
   - Consistent and reproducible

4. ‚úÖ **Measurement methodology is reliable**
   - Unique sector counts perfectly consistent across runs
   - Gap distribution repeatable (variance <1%)

**What Remains Uncertain:**

1. ‚ùì **Why does cache delta differ between models?**
   - llama-2-7b: 24% cached
   - gpt-oss-20b: 100% cached
   - Need apples-to-apples comparison

2. ‚ùì **Why no thrashing in gpt-oss tests?**
   - All tests had comfortable free RAM
   - Need higher memory locks to find threshold

3. ‚ùì **Is 100% coverage due to initialization or inference?**
   - Need to test with 10 tokens vs 1000 tokens
   - Profile initialization phase separately

4. ‚ùì **Why is gpt-oss ~3 GB overhead?**
   - Model: 12.85 GB
   - Cached: 12.92 GB
   - Overhead: ~70 MB (much smaller than llama-2-7b's ~2 GB overhead)
   - Proportion to model size?

---

### 4.11 Remaining Concerns for Thesis Validity

**Critical Issues to Address:**

1. **Memory overhead explanation needed:**
   - llama-2-7b: 3.8 GB file requires ~6 GB RAM (58% overhead)
   - gpt-oss-20b: 12.85 GB file requires ~15 GB RAM (17% overhead)
   - **Overhead percentage decreases with larger models?**
   - Need to understand llama.cpp memory allocation

2. **Thrashing threshold not found for gpt-oss:**
   - Cannot claim universal understanding without finding limits
   - Need to test 20 GB, 22 GB, 25 GB locks

3. **100% coverage needs validation:**
   - Is this initialization artifact or true inference pattern?
   - Test with varying token counts (10, 100, 1000, 10000)

4. **Backward seeks interpretation for thesis:**
   - 67% backward seeks acceptable for SSD offloading claim?
   - How does this compare to CHEOPS assumptions?
   - Are random access patterns viable for inference?

---

### 4.12 Next Steps (Prioritized)

**Priority 1: Test gpt-oss at Higher Memory Locks**
- Run with 20 GB, 22 GB, 25 GB locks
- Find actual thrashing threshold
- Compare threshold ratio with llama-2-7b

**Priority 2: Test llama-2-7b with Low Memory Lock**
- Run with 0 GB lock (30 GB free RAM)
- Check if cache delta reaches 100%
- Validate apples-to-apples comparison

**Priority 3: Vary Token Count**
```bash
# Test coverage vs token count
tokens_to_generate: 10    # Should coverage drop?
tokens_to_generate: 100   # Current baseline
tokens_to_generate: 1000  # Should coverage stay 100%?
```

**Priority 4: Study llama.cpp Memory Usage**
```bash
# During inference, analyze memory layout
pmap -x [llama-cli-pid] > memory_map.txt
cat /proc/[llama-cli-pid]/maps | grep gguf
```

**Priority 5: Separate Initialization from Inference**
- Profile blktrace separately for:
  - Model loading phase (first few seconds)
  - Token generation phase (steady state)
- Determine if 100% coverage is from initialization

---

### 4.13 Summary

**Major Achievements:**
1. ‚úÖ Successfully tested gpt-oss-20b MoE model (12.85 GB, 1 extent)
2. ‚úÖ Validated gap distribution scales across models (~67% backward seeks)
3. ‚úÖ Confirmed I/O amplification factor (~3x) is consistent
4. ‚úÖ Demonstrated measurement reliability (perfect consistency)

**Critical Findings:**
1. üîç **Backward seeks ARE intrinsic** - strong evidence across two models
2. üîç I/O amplification is predictable (~3x total/unique)
3. üîç 100% coverage consistent across models
4. ‚ùó **Cache delta discrepancy** needs explanation (24% vs 100%)
5. ‚ùó **No thrashing observed** - need higher memory locks

**For Thesis:**
- Gap distribution (~67% backward) is now well-validated
- Can confidently claim this is intrinsic to transformer inference
- Still need to explain memory overhead and cache behavior
- Need to find thrashing thresholds for gpt-oss to complete picture

---

## Appendix: Raw Data

### Experiment 1 (0 GB lock):
- Time: 20.78s
- Tokens/sec: 4.81
- Total reads: 38.81 GB
- Unique sectors: 26,938,752 (12.85 GB, 100%)
- Gap distribution: 33.1% seq, 66.9% backward
- Cache delta: 12.94 GB
- Bandwidth: 2838.9 MB/s avg, 3097.8 MB/s peak

### Experiment 2 (10 GB lock):
- Time: 20.73s
- Tokens/sec: 4.82
- Total reads: 38.80 GB
- Unique sectors: 26,938,752 (12.85 GB, 100%)
- Gap distribution: 33.1% seq, 66.8% backward
- Cache delta: 12.93 GB
- Bandwidth: 2837.7 MB/s avg, 3097.8 MB/s peak

### Experiment 3 (12 GB lock):
- Time: 20.68s
- Tokens/sec: 4.84
- Total reads: 38.79 GB
- Unique sectors: 26,938,752 (12.85 GB, 100%)
- Gap distribution: 33.1% seq, 66.8% backward
- Cache delta: 12.92 GB
- Bandwidth: 2837.1 MB/s avg, 3106.8 MB/s peak

**Consistency:**
- Unique sectors identical across all runs (validates measurement)
- Gap distribution variance: <0.3%
- Performance variance: <0.5%
- Highly reproducible results
